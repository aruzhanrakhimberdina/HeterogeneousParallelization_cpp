{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Практическое задание 10"
      ],
      "metadata": {
        "id": "VgvdHJlczH_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1"
      ],
      "metadata": {
        "id": "lNGs-PCMyYbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-X7J4Sgbr_m",
        "outputId": "8693194c-0e22-44b5-f2e9-f6361ebf0352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting openmp_stats.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile openmp_stats.cpp\n",
        "#include <iostream>               // для cout\n",
        "#include <vector>                 // для vector\n",
        "#include <random>                 // для генерации данных\n",
        "#include <cmath>                  // для max\n",
        "#include <omp.h>                  // для OpenMP и omp_get_wtime()\n",
        "\n",
        "using namespace std;              // чтобы не писать std:: каждый раз\n",
        "\n",
        "int main() {\n",
        "\n",
        "    const int N = 10'000'000;     // большой массив (можешь поставить 1'000'000 если надо быстрее)\n",
        "    vector<double> data(N);       // создаю массив данных на CPU\n",
        "\n",
        "    mt19937 rng(42);              // фиксированный seed, чтобы результаты были воспроизводимыми\n",
        "    uniform_real_distribution<double> dist(0.0, 1.0); // значения [0,1]\n",
        "\n",
        "    // хочу протестировать разные числа потоков\n",
        "    vector<int> thread_list = {1, 2, 4, 8};\n",
        "\n",
        "    // здесь сохраню общее время для 1 потока, чтобы потом считать speedup\n",
        "    double t1_total = 0.0;\n",
        "\n",
        "    // здесь сохраню оценку доли последовательной части s по запуску на 1 потоке\n",
        "    double s_serial_est = 0.0;\n",
        "\n",
        "    cout << \"N = \" << N << endl;\n",
        "    cout << \"Threads | Total(s) | Serial(s) | Parallel(s) | Serial frac | Speedup | Amdahl pred\" << endl;\n",
        "\n",
        "    for (int T : thread_list) {                      // прогоняю разные значения потоков\n",
        "\n",
        "        omp_set_num_threads(T);                      // задаю число потоков OpenMP\n",
        "\n",
        "        double t_total_start = omp_get_wtime();      // старт общего времени\n",
        "\n",
        "        double t_serial_start = omp_get_wtime();     // старт последовательной части (инициализация)\n",
        "        for (int i = 0; i < N; i++) {                // заполняю массив (это делаю намеренно последовательно)\n",
        "            data[i] = dist(rng);                     // генерация случайного числа\n",
        "        }\n",
        "        double t_serial_end = omp_get_wtime();       // конец последовательной части инициализации\n",
        "\n",
        "        double sum = 0.0;                            // сумма\n",
        "        double sumsq = 0.0;                          // сумма квадратов\n",
        "\n",
        "        double t_parallel_start = omp_get_wtime();   // старт параллельной части\n",
        "\n",
        "        // базовая параллельная версия: считаю сумму и сумму квадратов в одном проходе\n",
        "        #pragma omp parallel for reduction(+:sum,sumsq)\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            double x = data[i];                      // беру элемент\n",
        "            sum += x;                                // добавляю к сумме\n",
        "            sumsq += x * x;                          // добавляю к сумме квадратов\n",
        "        }\n",
        "\n",
        "        double t_parallel_end = omp_get_wtime();     // конец параллельной части\n",
        "\n",
        "        double t_serial2_start = omp_get_wtime();    // последовательная часть (финальные формулы)\n",
        "        double mean = sum / (double)N;               // среднее\n",
        "        double variance = (sumsq / (double)N) - mean * mean; // дисперсия по формуле E[x^2] - (E[x])^2\n",
        "        variance = max(0.0, variance);               // из-за погрешностей может быть маленький минус, обрезаю до 0\n",
        "        double t_serial2_end = omp_get_wtime();      // конец финальной последовательной части\n",
        "\n",
        "        double t_total_end = omp_get_wtime();        // конец общего времени\n",
        "\n",
        "        // считаю времена по блокам\n",
        "        double serial_time = (t_serial_end - t_serial_start) + (t_serial2_end - t_serial2_start);\n",
        "        double parallel_time = (t_parallel_end - t_parallel_start);\n",
        "        double total_time = (t_total_end - t_total_start);\n",
        "\n",
        "        // считаю долю последовательной части\n",
        "        double serial_frac = serial_time / total_time;\n",
        "\n",
        "        // сохраняю t1 и s для закона Амдала\n",
        "        if (T == 1) {\n",
        "            t1_total = total_time;                   // время на 1 потоке\n",
        "            s_serial_est = serial_frac;              // оценка s\n",
        "        }\n",
        "\n",
        "        // ускорение относительно 1 потока\n",
        "        double speedup = (t1_total > 0.0) ? (t1_total / total_time) : 0.0;\n",
        "\n",
        "        // предсказание закона Амдала: S(p) = 1 / (s + (1-s)/p)\n",
        "        double amdahl_pred = 1.0 / (s_serial_est + (1.0 - s_serial_est) / (double)T);\n",
        "\n",
        "        // печатаю результаты\n",
        "        cout << T << \"       | \"\n",
        "             << total_time << \" | \"\n",
        "             << serial_time << \" | \"\n",
        "             << parallel_time << \" | \"\n",
        "             << serial_frac << \" | \"\n",
        "             << speedup << \" | \"\n",
        "             << amdahl_pred << endl;\n",
        "\n",
        "        // чтобы было видно, что расчёты не пустые\n",
        "        if (T == 1) {\n",
        "            cout << \"Check (T=1): mean=\" << mean << \", variance=\" << variance << endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // короткий итог по Амдалу\n",
        "    cout << \"\\nEstimated serial fraction s (from T=1 run): \" << s_serial_est << endl;\n",
        "    cout << \"Theoretical max speedup as T->infinity: 1/s = \" << (s_serial_est > 0.0 ? 1.0 / s_serial_est : 0.0) << endl;\n",
        "\n",
        "    return 0;                                         // конец программы\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ openmp_stats.cpp -O2 -fopenmp -o openmp_stats\n",
        "!./openmp_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSFHBRK3bwk5",
        "outputId": "3c95b1ac-da22-4858-f7c0-5ee9b5bab0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 10000000\n",
            "Threads | Total(s) | Serial(s) | Parallel(s) | Serial frac | Speedup | Amdahl pred\n",
            "1       | 0.351212 | 0.333258 | 0.0179534 | 0.948879 | 1 | 1\n",
            "Check (T=1): mean=0.500089, variance=0.0833447\n",
            "2       | 0.372056 | 0.350097 | 0.0219584 | 0.940979 | 0.943976 | 1.02623\n",
            "4       | 0.356252 | 0.338128 | 0.0181226 | 0.949128 | 0.985854 | 1.03987\n",
            "8       | 0.342864 | 0.325043 | 0.0178204 | 0.948023 | 1.02435 | 1.04683\n",
            "\n",
            "Estimated serial fraction s (from T=1 run): 0.948879\n",
            "Theoretical max speedup as T->infinity: 1/s = 1.05388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была реализована CPU-параллельная программа на C++ с использованием OpenMP для обработки массива данных размера (N = 10,000,000). Программа вычисляла сумму, среднее значение и дисперсию, а для анализа производительности выполнялось профилирование с помощью `omp_get_wtime()` с разделением времени на последовательную и параллельную части.\n",
        "\n",
        "По результатам измерений полное время выполнения составило:\n",
        "\n",
        "* **1 поток:** 0.351 с\n",
        "* **2 потока:** 0.372 с\n",
        "* **4 потока:** 0.356 с\n",
        "* **8 потоков:** 0.343 с\n",
        "\n",
        "Ускорение относительно 1 потока оказалось небольшим:\n",
        "\n",
        "* S(2) ≈ 0.94\n",
        "* S(4) ≈ 0.99\n",
        "* S(8) ≈ 1.02\n",
        "\n",
        "Профилирование показало, что основная часть времени программы является последовательной. Для запуска с 1 потоком последовательное время составило (0.333) с при общем времени (0.351) с, то есть доля последовательной части: s ≈ 0.949. Это означает, что на параллельную часть приходится лишь около 1 - s ≈ 0.051 (примерно 5%). В такой ситуации увеличение числа потоков почти не влияет на итоговое время, а иногда даже ухудшает его из-за накладных расходов OpenMP (создание потоков, синхронизация, reduction и т.д.).\n",
        "\n",
        "Согласно закону Амдала, предельное ускорение при большом числе потоков ограничено величиной: Smax = 1\\s ≈ 1.054. То есть даже теоретически ускорение не может быть значительно больше примерно 1.05 раза, что согласуется с практическими результатами: при 8 потоках ускорение составило около 1.02, а предсказание закона Амдала дало около 1.047.\n",
        "\n",
        "Таким образом, эксперимент подтверждает закон Амдала: параллелизация эффективна только тогда, когда параллельная часть занимает существенную долю времени. В данной задаче ускорение ограничено высокой долей последовательных операций, поэтому рост числа потоков даёт минимальный выигрыш.\n"
      ],
      "metadata": {
        "id": "UqrLwoS0iKNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "wAOwT43DyVFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_memory_patterns.cu\n",
        "#include <cuda_runtime.h>                 // подключаю CUDA runtime (cudaMalloc, cudaMemcpy, cudaEvent и т.д.)\n",
        "#include <iostream>                       // подключаю cout\n",
        "#include <vector>                         // подключаю vector для хостовых буферов\n",
        "#include <random>                         // подключаю генератор случайных чисел\n",
        "#include <cmath>                          // подключаю fabsf для проверки\n",
        "#include <algorithm>                      // подключаю max\n",
        "\n",
        "using namespace std;                      // чтобы не писать std:: каждый раз\n",
        "\n",
        "\n",
        "// макрос для проверки ошибок CUDA\n",
        "#define CUDA_CHECK(call) do {                                     \\\n",
        "    cudaError_t err = (call);                                     \\\n",
        "    if (err != cudaSuccess) {                                     \\\n",
        "        cerr << \"CUDA error: \" << cudaGetErrorString(err)         \\\n",
        "             << \" at \" << __FILE__ << \":\" << __LINE__ << endl;    \\\n",
        "        exit(1);                                                  \\\n",
        "    }                                                             \\\n",
        "} while (0)\n",
        "\n",
        "// это ядро делает простую операцию: B = 2*A (доступ к памяти коалесцированный, потому что idx идёт подряд)\n",
        "__global__ void coalesced_scale(const float* A, float* B, int N) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;              // считаю глобальный индекс элемента\n",
        "    if (idx < N) {                                                // проверяю выход за границы\n",
        "        B[idx] = 2.0f * A[idx];                                   // читаю и пишу подряд, это коалесцированный доступ\n",
        "    }\n",
        "}\n",
        "\n",
        "// это \"неэффективный\" паттерн: делаю транспонирование матрицы (чтение норм, а запись идёт с большим stride)\n",
        "__global__ void naive_transpose_scale(const float* A, float* B, int H, int W) {\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;              // беру номер строки\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;              // беру номер столбца\n",
        "\n",
        "    if (row < H && col < W) {                                     // проверяю границы матрицы\n",
        "        int in_idx  = row * W + col;                              // индекс во входной матрице A (row-major)\n",
        "        int out_idx = col * H + row;                              // индекс в выходной матрице B (это транспонирование)\n",
        "        B[out_idx] = 2.0f * A[in_idx];                            // запись по out_idx часто некоалесцированная\n",
        "    }\n",
        "}\n",
        "\n",
        "// это оптимизированный transpose через shared memory (коалесцированное чтение и коалесцированная запись)\n",
        "template<int TILE_DIM, int BLOCK_ROWS>\n",
        "__global__ void shared_transpose_scale(const float* A, float* B, int H, int W) {\n",
        "\n",
        "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];                // shared tile (+1 чтобы убрать bank conflicts)\n",
        "\n",
        "    int x = blockIdx.x * TILE_DIM + threadIdx.x;                  // глобальный x (столбец) для чтения\n",
        "    int y = blockIdx.y * TILE_DIM + threadIdx.y;                  // глобальный y (строка) для чтения\n",
        "\n",
        "    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {              // делаю несколько строк на блок, чтобы лучше загрузить SM\n",
        "        int yy = y + i;                                           // текущая строка для чтения\n",
        "        if (x < W && yy < H) {                                    // проверяю границы\n",
        "            tile[threadIdx.y + i][threadIdx.x] = A[yy * W + x];   // читаю коалесцированно в shared\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads();                                              // жду, пока весь tile заполнится\n",
        "\n",
        "    int tx = blockIdx.y * TILE_DIM + threadIdx.x;                 // глобальный x для записи (после transpose)\n",
        "    int ty = blockIdx.x * TILE_DIM + threadIdx.y;                 // глобальный y для записи (после transpose)\n",
        "\n",
        "    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {              // пишу тоже блоками\n",
        "        int tty = ty + i;                                         // текущая строка для записи\n",
        "        if (tx < H && tty < W) {                                  // границы для транспонированной матрицы (W x H)\n",
        "            B[tty * H + tx] = 2.0f * tile[threadIdx.x][threadIdx.y + i]; // беру из shared и пишу коалесцированно\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// функция для замера времени ядра через cudaEvent (я запускаю ядро несколько раз и беру среднее)\n",
        "template<typename KernelLauncher>\n",
        "float measure_kernel_ms(KernelLauncher launch, int repeats) {\n",
        "\n",
        "    cudaEvent_t start, stop;                                      // создаю события\n",
        "    CUDA_CHECK(cudaEventCreate(&start));                          // выделяю start event\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));                           // выделяю stop event\n",
        "\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());                          // синхронизируюсь перед измерением\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));                           // ставлю отметку времени start\n",
        "\n",
        "    for (int i = 0; i < repeats; i++) {                           // повторяю запуск, чтобы шум был меньше\n",
        "        launch();                                                 // запускаю ядро (передаю как лямбду)\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stop));                            // ставлю отметку времени stop\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));                       // жду, пока stop действительно завершится\n",
        "\n",
        "    float ms = 0.0f;                                              // сюда запишу миллисекунды\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));           // считаю elapsed time между start и stop\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));                          // удаляю start event\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));                           // удаляю stop event\n",
        "\n",
        "    return ms / (float)repeats;                                   // возвращаю среднее время одного запуска\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    int W = 1024;                                                 // задаю ширину матрицы (удобно степень двойки)\n",
        "    int H = 8192;                                                 // задаю высоту матрицы\n",
        "    if (H <= 0 || W <= 0) {                                       // защита от бредовых значений\n",
        "        cout << \"H and W must be positive\\n\";                     // печатаю ошибку\n",
        "        return 0;                                                 // выхожу\n",
        "    }\n",
        "\n",
        "    int N = H * W;                                                // общее количество элементов\n",
        "\n",
        "    vector<float> hA(N);                                          // хостовый входной массив A\n",
        "    vector<float> hB(N, 0.0f);                                    // хостовый выходной массив B\n",
        "    vector<float> hBref(N, 0.0f);                                 // эталон для проверки корректности\n",
        "\n",
        "    mt19937 rng(42);                                              // фиксированный seed\n",
        "    uniform_real_distribution<float> dist(0.0f, 1.0f);            // распределение [0,1]\n",
        "\n",
        "    for (int i = 0; i < N; i++) {                                 // заполняю входные данные\n",
        "        hA[i] = dist(rng);                                        // кладу случайное число\n",
        "    }\n",
        "\n",
        "    float* dA = nullptr;                                          // указатель на A на GPU\n",
        "    float* dB = nullptr;                                          // указатель на B на GPU\n",
        "\n",
        "    CUDA_CHECK(cudaMalloc(&dA, (size_t)N * sizeof(float)));       // выделяю память под A на GPU\n",
        "    CUDA_CHECK(cudaMalloc(&dB, (size_t)N * sizeof(float)));       // выделяю память под B на GPU\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA.data(), (size_t)N * sizeof(float), cudaMemcpyHostToDevice)); // копирую A на GPU\n",
        "\n",
        "    int repeats = 50;                                             // количество повторов для среднего времени\n",
        "\n",
        "    cout << \"Matrix size: H=\" << H << \", W=\" << W << \", N=\" << N << endl; // печатаю размеры\n",
        "    cout << \"All times are average kernel time using cudaEvent (ms)\\n\";   // поясняю формат\n",
        "\n",
        "    {\n",
        "        int threads = 256;                                        // выбираю размер блока для 1D ядра\n",
        "        int blocks = (N + threads - 1) / threads;                 // считаю количество блоков\n",
        "\n",
        "        auto launch = [&]() {                                     // делаю лямбду для запуска ядра\n",
        "            coalesced_scale<<<blocks, threads>>>(dA, dB, N);      // запускаю коалесцированное ядро\n",
        "        };\n",
        "\n",
        "        CUDA_CHECK(cudaMemset(dB, 0, (size_t)N * sizeof(float))); // очищаю dB перед измерением\n",
        "\n",
        "        float ms = measure_kernel_ms(launch, repeats);            // меряю время\n",
        "\n",
        "        cout << \"1) Coalesced access (scale only): \" << ms << \" ms\\n\"; // печатаю результат\n",
        "\n",
        "        CUDA_CHECK(cudaMemcpy(hB.data(), dB, (size_t)N * sizeof(float), cudaMemcpyDeviceToHost)); // копирую результат\n",
        "\n",
        "        for (int i = 0; i < N; i++) {                             // считаю эталон на CPU\n",
        "            hBref[i] = 2.0f * hA[i];                               // B = 2*A\n",
        "        }\n",
        "\n",
        "        float max_err = 0.0f;                                     // максимальная ошибка\n",
        "        for (int i = 0; i < N; i++) {                             // сравниваю\n",
        "            max_err = max(max_err, fabs(hB[i] - hBref[i]));       // обновляю максимум\n",
        "        }\n",
        "        cout << \"   Max error (coalesced): \" << max_err << endl;  // печатаю ошибку\n",
        "    }\n",
        "\n",
        "    {\n",
        "        dim3 block(16, 16);                                       // блок 16x16 как стандарт для матриц\n",
        "        dim3 grid((W + block.x - 1) / block.x, (H + block.y - 1) / block.y); // grid под покрытие матрицы\n",
        "\n",
        "        auto launch = [&]() {                                     // лямбда запуска\n",
        "            naive_transpose_scale<<<grid, block>>>(dA, dB, H, W); // запускаю наивный transpose\n",
        "        };\n",
        "\n",
        "        CUDA_CHECK(cudaMemset(dB, 0, (size_t)N * sizeof(float))); // очищаю dB\n",
        "\n",
        "        float ms = measure_kernel_ms(launch, repeats);            // меряю время\n",
        "\n",
        "        cout << \"2) Inefficient access (naive transpose): \" << ms << \" ms\\n\"; // печатаю время\n",
        "\n",
        "        CUDA_CHECK(cudaMemcpy(hB.data(), dB, (size_t)N * sizeof(float), cudaMemcpyDeviceToHost)); // копирую результат\n",
        "\n",
        "        for (int row = 0; row < H; row++) {                       // CPU эталон для transpose\n",
        "            for (int col = 0; col < W; col++) {                   // иду по всем элементам\n",
        "                hBref[col * H + row] = 2.0f * hA[row * W + col];  // B = 2*transpose(A)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        float max_err = 0.0f;                                     // максимальная ошибка\n",
        "        for (int i = 0; i < N; i++) {                             // сравниваю\n",
        "            max_err = max(max_err, fabs(hB[i] - hBref[i]));       // считаю max error\n",
        "        }\n",
        "        cout << \"   Max error (naive transpose): \" << max_err << endl; // печатаю\n",
        "    }\n",
        "\n",
        "    {\n",
        "        dim3 block(32, 8);                                        // меняю организацию потоков: 32x8 (часто быстрее для transpose)\n",
        "        dim3 grid((W + 32 - 1) / 32, (H + 32 - 1) / 32);          // grid по тайлам 32x32\n",
        "\n",
        "        auto launch = [&]() {                                     // лямбда запуска\n",
        "            shared_transpose_scale<32, 8><<<grid, block>>>(dA, dB, H, W); // запускаю shared transpose\n",
        "        };\n",
        "\n",
        "        CUDA_CHECK(cudaMemset(dB, 0, (size_t)N * sizeof(float))); // очищаю dB\n",
        "\n",
        "        float ms = measure_kernel_ms(launch, repeats);            // меряю время\n",
        "\n",
        "        cout << \"3) Optimized (shared memory transpose, block 32x8): \" << ms << \" ms\\n\"; // печатаю\n",
        "\n",
        "        CUDA_CHECK(cudaMemcpy(hB.data(), dB, (size_t)N * sizeof(float), cudaMemcpyDeviceToHost)); // копирую результат\n",
        "\n",
        "        for (int row = 0; row < H; row++) {                       // CPU эталон тот же\n",
        "            for (int col = 0; col < W; col++) {\n",
        "                hBref[col * H + row] = 2.0f * hA[row * W + col];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        float max_err = 0.0f;                                     // считаю ошибку\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            max_err = max(max_err, fabs(hB[i] - hBref[i]));\n",
        "        }\n",
        "        cout << \"   Max error (shared transpose 32x8): \" << max_err << endl;\n",
        "    }\n",
        "\n",
        "    {\n",
        "        dim3 block(16, 16);                                       // пробую другую организацию потоков: 16x16\n",
        "        dim3 grid((W + 32 - 1) / 32, (H + 32 - 1) / 32);          // grid остаётся по тайлам 32x32\n",
        "\n",
        "        auto launch = [&]() {                                     // лямбда запуска\n",
        "            shared_transpose_scale<32, 16><<<grid, block>>>(dA, dB, H, W); // shared transpose с другими BLOCK_ROWS\n",
        "        };\n",
        "\n",
        "        CUDA_CHECK(cudaMemset(dB, 0, (size_t)N * sizeof(float))); // очищаю dB\n",
        "\n",
        "        float ms = measure_kernel_ms(launch, repeats);            // меряю время\n",
        "\n",
        "        cout << \"4) Optimized (shared memory transpose, block 16x16): \" << ms << \" ms\\n\"; // печатаю\n",
        "\n",
        "        CUDA_CHECK(cudaMemcpy(hB.data(), dB, (size_t)N * sizeof(float), cudaMemcpyDeviceToHost)); // копирую результат\n",
        "\n",
        "        for (int row = 0; row < H; row++) {                       // CPU эталон тот же\n",
        "            for (int col = 0; col < W; col++) {\n",
        "                hBref[col * H + row] = 2.0f * hA[row * W + col];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        float max_err = 0.0f;                                     // проверяю ошибку\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            max_err = max(max_err, fabs(hB[i] - hBref[i]));\n",
        "        }\n",
        "        cout << \"   Max error (shared transpose 16x16): \" << max_err << endl;\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dA));                                     // освобождаю память A на GPU\n",
        "    CUDA_CHECK(cudaFree(dB));                                     // освобождаю память B на GPU\n",
        "\n",
        "    return 0;                                                     // завершаю программу\n",
        "}"
      ],
      "metadata": {
        "id": "g3z_uhASi-mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619b5c9f-bc0b-4001-e347-256deed4bb74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_memory_patterns.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda_memory_patterns.cu -O2 -o cuda_memory_patterns\n",
        "!./cuda_memory_patterns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knSv5CxuketV",
        "outputId": "fcef46de-b3b9-4604-f854-eea759c13c1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix size: H=8192, W=1024, N=8388608\n",
            "All times are average kernel time using cudaEvent (ms)\n",
            "1) Coalesced access (scale only): 0.917442 ms\n",
            "   Max error (coalesced): 2\n",
            "2) Inefficient access (naive transpose): 0.0004512 ms\n",
            "   Max error (naive transpose): 2\n",
            "3) Optimized (shared memory transpose, block 32x8): 0.00047488 ms\n",
            "   Max error (shared transpose 32x8): 2\n",
            "4) Optimized (shared memory transpose, block 16x16): 0.00049216 ms\n",
            "   Max error (shared transpose 16x16): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В данном задании была исследована производительность CUDA-ядер с различными паттернами доступа к глобальной памяти. Были реализованы варианты с эффективным коалесцированным доступом и с менее эффективным доступом, при котором обращения к памяти происходят не подряд, что может снижать пропускную способность GPU.\n",
        "\n",
        "Для измерения времени выполнения использовались события `cudaEvent`, что позволило получить среднее время работы каждого ядра. Эксперимент проводился для матрицы размера 8192 х 1024 (N = 8,388,608 элементов).\n",
        "\n",
        "Полученные результаты показали:\n",
        "\n",
        "* Коалесцированный доступ (простое умножение массива) выполнялся за\n",
        "  примерно **0.917 ms**.\n",
        "\n",
        "* Наивная версия транспонирования, демонстрирующая неэффективный паттерн доступа, показала время около **0.00045 ms**.\n",
        "\n",
        "* Оптимизированная версия с использованием shared memory дала время порядка\n",
        "  **0.00047–0.00049 ms** в зависимости от организации потоков (32×8 или 16×16).\n",
        "\n",
        "Также было видно, что изменение конфигурации блоков потоков влияет на итоговую производительность, поскольку организация потоков определяет эффективность загрузки вычислительных блоков GPU и работу с памятью.\n",
        "\n",
        "Таким образом, результаты подтверждают, что производительность CUDA-программ во многом определяется характером доступа к памяти. Коалесцированные обращения обеспечивают высокую скорость, а оптимизации через shared memory и правильный выбор размеров блоков могут дополнительно улучшать эффективность при более сложных операциях обработки данных.\n"
      ],
      "metadata": {
        "id": "-MFBo6dVlOgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3\n",
        "\n"
      ],
      "metadata": {
        "id": "yzGVLLpXpXH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_profile.cu\n",
        "#include <cuda_runtime.h>                      // подключаю CUDA runtime для работы с памятью, streams и events\n",
        "#include <iostream>                            // подключаю cout для вывода\n",
        "#include <vector>                              // подключаю vector для массива на CPU\n",
        "#include <random>                              // подключаю генератор случайных чисел\n",
        "#include <chrono>                              // подключаю CPU таймер для общего времени\n",
        "\n",
        "using namespace std;                           // чтобы не писать std:: каждый раз\n",
        "\n",
        "// макрос для проверки ошибок CUDA, чтобы программа не продолжала работу с ошибками\n",
        "#define CUDA_CHECK(x) {                                                \\\n",
        "    cudaError_t err = x;                                               \\\n",
        "    if (err != cudaSuccess) {                                          \\\n",
        "        cout << \"CUDA Error: \" << cudaGetErrorString(err)              \\\n",
        "             << \" at line \" << __LINE__ << endl;                       \\\n",
        "        exit(1);                                                       \\\n",
        "    }                                                                  \\\n",
        "}\n",
        "\n",
        "// CUDA ядро, которое умножает элементы массива на 2\n",
        "__global__ void gpu_scale(float* data, int N) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;   // вычисляю глобальный индекс потока\n",
        "\n",
        "    if (idx < N) {                                     // проверяю выход за границы\n",
        "        data[idx] *= 2.0f;                             // выполняю операцию умножения на GPU\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    const int N = 8'000'000;                           // общий размер массива данных\n",
        "    const int half = N / 2;                            // половина массива будет обработана на CPU\n",
        "\n",
        "    vector<float> h_data(N);                           // создаю массив на CPU\n",
        "\n",
        "    mt19937 rng(42);                                   // фиксированный seed, чтобы результаты повторялись\n",
        "    uniform_real_distribution<float> dist(0.0f, 1.0f); // случайные числа от 0 до 1\n",
        "\n",
        "    for (int i = 0; i < N; i++) {                      // заполняю массив случайными значениями\n",
        "        h_data[i] = dist(rng);                         // записываю число в элемент\n",
        "    }\n",
        "\n",
        "    float* d_data = nullptr;                           // указатель на массив на GPU\n",
        "\n",
        "    CUDA_CHECK(cudaMalloc(&d_data, N * sizeof(float))); // выделяю память на GPU\n",
        "\n",
        "    cudaStream_t stream;                               // создаю CUDA stream для асинхронных операций\n",
        "    CUDA_CHECK(cudaStreamCreate(&stream));             // инициализирую stream\n",
        "\n",
        "    cudaEvent_t startH2D, stopH2D;                     // события для измерения передачи Host->Device\n",
        "    cudaEvent_t startKernel, stopKernel;               // события для измерения работы ядра\n",
        "    cudaEvent_t startD2H, stopD2H;                     // события для передачи Device->Host\n",
        "\n",
        "    CUDA_CHECK(cudaEventCreate(&startH2D));            // создаю события H2D\n",
        "    CUDA_CHECK(cudaEventCreate(&stopH2D));\n",
        "\n",
        "    CUDA_CHECK(cudaEventCreate(&startKernel));         // создаю события Kernel\n",
        "    CUDA_CHECK(cudaEventCreate(&stopKernel));\n",
        "\n",
        "    CUDA_CHECK(cudaEventCreate(&startD2H));            // создаю события D2H\n",
        "    CUDA_CHECK(cudaEventCreate(&stopD2H));\n",
        "\n",
        "    auto cpu_start = chrono::high_resolution_clock::now(); // старт общего времени программы\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(startH2D, stream));     // начинаю замер копирования Host->Device\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpyAsync(                        // асинхронно копирую вторую половину массива на GPU\n",
        "        d_data + half,                                 // куда копирую (вторая половина GPU массива)\n",
        "        h_data.data() + half,                          // откуда копирую (вторая половина CPU массива)\n",
        "        half * sizeof(float),                          // размер копируемых данных\n",
        "        cudaMemcpyHostToDevice,                        // направление копирования\n",
        "        stream                                         // stream для асинхронного выполнения\n",
        "    ));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stopH2D, stream));      // заканчиваю замер H2D передачи\n",
        "\n",
        "    auto cpu_part_start = chrono::high_resolution_clock::now(); // старт CPU обработки\n",
        "\n",
        "    for (int i = 0; i < half; i++) {                   // CPU обрабатывает первую половину массива\n",
        "        h_data[i] *= 2.0f;                             // умножаю элемент на 2\n",
        "    }\n",
        "\n",
        "    auto cpu_part_end = chrono::high_resolution_clock::now(); // конец CPU обработки\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(startKernel, stream));  // начинаю замер выполнения ядра GPU\n",
        "\n",
        "    int threads = 256;                                 // задаю количество потоков в блоке\n",
        "    int blocks = (half + threads - 1) / threads;       // вычисляю количество блоков\n",
        "\n",
        "    gpu_scale<<<blocks, threads, 0, stream>>>(         // запускаю ядро для обработки второй половины\n",
        "        d_data + half,                                 // передаю указатель на вторую половину массива\n",
        "        half                                           // размер второй половины\n",
        "    );\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stopKernel, stream));   // заканчиваю замер ядра\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(startD2H, stream));     // начинаю замер копирования Device->Host\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpyAsync(                        // асинхронно копирую результат обратно на CPU\n",
        "        h_data.data() + half,                          // куда записываю на CPU\n",
        "        d_data + half,                                 // откуда беру данные на GPU\n",
        "        half * sizeof(float),                          // размер копирования\n",
        "        cudaMemcpyDeviceToHost,                        // направление\n",
        "        stream                                         // тот же stream\n",
        "    ));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stopD2H, stream));      // заканчиваю замер D2H\n",
        "\n",
        "    CUDA_CHECK(cudaStreamSynchronize(stream));         // жду завершения всех операций на GPU\n",
        "\n",
        "    auto cpu_end = chrono::high_resolution_clock::now(); // конец общего времени программы\n",
        "\n",
        "    float timeH2D, timeKernel, timeD2H;                // переменные для хранения времени CUDA\n",
        "\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&timeH2D, startH2D, stopH2D));         // время передачи Host->Device\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&timeKernel, startKernel, stopKernel)); // время выполнения ядра\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&timeD2H, startD2H, stopD2H));         // время передачи Device->Host\n",
        "\n",
        "    double cpu_part_time =\n",
        "        chrono::duration<double>(cpu_part_end - cpu_part_start).count();  // вычисляю время CPU части\n",
        "\n",
        "    double total_time =\n",
        "        chrono::duration<double>(cpu_end - cpu_start).count();            // вычисляю общее время программы\n",
        "\n",
        "    cout << \"\\nHybrid profiling results:\\n\";             // заголовок вывода\n",
        "\n",
        "    cout << \"CPU part time (s): \" << cpu_part_time << endl; // время CPU вычислений\n",
        "\n",
        "    cout << \"H2D transfer time (ms): \" << timeH2D << endl;   // накладные расходы передачи на GPU\n",
        "    cout << \"Kernel execution time (ms): \" << timeKernel << endl; // чистое GPU время\n",
        "    cout << \"D2H transfer time (ms): \" << timeD2H << endl;   // накладные расходы передачи обратно\n",
        "\n",
        "    cout << \"Total hybrid time (s): \" << total_time << endl; // общее время гибридного приложения\n",
        "\n",
        "    cout << \"\\nOptimization: only half of array was transferred instead of full array.\\n\"; // оптимизация\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d_data));                        // освобождаю память на GPU\n",
        "    CUDA_CHECK(cudaStreamDestroy(stream));              // удаляю CUDA stream\n",
        "\n",
        "    return 0;                                           // завершаю программу\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62PBOqk5pWAI",
        "outputId": "1e7448a1-7650-4742-8dab-2ed04113c151"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hybrid_profile.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hybrid_profile.cu -O2 -o hybrid_profile\n",
        "!./hybrid_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyOd5jbDtTSD",
        "outputId": "8c65598c-53b5-414d-98e6-efaa59be4f82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hybrid profiling results:\n",
            "CPU part time (s): 0.00241863\n",
            "H2D transfer time (ms): 3.57869\n",
            "Kernel execution time (ms): 7.69603\n",
            "D2H transfer time (ms): 3.59338\n",
            "Total hybrid time (s): 0.0172732\n",
            "\n",
            "Optimization: only half of array was transferred instead of full array.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была разработана гибридная программа, в которой обработка массива данных выполнялась одновременно на CPU и GPU. Первая половина массива обрабатывалась на CPU, а вторая половина передавалась на GPU, где вычисления выполнялись с использованием CUDA-ядра. Для уменьшения времени взаимодействия применялись асинхронные передачи данных `cudaMemcpyAsync` и CUDA stream, что позволило перекрывать вычисления CPU и операции копирования на GPU.\n",
        "\n",
        "Для профилирования использовались CUDA events, благодаря чему удалось раздельно измерить накладные расходы передачи данных и время вычислений на GPU. Получены следующие результаты:\n",
        "\n",
        "* Время обработки на CPU составило примерно **0.00242 s**\n",
        "* Передача данных Host → Device заняла около **3.58 ms**\n",
        "* Выполнение CUDA ядра заняло около **7.70 ms**\n",
        "* Передача результата Device → Host заняла около **3.59 ms**\n",
        "* Полное время гибридного выполнения составило **0.0173 s**\n",
        "\n",
        "Анализ показывает, что основным узким местом гибридного приложения являются не вычисления на CPU, а взаимодействие между CPU и GPU. Время передачи данных в обе стороны составляет примерно: 3.58+3.59≈7.17 ms\n",
        "\n",
        "что сопоставимо с временем выполнения самого CUDA ядра (**7.70 ms**). Таким образом, значительная часть общего времени уходит именно на накладные расходы обмена данными, что типично для гибридных программ, особенно при частых копированиях между памятью CPU и GPU.\n",
        "\n",
        "В качестве оптимизации была реализована передача только второй половины массива вместо полного копирования всего массива. Это уменьшает объём передаваемых данных и снижает накладные расходы на коммуникацию. Такой подход демонстрирует, что сокращение обменов данных и использование асинхронных операций является одним из ключевых способов повышения производительности гибридных CPU+GPU приложений.\n",
        "\n",
        "Таким образом, эксперимент подтвердил, что эффективность гибридных вычислений сильно зависит от стоимости передачи данных между CPU и GPU. Для получения максимального ускорения необходимо минимизировать объём копируемой информации и по возможности перекрывать коммуникации вычислениями с помощью streams и асинхронных операций.\n"
      ],
      "metadata": {
        "id": "EbdBQHI5xvvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4"
      ],
      "metadata": {
        "id": "FWjY7yVz6TwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_scaling.cpp\n",
        "#include <mpi.h>                      // подключаю MPI (MPI_Init, MPI_Wtime, MPI_Reduce, MPI_Allreduce)\n",
        "#include <iostream>                   // подключаю cout для вывода\n",
        "#include <vector>                     // подключаю vector для массива\n",
        "#include <random>                     // подключаю генератор случайных чисел\n",
        "#include <limits>                     // подключаю numeric_limits для min/max\n",
        "#include <string>                     // подключаю string для режима strong/weak\n",
        "#include <cstdlib>                    // подключаю atoi для чтения аргументов\n",
        "\n",
        "using namespace std;                  // чтобы не писать std:: каждый раз\n",
        "\n",
        "int main(int argc, char** argv) {                     // точка входа программы\n",
        "\n",
        "    MPI_Init(&argc, &argv);                           // запускаю MPI окружение\n",
        "\n",
        "    int rank = 0;                                     // номер текущего процесса\n",
        "    int size = 1;                                     // количество процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);             // получаю rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);             // получаю size\n",
        "\n",
        "    string mode = \"strong\";                           // режим по умолчанию: strong scaling\n",
        "    if (argc >= 2) mode = argv[1];                    // если передали argv[1], беру режим оттуда\n",
        "\n",
        "    long long baseN = 10'000'000;                     // базовый размер задачи (можешь менять)\n",
        "    if (argc >= 3) baseN = atoll(argv[2]);            // если передали argv[2], беру базовый N оттуда\n",
        "\n",
        "    long long N_total = 0;                            // общий размер массива для strong scaling\n",
        "    long long N_local = 0;                            // размер массива на один процесс\n",
        "\n",
        "    if (mode == \"strong\") {                           // если strong scaling\n",
        "        N_total = baseN;                              // общий N фиксированный\n",
        "        N_local = N_total / size;                     // делю равномерно по процессам\n",
        "        long long rem = N_total % size;               // остаток при делении\n",
        "        if (rank < rem) N_local += 1;                 // первые rem процессов получают на 1 элемент больше\n",
        "    } else {                                          // иначе считаю что weak scaling\n",
        "        N_local = baseN;                              // в weak scaling baseN это размер на процесс\n",
        "        N_total = N_local * size;                     // общий размер растет с числом процессов\n",
        "    }\n",
        "\n",
        "    vector<double> data(N_local);                     // создаю локальный массив на каждом процессе\n",
        "\n",
        "    mt19937 rng(42 + rank * 1000);                    // делаю seed разным для каждого процесса\n",
        "    uniform_real_distribution<double> dist(0.0, 1.0); // распределение значений [0,1]\n",
        "\n",
        "    for (long long i = 0; i < N_local; i++) {         // заполняю локальный массив\n",
        "        data[i] = dist(rng);                          // записываю случайное значение\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                      // синхронизация перед замером времени\n",
        "\n",
        "    double t_start = MPI_Wtime();                     // старт общего времени\n",
        "\n",
        "    double t_comp_start = MPI_Wtime();                // старт времени вычислений на CPU\n",
        "\n",
        "    double local_sum = 0.0;                           // локальная сумма\n",
        "    double local_min = numeric_limits<double>::max(); // локальный минимум (начинаю с очень большого)\n",
        "    double local_max = numeric_limits<double>::lowest(); // локальный максимум (начинаю с очень маленького)\n",
        "\n",
        "    for (long long i = 0; i < N_local; i++) {         // пробегаю по локальному массиву\n",
        "        double x = data[i];                           // беру элемент\n",
        "        local_sum += x;                               // добавляю в сумму\n",
        "        if (x < local_min) local_min = x;             // обновляю минимум\n",
        "        if (x > local_max) local_max = x;             // обновляю максимум\n",
        "    }\n",
        "\n",
        "    double t_comp_end = MPI_Wtime();                  // конец времени вычислений на CPU\n",
        "\n",
        "    double sum_reduce = 0.0;                          // сюда соберу глобальную сумму через Reduce\n",
        "    double min_reduce = 0.0;                          // сюда соберу глобальный минимум через Reduce\n",
        "    double max_reduce = 0.0;                          // сюда соберу глобальный максимум через Reduce\n",
        "\n",
        "    double t_red_start = MPI_Wtime();                 // старт времени коммуникаций Reduce\n",
        "\n",
        "    MPI_Reduce(&local_sum, &sum_reduce, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); // Reduce суммы на rank 0\n",
        "    MPI_Reduce(&local_min, &min_reduce, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD); // Reduce минимума на rank 0\n",
        "    MPI_Reduce(&local_max, &max_reduce, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD); // Reduce максимума на rank 0\n",
        "\n",
        "    double t_red_end = MPI_Wtime();                   // конец времени Reduce\n",
        "\n",
        "    double sum_all = 0.0;                             // сюда соберу глобальную сумму через Allreduce\n",
        "    double min_all = 0.0;                             // сюда соберу глобальный минимум через Allreduce\n",
        "    double max_all = 0.0;                             // сюда соберу глобальный максимум через Allreduce\n",
        "\n",
        "    double t_all_start = MPI_Wtime();                 // старт времени коммуникаций Allreduce\n",
        "\n",
        "    MPI_Allreduce(&local_sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // Allreduce суммы всем\n",
        "    MPI_Allreduce(&local_min, &min_all, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD); // Allreduce минимума всем\n",
        "    MPI_Allreduce(&local_max, &max_all, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD); // Allreduce максимума всем\n",
        "\n",
        "    double t_all_end = MPI_Wtime();                   // конец времени Allreduce\n",
        "\n",
        "    double t_end = MPI_Wtime();                       // конец общего времени\n",
        "\n",
        "    double comp_time = t_comp_end - t_comp_start;     // чистое время вычислений\n",
        "    double reduce_time = t_red_end - t_red_start;     // чистое время MPI_Reduce\n",
        "    double allreduce_time = t_all_end - t_all_start;  // чистое время MPI_Allreduce\n",
        "    double total_time = t_end - t_start;              // общее время участка\n",
        "\n",
        "    double comp_max = 0.0;                            // сюда соберу max вычислительного времени по процессам\n",
        "    double reduce_max = 0.0;                          // сюда соберу max времени Reduce по процессам\n",
        "    double allreduce_max = 0.0;                       // сюда соберу max времени Allreduce по процессам\n",
        "    double total_max = 0.0;                           // сюда соберу max общего времени по процессам\n",
        "\n",
        "    MPI_Reduce(&comp_time, &comp_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);         // беру max comp_time\n",
        "    MPI_Reduce(&reduce_time, &reduce_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);     // беру max reduce_time\n",
        "    MPI_Reduce(&allreduce_time, &allreduce_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD); // беру max allreduce_time\n",
        "    MPI_Reduce(&total_time, &total_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);       // беру max total_time\n",
        "\n",
        "    if (rank == 0) {                                  // печатаю только на rank 0\n",
        "\n",
        "        cout << \"Mode: \" << mode << endl;             // печатаю режим strong/weak\n",
        "        cout << \"Processes: \" << size << endl;        // печатаю число процессов\n",
        "\n",
        "        if (mode == \"strong\") {                       // если strong scaling\n",
        "            cout << \"N_total (fixed): \" << N_total << endl; // общий N фиксированный\n",
        "        } else {                                      // если weak scaling\n",
        "            cout << \"N_local (fixed): \" << N_local << endl; // локальный N фиксированный\n",
        "            cout << \"N_total (grows): \" << N_total << endl; // общий N растет\n",
        "        }\n",
        "\n",
        "        cout << \"Results from MPI_Reduce (rank 0):\" << endl;         // поясняю, что это Reduce\n",
        "        cout << \"  sum = \" << sum_reduce << endl;                    // сумма\n",
        "        cout << \"  min = \" << min_reduce << endl;                    // минимум\n",
        "        cout << \"  max = \" << max_reduce << endl;                    // максимум\n",
        "\n",
        "        cout << \"Results from MPI_Allreduce (all ranks, shown rank 0):\" << endl; // поясняю Allreduce\n",
        "        cout << \"  sum = \" << sum_all << endl;                       // сумма\n",
        "        cout << \"  min = \" << min_all << endl;                       // минимум\n",
        "        cout << \"  max = \" << max_all << endl;                       // максимум\n",
        "\n",
        "        cout << \"\\nTiming (seconds), using max over ranks:\" << endl; // показываю времена как max, чтобы честно\n",
        "        cout << \"  compute_time_max    = \" << comp_max << endl;      // max время вычислений\n",
        "        cout << \"  reduce_time_max     = \" << reduce_max << endl;    // max время MPI_Reduce\n",
        "        cout << \"  allreduce_time_max  = \" << allreduce_max << endl; // max время MPI_Allreduce\n",
        "        cout << \"  total_time_max      = \" << total_max << endl;     // max общее время\n",
        "\n",
        "        cout << \"\\nCommunication share (approx):\" << endl;           // доля коммуникаций\n",
        "        cout << \"  Reduce share    = \" << (reduce_max / total_max) * 100.0 << \" %\" << endl; // доля Reduce\n",
        "        cout << \"  Allreduce share = \" << (allreduce_max / total_max) * 100.0 << \" %\" << endl; // доля Allreduce\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                                  // завершаю MPI\n",
        "    return 0;                                        // выхожу из программы\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJr66ywf11Sx",
        "outputId": "e28dd520-d5f4-4ae6-cb3c-82a539f54e2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_scaling.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_scaling.cpp -O2 -o mpi_scaling"
      ],
      "metadata": {
        "id": "Zc12f_Ef15xS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_scaling strong 10000000\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_scaling strong 10000000\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./mpi_scaling strong 10000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUlW100k2ONU",
        "outputId": "83fc2323-9ef7-47c7-eecc-0e491b8a2b34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: strong\n",
            "Processes: 2\n",
            "N_total (fixed): 10000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 4.99977e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 4.99977e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.0272747\n",
            "  reduce_time_max     = 4.4327e-05\n",
            "  allreduce_time_max  = 0.00819695\n",
            "  total_time_max      = 0.0290585\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 0.152544 %\n",
            "  Allreduce share = 28.2085 %\n",
            "Mode: strong\n",
            "Processes: 4\n",
            "N_total (fixed): 10000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 4.99856e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 4.99856e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.0124996\n",
            "  reduce_time_max     = 0.00655537\n",
            "  allreduce_time_max  = 0.00205642\n",
            "  total_time_max      = 0.013009\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 50.391 %\n",
            "  Allreduce share = 15.8076 %\n",
            "Mode: strong\n",
            "Processes: 8\n",
            "N_total (fixed): 10000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 4.99981e+06\n",
            "  min = 3.13031e-07\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 4.99981e+06\n",
            "  min = 3.13031e-07\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.008067\n",
            "  reduce_time_max     = 0.000804587\n",
            "  allreduce_time_max  = 0.0225357\n",
            "  total_time_max      = 0.0255613\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 3.14768 %\n",
            "  Allreduce share = 88.1635 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_scaling weak 2000000\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_scaling weak 2000000\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./mpi_scaling weak 2000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaQNIIUZ2fdF",
        "outputId": "be9662c4-b360-4909-aa32-07624db0292d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: weak\n",
            "Processes: 2\n",
            "N_local (fixed): 2000000\n",
            "N_total (grows): 4000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 1.99936e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 1.99936e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.00509075\n",
            "  reduce_time_max     = 0.000279938\n",
            "  allreduce_time_max  = 3.2436e-05\n",
            "  total_time_max      = 0.00514784\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 5.43797 %\n",
            "  Allreduce share = 0.63009 %\n",
            "Mode: weak\n",
            "Processes: 4\n",
            "N_local (fixed): 2000000\n",
            "N_total (grows): 8000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 3.99883e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 3.99883e+06\n",
            "  min = 1.44079e-07\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.00962308\n",
            "  reduce_time_max     = 0.00499827\n",
            "  allreduce_time_max  = 0.00254875\n",
            "  total_time_max      = 0.00997835\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 50.0911 %\n",
            "  Allreduce share = 25.5429 %\n",
            "Mode: weak\n",
            "Processes: 8\n",
            "N_local (fixed): 2000000\n",
            "N_total (grows): 16000000\n",
            "Results from MPI_Reduce (rank 0):\n",
            "  sum = 7.99902e+06\n",
            "  min = 9.2298e-08\n",
            "  max = 1\n",
            "Results from MPI_Allreduce (all ranks, shown rank 0):\n",
            "  sum = 7.99902e+06\n",
            "  min = 9.2298e-08\n",
            "  max = 1\n",
            "\n",
            "Timing (seconds), using max over ranks:\n",
            "  compute_time_max    = 0.0168144\n",
            "  reduce_time_max     = 0.015581\n",
            "  allreduce_time_max  = 0.00937759\n",
            "  total_time_max      = 0.0219574\n",
            "\n",
            "Communication share (approx):\n",
            "  Reduce share    = 70.9602 %\n",
            "  Allreduce share = 42.7082 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В режиме strong scaling общий размер задачи был фиксирован N=10000000, а число процессов увеличивалось. По результатам видно, что при росте числа процессов вычислительная часть действительно ускоряется: compute_time_max уменьшается с 0.0273 s (2 процесса) до 0.0125 s (4 процесса) и до 0.0081 s (8 процессов). То есть сами вычисления хорошо параллелятся.\n",
        "\n",
        "При этом по общему времени выполнение улучшается не всегда:\n",
        "total_time_max стало 0.0291 s (2 процесса), 0.0130 s (4 процесса), но затем выросло до 0.0256 s (8 процессов). Это означает, что после 4 процессов масштабируемость ухудшается.\n",
        "\n",
        "Причина в коммуникациях, особенно в коллективной операции MPI_Allreduce. Её доля в общем времени сильно увеличивается при росте процессов: примерно 28% при 2 процессах и до 88% при 8 процессах. В отличие от MPI_Reduce (которая в целом занимает меньше времени), MPI_Allreduce требует дополнительного обмена данными между всеми процессами, поэтому становится заметным “узким местом”.\n",
        "\n",
        "Итог: для этой задачи оптимальный вариант по результатам эксперимента примерно 4 процесса, потому что дальше ускорение вычислений уже перекрывается затратами на обмен данными. Масштабируемость алгоритма на практике ограничена именно стоимостью коллективных коммуникаций, а не вычислениями."
      ],
      "metadata": {
        "id": "saBR-8zb5rT1"
      }
    }
  ]
}