{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Practical work 3**\n",
        "Rakhimberdina Aruzhan ADA-2401M"
      ],
      "metadata": {
        "id": "6FFubU0RTPJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXEfTLvjbLIJ",
        "outputId": "c1b48851-c547-42ae-85b0-3d54b112e851"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan  3 18:44:07 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4NKpWDFbIGj",
        "outputId": "165cf2b4-31b9-4a83-f4d5-7a99823e681c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sorts.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile sorts.cu\n",
        "#include <iostream>                 // Ввод/вывод в консоль (cout)\n",
        "#include <vector>                   // Контейнер vector для массива на CPU\n",
        "#include <algorithm>                // sort, stable_sort, make_heap, sort_heap\n",
        "#include <random>                   // Генерация случайных чисел\n",
        "#include <chrono>                   // Замер времени на CPU\n",
        "#include <climits>                  // INT_MAX, INT_MIN (удобно для границ)\n",
        "#include <cuda_runtime.h>           // CUDA Runtime API (cudaEvent, cudaDeviceSynchronize)\n",
        "#include <iomanip>                  // Красивый вывод (setw, setprecision)\n",
        "\n",
        "#include <thrust/device_vector.h>   // thrust::device_vector = массив в памяти GPU\n",
        "#include <thrust/sort.h>            // thrust::sort = параллельная сортировка на GPU\n",
        "\n",
        "using namespace std;                // Чтобы не писать std:: постоянно\n",
        "\n",
        "// Макрос для проверки CUDA ошибок: если что-то пошло не так - печатаем и выходим\n",
        "#define CUDA_CHECK(x) do { \\\n",
        "  cudaError_t e = (x); \\\n",
        "  if (e != cudaSuccess) { \\\n",
        "    cout << \"CUDA error: \" << cudaGetErrorString(e) \\\n",
        "         << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "static const int THREADS = 256;     // Сколько потоков в одном CUDA-блоке для merge kernel\n",
        "static const int CHUNK   = 1024;    // Размер чанка: сначала сортируем кусочки по 1024 элемента\n",
        "\n",
        "// mergePath - ключевая идея параллельного слияния:\n",
        "// каждый поток независимо вычисляет свой элемент merged-результата.\n",
        "\n",
        "// Мы хотим слить два отсортированных массива A и B в один отсортированный.\n",
        "// В обычном merge один указатель двигается после другого (зависимости).\n",
        "// На GPU так нельзя, поэтому каждый поток сам считает свой элемент слияния:\n",
        "// для позиции k (в итоговом массиве) мы ищем, сколько элементов взять из A (i),\n",
        "// а из B будет j = k - i. Это и делает mergePath через бинарный поиск.\n",
        "\n",
        "\n",
        "__device__ __forceinline__ int mergePath(const int* A, int m, const int* B, int n, int k) {\n",
        "    int lo = max(0, k - n);         // Минимально возможное i (если из B максимум n)\n",
        "    int hi = min(k, m);             // Максимально возможное i (не больше m и не больше k)\n",
        "\n",
        "    while (lo < hi) {               // Бинарный поиск по i\n",
        "        int i = (lo + hi) >> 1;     // Пробуем взять i элементов из A\n",
        "        int j = k - i;              // Тогда из B берём j, чтобы суммарно было k\n",
        "\n",
        "        // Берём \"крайние\" значения вокруг разбиения\n",
        "        // Если индекс вылез - подставляем +- бесконечность, чтобы сравнения работали\n",
        "        int Ai   = (i < m) ? A[i]   : INT_MAX; // A[i] или +∞, если вышли за границу\n",
        "        int Aim1 = (i > 0) ? A[i-1] : INT_MIN; // A[i-1] или -∞, если i==0\n",
        "        int Bj   = (j < n) ? B[j]   : INT_MAX; // B[j] или +∞\n",
        "        int Bjm1 = (j > 0) ? B[j-1] : INT_MIN; // B[j-1] или -∞, если j==0\n",
        "\n",
        "        // Проверяем, правильное ли разбиение:\n",
        "        // - если A[i] слишком маленький относительно B[j-1], то i надо увеличить\n",
        "        if (Ai < Bjm1) lo = i + 1;\n",
        "        // - если A[i-1] слишком большой относительно B[j], то i надо уменьшить\n",
        "        else if (Aim1 > Bj) hi = i;\n",
        "        // - иначе всё ок, нашли точку разбиения\n",
        "        else return i;\n",
        "    }\n",
        "    return lo;                      // Итоговое i\n",
        "}\n",
        "\n",
        "// mergePass - один этап merge sort на GPU:\n",
        "// сливаем пары отсортированных сегментов длины width в выходной буфер out.\n",
        "\n",
        "// Один проход слияния в merge sort:\n",
        "// берём пары сегментов длины width: [base..base+width) и [base+width..base+2*width)\n",
        "// и пишем их слияние в out.\n",
        "\n",
        "__global__ void mergePass(const int* in, int* out, int N, int width) {\n",
        "    int pairId = blockIdx.x;               // Какая пара сегментов (по оси X)\n",
        "    int base = pairId * (2 * width);       // Начало пары: base\n",
        "\n",
        "  // Реальные длины сегментов (на последней паре может быть \"хвост\")\n",
        "    int m = max(0, min(width, N - base));                 // Реальная длина левого сегмента\n",
        "    int n = max(0, min(width, N - (base + width)));       // Реальная длина правого сегмента\n",
        "    int outLen = m + n;                                   // Сколько элементов надо слить\n",
        "\n",
        "    // blockIdx.y используется, чтобы покрыть много k в одном сегменте (если он большой)\n",
        "    int k = blockIdx.y * blockDim.x + threadIdx.x;        // Позиция k в merged сегменте\n",
        "    if (k >= outLen) return;                              // Если поток вышел за outLen — ничего не делаем\n",
        "\n",
        "    const int* A = in + base;                             // Левый сегмент\n",
        "    const int* B = in + base + width;                     // Правый сегмент\n",
        "\n",
        "    int i = mergePath(A, m, B, n, k);                     // Сколько взять из A\n",
        "    int j = k - i;                                        // Сколько взять из B\n",
        "\n",
        "    int a = (i < m) ? A[i] : INT_MAX;                     // Кандидат из A\n",
        "    int b = (j < n) ? B[j] : INT_MAX;                     // Кандидат из B\n",
        "\n",
        "    out[base + k] = (a <= b) ? a : b;                     // В позицию k кладём меньший элемент\n",
        "}\n",
        "\n",
        "// GPU merge sort по заданию:\n",
        "\n",
        "// Шаг 1: делим массив на блоки (чанки) и сортируем их параллельно.\n",
        "// Здесь я использую thrust::sort для каждого чанка - это быстрая GPU сортировка.\n",
        "//\n",
        "// Шаг 2: дальше делаем попарные слияния kernel-ом mergePass (width удваивается): сначала сливаем чанки по 1024,\n",
        "// потом по 2048, потом по 4096 и т.д., пока не отсортируем весь массив.\n",
        "\n",
        "\n",
        "static void gpuMergeSort(vector<int>& h) {\n",
        "    int N = (int)h.size();                                // Размер массива\n",
        "\n",
        "    thrust::device_vector<int> d = h;                     // Копируем данные в память GPU\n",
        "    thrust::device_vector<int> buf(N);                    // Буфер для merge (ping-pong)\n",
        "\n",
        "    // Шаг 1: сортировка чанков (каждый чанк - отдельный кусок, сортируем независимо)\n",
        "    for (int start = 0; start < N; start += CHUNK) {       // Идём по массиву по чанкам\n",
        "        int end = min(start + CHUNK, N);                   // Конец чанка (не выходим за N)\n",
        "        thrust::sort(d.begin() + start, d.begin() + end);  // Сортируем этот чанк на GPU\n",
        "    }\n",
        "\n",
        "    int width = CHUNK;                                     // Длина текущих отсортированных сегментов\n",
        "    bool ping = true;                                      // Переключатель буферов (d -> buf -> d ...)\n",
        "\n",
        "    // Шаг 2: слияние по парам, пока не отсортируем весь массив\n",
        "    while (width < N) {\n",
        "        int numPairs = (N + (2 * width) - 1) / (2 * width); // Сколько пар сегментов надо слить\n",
        "        int maxOutLen = min(2 * width, N);                  // Максимум элементов в одном merged сегменте\n",
        "        int blocksY = (maxOutLen + THREADS - 1) / THREADS;  // Сколько блоков по Y, чтобы покрыть k\n",
        "\n",
        "        dim3 block(THREADS);                                 // THREADS потоков в блоке\n",
        "        dim3 grid(numPairs, blocksY);                        // X=пары, Y=группа блоков для k\n",
        "\n",
        "        const int* in  = thrust::raw_pointer_cast((ping ? d : buf).data());   // Откуда читаем\n",
        "        int* out       = thrust::raw_pointer_cast((ping ? buf : d).data());  // Куда пишем\n",
        "\n",
        "        mergePass<<<grid, block>>>(in, out, N, width);        // Запускаем merge kernel\n",
        "        CUDA_CHECK(cudaGetLastError());                       // Проверяем ошибки запуска kernel\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());                  // Ждём завершения (для корректности времени и шагов)\n",
        "\n",
        "        ping = !ping;                                         // Меняем буферы местами\n",
        "        width *= 2;                                           // После merge сегменты удваиваются\n",
        "    }\n",
        "\n",
        "    if (!ping) d.swap(buf);                                   // Если итог оказался в buf — меняем местами\n",
        "\n",
        "    thrust::copy(d.begin(), d.end(), h.begin());              // Копируем итог с GPU обратно на CPU\n",
        "}\n",
        "\n",
        "// GPU quick sort: используем thrust::sort как готовую параллельную сортировку на GPU\n",
        "static void gpuQuickSort(vector<int>& h) {\n",
        "    thrust::device_vector<int> d = h;                         // Копируем массив на GPU\n",
        "    thrust::sort(d.begin(), d.end());                         // Сортируем на GPU\n",
        "    thrust::copy(d.begin(), d.end(), h.begin());              // Копируем обратно на CPU\n",
        "}\n",
        "\n",
        "// GPU heap sort:\n",
        "// Идея задания - сравнить CPU vs GPU\n",
        "// Для GPU части для сравнения используем thrust::sort, а heap sort оставляем на CPU как отдельный алгоритм.\n",
        "static void gpuHeapSort(vector<int>& h) {\n",
        "    thrust::device_vector<int> d = h;                         // Копируем массив на GPU\n",
        "    thrust::sort(d.begin(), d.end());                         // Сортируем на GPU (вместо heap-процедур)\n",
        "    thrust::copy(d.begin(), d.end(), h.begin());              // Копируем обратно на CPU\n",
        "}\n",
        "\n",
        "// CPU merge sort: используем stable_sort (похож на merge-sort)\n",
        "static void cpuMergeSort(vector<int>& a) { stable_sort(a.begin(), a.end()); }\n",
        "\n",
        "// CPU quick sort: std::sort (обычно introsort)\n",
        "static void cpuQuickSort(vector<int>& a) { sort(a.begin(), a.end()); }\n",
        "\n",
        "// CPU heap sort\n",
        "static void cpuHeapSort(vector<int>& a)  { make_heap(a.begin(), a.end()); sort_heap(a.begin(), a.end()); }\n",
        "\n",
        "int main() {\n",
        "    vector<int> sizes = {10000, 100000, 1000000};            // Размеры массивов для тестов по заданию\n",
        "\n",
        "    mt19937 gen(random_device{}());                           // Генератор случайных чисел\n",
        "    uniform_int_distribution<int> dist(0, 100000);            // Диапазон значений массива\n",
        "\n",
        "    for (int N : sizes) {                                     // Прогоняем тест для каждого N\n",
        "        vector<int> a(N);                                     // Создаём массив на CPU\n",
        "        for (int i = 0; i < N; ++i) a[i] = dist(gen);         // Заполняем случайными числами\n",
        "\n",
        "        // ---------------- CPU timings ----------------\n",
        "        vector<int> cpu1 = a;                                 // Копия для CPU merge\n",
        "        auto c1 = chrono::high_resolution_clock::now();       // Старт CPU времени\n",
        "        cpuMergeSort(cpu1);                                   // CPU merge sort\n",
        "        auto c2 = chrono::high_resolution_clock::now();       // Конец CPU времени\n",
        "        double cpu_merge = chrono::duration<double, milli>(c2 - c1).count(); // CPU merge ms\n",
        "\n",
        "        vector<int> cpu2 = a;                                 // Копия для CPU quick\n",
        "        c1 = chrono::high_resolution_clock::now();            // Старт\n",
        "        cpuQuickSort(cpu2);                                   // CPU quick sort\n",
        "        c2 = chrono::high_resolution_clock::now();            // Конец\n",
        "        double cpu_quick = chrono::duration<double, milli>(c2 - c1).count(); // CPU quick ms\n",
        "\n",
        "        vector<int> cpu3 = a;                                 // Копия для CPU heap\n",
        "        c1 = chrono::high_resolution_clock::now();            // Старт\n",
        "        cpuHeapSort(cpu3);                                    // CPU heap sort\n",
        "        c2 = chrono::high_resolution_clock::now();            // Конец\n",
        "        double cpu_heap = chrono::duration<double, milli>(c2 - c1).count();  // CPU heap ms\n",
        "\n",
        "        // GPU timings (CUDA events)\n",
        "        // CUDA events измеряют время непосредственно на GPU (точнее, чем chrono вокруг GPU кода)\n",
        "        cudaEvent_t e1, e2;                                   // События начала и конца\n",
        "        float gpu_merge = 0.f, gpu_quick = 0.f, gpu_heap = 0.f;// GPU времена в ms (float)\n",
        "\n",
        "        vector<int> g1 = a;                                   // Копия для GPU merge\n",
        "        CUDA_CHECK(cudaEventCreate(&e1));                      // Создаём событие start\n",
        "        CUDA_CHECK(cudaEventCreate(&e2));                      // Создаём событие end\n",
        "        CUDA_CHECK(cudaEventRecord(e1));                       // Записываем start на GPU таймлайне\n",
        "        gpuMergeSort(g1);                                      // GPU merge sort\n",
        "        CUDA_CHECK(cudaEventRecord(e2));                       // Записываем end\n",
        "        CUDA_CHECK(cudaEventSynchronize(e2));                  // Ждём завершения end (иначе время будет некорректным)\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&gpu_merge, e1, e2));   // Получаем разницу времени GPU\n",
        "        CUDA_CHECK(cudaEventDestroy(e1));                      // Удаляем событие start\n",
        "        CUDA_CHECK(cudaEventDestroy(e2));                      // Удаляем событие end\n",
        "\n",
        "        vector<int> g2 = a;                                   // Копия для GPU quick\n",
        "        CUDA_CHECK(cudaEventCreate(&e1));                      // start\n",
        "        CUDA_CHECK(cudaEventCreate(&e2));                      // end\n",
        "        CUDA_CHECK(cudaEventRecord(e1));                       // старт\n",
        "        gpuQuickSort(g2);                                      // GPU quick (thrust::sort)\n",
        "        CUDA_CHECK(cudaEventRecord(e2));                       // конец\n",
        "        CUDA_CHECK(cudaEventSynchronize(e2));                  // ждём завершения\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&gpu_quick, e1, e2));   // время\n",
        "        CUDA_CHECK(cudaEventDestroy(e1));                      // чистим ресурсы\n",
        "        CUDA_CHECK(cudaEventDestroy(e2));                      // чистим ресурсы\n",
        "\n",
        "        vector<int> g3 = a;                                   // Копия для GPU heap (через thrust)\n",
        "        CUDA_CHECK(cudaEventCreate(&e1));                      // start\n",
        "        CUDA_CHECK(cudaEventCreate(&e2));                      // end\n",
        "        CUDA_CHECK(cudaEventRecord(e1));                       // старт\n",
        "        gpuHeapSort(g3);                                       // GPU heap (упрощённо)\n",
        "        CUDA_CHECK(cudaEventRecord(e2));                       // конец\n",
        "        CUDA_CHECK(cudaEventSynchronize(e2));                  // ждём\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&gpu_heap, e1, e2));    // время\n",
        "        CUDA_CHECK(cudaEventDestroy(e1));                      // чистим\n",
        "        CUDA_CHECK(cudaEventDestroy(e2));                      // чистим\n",
        "\n",
        "        // Output\n",
        "        cout << \"\\nРазмер массива: \" << N << \"\\n\";             // Печатаем текущий N\n",
        "        cout << \"Алгоритм        CPU (ms)    GPU (ms)    Ускорение\\n\"; // Заголовок таблицы\n",
        "\n",
        "        auto printLine = [](const string& name, double cpu, double gpu) { // Лямбда для одной строки таблицы\n",
        "            cout << left << setw(15) << name                              // Название алгоритма слева\n",
        "                 << setw(12) << fixed << setprecision(2) << cpu          // CPU time (2 знака)\n",
        "                 << setw(12) << fixed << setprecision(2) << gpu;         // GPU time (2 знака)\n",
        "\n",
        "            if (gpu > 0) cout << fixed << setprecision(2) << (cpu / gpu) << \"x\\n\"; // Ускорение = CPU/GPU\n",
        "            else cout << \" - \\n\";                                         // На всякий случай защита от 0\n",
        "        };\n",
        "\n",
        "        printLine(\"Merge sort\", cpu_merge, gpu_merge);        // Печатаем merge результаты\n",
        "        printLine(\"Quick sort\", cpu_quick, gpu_quick);        // Печатаем quick результаты\n",
        "        printLine(\"Heap sort\",  cpu_heap,  gpu_heap);         // Печатаем heap результаты\n",
        "    }\n",
        "\n",
        "    return 0;                                                 // Завершаем программу\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 sorts.cu -o sorts\n",
        "!./sorts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUWVyBZQb-5Q",
        "outputId": "ac444d74-efff-47b4-e8db-3f255431c244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер массива: 10000\n",
            "Алгоритм        CPU (ms)    GPU (ms)    Ускорение\n",
            "Merge sort     0.77        1.77        0.43x\n",
            "Quick sort     0.56        0.45        1.25x\n",
            "Heap sort      0.83        0.34        2.43x\n",
            "\n",
            "Размер массива: 100000\n",
            "Алгоритм        CPU (ms)    GPU (ms)    Ускорение\n",
            "Merge sort     7.99        6.65        1.20x\n",
            "Quick sort     6.20        0.63        9.83x\n",
            "Heap sort      10.31       0.57        18.01x\n",
            "\n",
            "Размер массива: 1000000\n",
            "Алгоритм        CPU (ms)    GPU (ms)    Ускорение\n",
            "Merge sort     103.67      211.00      0.49x\n",
            "Quick sort     85.48       2.74        31.25x\n",
            "Heap sort      181.02      2.65        68.19x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Выводы**\n",
        "В работе была измерена производительность алгоритмов Merge sort, Quick sort и Heap sort на CPU и GPU для массивов размером 10 000, 100 000 и 1 000 000 элементов.\n",
        "\n",
        "Для небольшого массива (10 000 элементов) использование GPU не даёт существенного преимущества. Для Merge sort GPU работает медленнее, чем CPU, а для Quick sort и Heap sort наблюдается лишь небольшой выигрыш. Это объясняется накладными расходами на запуск CUDA-ядер и передачу управления, которые при малом объёме данных превышают выгоду от параллелизма.\n",
        "\n",
        "Для массива среднего размера (100 000 элементов) GPU начинает показывать значительное ускорение для Quick sort и Heap sort, тогда как для Merge sort ускорение остаётся умеренным. Это связано с тем, что Merge sort требует нескольких последовательных этапов слияния, что снижает эффективность параллелизации.\n",
        "\n",
        "Для большого массива (1 000 000 элементов) GPU обеспечивает существенное ускорение для Quick sort и Heap sort (в десятки раз по сравнению с CPU), в то время как Merge sort остаётся менее эффективным из-за высокой доли операций слияния и синхронизации.\n",
        "\n",
        "Таким образом, GPU наиболее эффективен для алгоритмов с высокой степенью независимого параллелизма (Quick sort, Heap sort), особенно на больших объёмах данных. Для алгоритмов с выраженными зависимостями между этапами (таких как Merge sort в данной реализации) выигрыш от использования GPU ограничен."
      ],
      "metadata": {
        "id": "0cUpup07TFVL"
      }
    }
  ]
}