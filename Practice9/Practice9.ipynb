{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Практическое задание 9**"
      ],
      "metadata": {
        "id": "mfgiWuzxOZP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1"
      ],
      "metadata": {
        "id": "HDXDY0RvOdqe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfnQT5bCOXli",
        "outputId": "dedcadce-5430-4303-8bfb-5053fe09b1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,881 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,648 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,292 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,640 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [86.7 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [61.5 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,486 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,971 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,604 kB]\n",
            "Fetched 30.3 MB in 5s (6,640 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "libopenmpi-dev set to manually installed.\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -y\n",
        "!apt-get install -y openmpi-bin libopenmpi-dev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cpp\n",
        "#include <mpi.h>           // MPI\n",
        "#include <iostream>        // cout\n",
        "#include <vector>          // vector\n",
        "#include <random>          // random\n",
        "#include <cmath>           // sqrt\n",
        "#include <algorithm>       // max\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    // Инициализация MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;                          // rank = номер процесса, size = количество процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Размер массива (можешь менять)\n",
        "    const long long N = 1'000'000;\n",
        "\n",
        "    // Засекаем время в начале (по инструкции)\n",
        "    double start_time = MPI_Wtime();\n",
        "\n",
        "    // base = сколько элементов получит каждый гарантированно\n",
        "    long long base = N / size;\n",
        "\n",
        "    // rem = сколько элементов \"остатка\" нужно распределить по 1 элементу\n",
        "    long long rem = N % size;\n",
        "\n",
        "    // На rank 0 будет полный массив данных\n",
        "    vector<double> data;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        data.resize(N);\n",
        "\n",
        "        // Генератор случайных чисел (фиксируем seed для воспроизводимости)\n",
        "        mt19937 rng(42);\n",
        "        uniform_real_distribution<double> dist(0.0, 1.0);\n",
        "\n",
        "        // Заполняем массив случайными числами\n",
        "        for (long long i = 0; i < N; i++) {\n",
        "            data[i] = dist(rng);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Каждый процесс принимает базовую часть размером base\n",
        "    vector<double> local(base);\n",
        "\n",
        "    // 1) Раздаём base элементов каждому процессу через MPI_Scatter\n",
        "    // sendbuf значим только на rank 0, остальные передают nullptr\n",
        "    MPI_Scatter(\n",
        "        (rank == 0 ? data.data() : nullptr), // откуда отправляем (только rank 0)\n",
        "        (int)base,                           // сколько элементов каждому\n",
        "        MPI_DOUBLE,                          // тип данных\n",
        "        local.data(),                        // куда принимаем\n",
        "        (int)base,                           // сколько принимаем\n",
        "        MPI_DOUBLE,                          // тип данных\n",
        "        0,                                   // root\n",
        "        MPI_COMM_WORLD                       // коммуникатор\n",
        "    );\n",
        "\n",
        "    // 2) Учитываем остаток: первые rem процессов получают +1 элемент\n",
        "    // Для них расширяем local на 1 и докидываем один элемент\n",
        "    if (rank < rem) {\n",
        "        local.resize(base + 1);              // увеличиваем local, чтобы поместить extra\n",
        "\n",
        "        if (rank == 0) {\n",
        "            // rank 0 просто берёт extra элемент из своего массива\n",
        "            // Индекс extra начинается после base*size\n",
        "            local[base] = data[base * size + rank];\n",
        "        } else {\n",
        "            // Остальные получают extra через MPI_Recv от rank 0\n",
        "            MPI_Recv(\n",
        "                &local[base],                // куда положить 1 элемент\n",
        "                1,                           // количество\n",
        "                MPI_DOUBLE,                  // тип\n",
        "                0,                           // от кого (root)\n",
        "                123,                         // tag\n",
        "                MPI_COMM_WORLD,              // коммуникатор\n",
        "                MPI_STATUS_IGNORE            // статус не нужен\n",
        "            );\n",
        "        }\n",
        "    } else {\n",
        "        // Эти процессы получили ровно base элементов, local уже правильного размера\n",
        "        // local.size() == base\n",
        "    }\n",
        "\n",
        "    // root отправляет extra элементы тем процессам, которым они нужны (rank 1..rem-1)\n",
        "    if (rank == 0) {\n",
        "        for (int r = 1; r < rem; r++) {\n",
        "            double extra = data[base * size + r]; // extra элемент для процесса r\n",
        "            MPI_Send(\n",
        "                &extra,                         // что отправляем\n",
        "                1,                              // 1 элемент\n",
        "                MPI_DOUBLE,                     // тип\n",
        "                r,                              // кому\n",
        "                123,                            // tag\n",
        "                MPI_COMM_WORLD                  // коммуникатор\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 3) Каждый процесс считает локальные суммы\n",
        "    double local_sum = 0.0;\n",
        "    double local_sumsq = 0.0;\n",
        "\n",
        "    for (double x : local) {\n",
        "        local_sum += x;           // сумма элементов\n",
        "        local_sumsq += x * x;     // сумма квадратов\n",
        "    }\n",
        "\n",
        "    // 4) Собираем суммы на rank 0 через MPI_Reduce\n",
        "    double global_sum = 0.0;\n",
        "    double global_sumsq = 0.0;\n",
        "\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&local_sumsq, &global_sumsq, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Засекаем время в конце (по инструкции)\n",
        "    double end_time = MPI_Wtime();\n",
        "\n",
        "    // 5) Rank 0 считает mean и std и печатает\n",
        "    if (rank == 0) {\n",
        "        double mean = global_sum / (double)N;\n",
        "\n",
        "        // variance = (1/N)*sum(x^2) - ((1/N)*sum(x))^2\n",
        "        double variance = (global_sumsq / (double)N) - (mean * mean);\n",
        "\n",
        "        // Из-за погрешностей variance может стать чуть отрицательной (например -1e-16)\n",
        "        variance = max(0.0, variance);\n",
        "\n",
        "        double stddev = sqrt(variance);\n",
        "\n",
        "        cout << \"N = \" << N << \", processes = \" << size << endl;\n",
        "        cout << \"Mean: \" << mean << endl;\n",
        "        cout << \"Std dev: \" << stddev << endl;\n",
        "        cout << \"Execution time: \" << (end_time - start_time) << \" seconds.\" << endl;\n",
        "    }\n",
        "\n",
        "    // Завершение MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83-TsW0KOjKP",
        "outputId": "c0031fc6-a03f-4d5a-cf7f-9b723e980aee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing program.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ program.cpp -O2 -o program"
      ],
      "metadata": {
        "id": "-eIWuLgJOuSb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root  --oversubscribe -np 2 ./program\n",
        "!mpirun --allow-run-as-root  --oversubscribe -np 4 ./program\n",
        "!mpirun --allow-run-as-root  --oversubscribe -np 8 ./program"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74RLrQ6MOz96",
        "outputId": "1fd52d49-a4fc-4a23-a778-002b0bf2a9ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1000000, processes = 2\n",
            "Mean: 0.500055\n",
            "Std dev: 0.288622\n",
            "Execution time: 0.0489197 seconds.\n",
            "N = 1000000, processes = 4\n",
            "Mean: 0.500055\n",
            "Std dev: 0.288622\n",
            "Execution time: 0.118118 seconds.\n",
            "N = 1000000, processes = 8\n",
            "Mean: 0.500055\n",
            "Std dev: 0.288622\n",
            "Execution time: 0.125014 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В ходе выполнения задания была разработана MPI-программа для распределённого вычисления среднего значения и стандартного отклонения массива случайных чисел размером N=1000000. Генерация массива выполнялась на процессе с рангом 0, после чего данные были распределены между процессами с помощью функции MPI_Scatter с учётом остатка при делении массива. Каждый процесс вычислял локальную сумму элементов и сумму квадратов, которые затем собирались на процессе rank = 0 с использованием MPI_Reduce. На основе глобальных сумм были рассчитаны среднее значение и стандартное отклонение.\n",
        "\n",
        "Полученные результаты:\n",
        "\n",
        "* При np=2:\n",
        "Execution time ≈ 0.049 с\n",
        "\n",
        "* При np=4:\n",
        "Execution time ≈ 0.118 с\n",
        "\n",
        "* При np=8:\n",
        "Execution time ≈ 0.125 с\n",
        "\n",
        "Среднее значение массива во всех экспериментах составило около 0.500055, а стандартное отклонение около 0.288622, что соответствует ожидаемым значениям для равномерного распределения случайных чисел на интервале [0,1]. Это подтверждает корректность реализации вычислений и согласованность результатов при различном количестве процессов.\n",
        "\n",
        "Анализ времени выполнения показывает, что увеличение числа процессов не привело к ускорению программы. Наоборот, при росте количества процессов общее время выполнения увеличилось. Это объясняется тем, что для данной задачи вычислительная нагрузка на каждый процесс относительно невелика, а накладные расходы на коммуникации (операции MPI_Scatter, MPI_Send/Recv, MPI_Reduce) начинают доминировать над временем самих вычислений. При большом числе процессов затраты на обмен данными и синхронизацию возрастают, что снижает эффективность масштабирования.\n",
        "\n",
        "Таким образом, распределённые вычисления наиболее оправданы для более тяжёлых задач или значительно больших объёмов данных, где выигрыш от параллельной обработки превышает накладные расходы передачи данных между процессами."
      ],
      "metadata": {
        "id": "oaBn3uYqPT_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "_QNi511fQHnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program2.cpp\n",
        "#include <mpi.h>                  // подключаю MPI, чтобы работать с mpirun и процессами\n",
        "#include <iostream>               // для cout и вывода на экран\n",
        "#include <vector>                 // для удобных динамических массивов vector\n",
        "#include <random>                 // чтобы генерировать случайные числа\n",
        "#include <cmath>                  // для fabs (модуль) и деления/проверок\n",
        "#include <algorithm>              // для min/max\n",
        "\n",
        "using namespace std;              // чтобы не писать std:: каждый раз\n",
        "\n",
        "// CUDA тут не нужна, поэтому только MPI версия метода Гаусса\n",
        "\n",
        "int main(int argc, char** argv) {                 // main принимает argc/argv, потому что так принято в MPI\n",
        "\n",
        "    MPI_Init(&argc, &argv);                       // запускаю MPI окружение\n",
        "\n",
        "    int rank;                                     // номер текущего процесса\n",
        "    int size;                                     // сколько всего процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);         // получаю rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);         // получаю size\n",
        "\n",
        "    const int N = 8;                              // по твоей просьбе делаю фиксированный размер N=8\n",
        "\n",
        "    double start_time;                            // переменная для времени старта\n",
        "    double end_time;                              // переменная для времени конца\n",
        "\n",
        "    vector<double> A;                             // матрица A будет храниться полностью только на rank 0\n",
        "    vector<double> b;                             // вектор b тоже полностью только на rank 0\n",
        "\n",
        "    int base = N / size;                          // сколько строк гарантированно достанется каждому процессу\n",
        "    int rem  = N % size;                          // остаток строк, которые распределяются по 1 строке\n",
        "\n",
        "    int local_rows = base + (rank < rem ? 1 : 0); // если rank меньше rem, то он получает +1 строку\n",
        "\n",
        "    int start_row;                                // глобальный индекс первой строки, принадлежащей этому процессу\n",
        "    if (rank < rem) {                             // если процесс среди первых rem\n",
        "        start_row = rank * (base + 1);            // его блоки длиннее на 1 строку\n",
        "    } else {                                      // иначе\n",
        "        start_row = rem * (base + 1) + (rank - rem) * base; // вычисляю сдвиг после \"длинных\" блоков\n",
        "    }\n",
        "\n",
        "    if (rank == 0) {                              // только root создаёт матрицу и вектор\n",
        "        A.resize(N * N);                          // выделяю память под матрицу N x N\n",
        "        b.resize(N);                              // выделяю память под вектор b\n",
        "\n",
        "        mt19937 rng(42);                          // фиксированный seed, чтобы результаты были одинаковые\n",
        "        uniform_real_distribution<double> dist(0.0, 1.0); // равномерные числа [0,1]\n",
        "\n",
        "        for (int i = 0; i < N; i++) {             // иду по строкам\n",
        "            double rowsum = 0.0;                  // сумма модулей строки, чтобы сделать диагональное доминирование\n",
        "\n",
        "            for (int j = 0; j < N; j++) {         // иду по столбцам\n",
        "                double val = dist(rng);           // генерирую случайный элемент\n",
        "                A[i * N + j] = val;               // записываю в A\n",
        "                rowsum += fabs(val);              // добавляю модуль в сумму строки\n",
        "            }\n",
        "\n",
        "            A[i * N + i] = rowsum + N;            // усиливаю диагональный элемент, чтобы pivot не был маленьким\n",
        "            b[i] = dist(rng);                     // генерирую элемент правой части\n",
        "        }\n",
        "    }\n",
        "\n",
        "    vector<double> localA(local_rows * N, 0.0);   // локальная часть матрицы (local_rows строк)\n",
        "    vector<double> localb(local_rows, 0.0);       // локальная часть вектора b\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                  // синхронизация, чтобы честно начать измерение времени\n",
        "    start_time = MPI_Wtime();                     // старт времени\n",
        "\n",
        "    // дальше я хочу распределить строки между процессами через MPI_Scatter\n",
        "    // но MPI_Scatter умеет раздавать только одинаковое количество строк\n",
        "    // поэтому я раздаю base строк каждому, а остаток rem добрасываю вручную send/recv\n",
        "\n",
        "    if (base > 0) {                               // если base=0 (когда процессов больше чем строк), Scatter будет бессмысленный\n",
        "        vector<double> scatterA(base * N);        // временный буфер для base строк матрицы\n",
        "        vector<double> scatterb(base);            // временный буфер для base элементов b\n",
        "\n",
        "        MPI_Scatter(                              // раздаю base строк матрицы\n",
        "            (rank == 0 ? A.data() : nullptr),     // root отправляет A, остальные nullptr\n",
        "            base * N,                             // сколько элементов double отправляется каждому\n",
        "            MPI_DOUBLE,                           // тип данных\n",
        "            scatterA.data(),                      // куда принимаю\n",
        "            base * N,                             // сколько принимаю\n",
        "            MPI_DOUBLE,                           // тип данных\n",
        "            0,                                    // root\n",
        "            MPI_COMM_WORLD                        // коммуникатор\n",
        "        );\n",
        "\n",
        "        MPI_Scatter(                              // раздаю base элементов вектора b\n",
        "            (rank == 0 ? b.data() : nullptr),     // root отправляет b\n",
        "            base,                                 // сколько элементов каждому\n",
        "            MPI_DOUBLE,                           // тип\n",
        "            scatterb.data(),                      // куда принимаю\n",
        "            base,                                 // сколько принимаю\n",
        "            MPI_DOUBLE,                           // тип\n",
        "            0,                                    // root\n",
        "            MPI_COMM_WORLD                        // коммуникатор\n",
        "        );\n",
        "\n",
        "        for (int i = 0; i < base; i++) {          // копирую base строк в localA/localb\n",
        "            for (int j = 0; j < N; j++) {         // копирую строку целиком\n",
        "                localA[i * N + j] = scatterA[i * N + j]; // перенос элемента\n",
        "            }\n",
        "            localb[i] = scatterb[i];              // перенос соответствующего b\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // теперь распределяю остаток rem строк\n",
        "    // каждая из этих строк идёт процессам rank=0..rem-1 как дополнительная строка\n",
        "\n",
        "    if (rank < rem) {                             // только процессы, которые получают extra строку\n",
        "        int local_extra = base;                   // extra строка сохраняется после base строк, то есть индекс base\n",
        "\n",
        "        if (rank == 0) {                          // root свою extra строку просто копирует сам\n",
        "            int global_row = base * size + rank;  // глобальный индекс extra строки\n",
        "            for (int j = 0; j < N; j++) {         // копирую всю строку\n",
        "                localA[local_extra * N + j] = A[global_row * N + j];\n",
        "            }\n",
        "            localb[local_extra] = b[global_row];  // копирую соответствующий элемент b\n",
        "        } else {                                  // остальные получают extra строку через Recv\n",
        "            MPI_Recv(                             // принимаю строку матрицы\n",
        "                &localA[local_extra * N],         // куда писать\n",
        "                N,                                // сколько элементов\n",
        "                MPI_DOUBLE,                       // тип\n",
        "                0,                                // от root\n",
        "                200,                              // tag\n",
        "                MPI_COMM_WORLD,                   // коммуникатор\n",
        "                MPI_STATUS_IGNORE                 // статус не нужен\n",
        "            );\n",
        "\n",
        "            MPI_Recv(                             // принимаю b для этой строки\n",
        "                &localb[local_extra],             // куда писать\n",
        "                1,                                // 1 элемент\n",
        "                MPI_DOUBLE,                       // тип\n",
        "                0,                                // от root\n",
        "                201,                              // tag\n",
        "                MPI_COMM_WORLD,                   // коммуникатор\n",
        "                MPI_STATUS_IGNORE                 // статус\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (rank == 0) {                              // root отправляет extra строки процессам 1..rem-1\n",
        "        for (int r = 1; r < rem; r++) {           // только тем, кому нужен extra\n",
        "            int global_row = base * size + r;     // индекс extra строки для процесса r\n",
        "\n",
        "            MPI_Send(                             // отправляю строку\n",
        "                &A[global_row * N],               // адрес начала строки\n",
        "                N,                                // N элементов\n",
        "                MPI_DOUBLE,                       // тип\n",
        "                r,                                // кому\n",
        "                200,                              // tag\n",
        "                MPI_COMM_WORLD                    // коммуникатор\n",
        "            );\n",
        "\n",
        "            MPI_Send(                             // отправляю b элемент\n",
        "                &b[global_row],                   // адрес элемента\n",
        "                1,                                // 1 элемент\n",
        "                MPI_DOUBLE,                       // тип\n",
        "                r,                                // кому\n",
        "                201,                              // tag\n",
        "                MPI_COMM_WORLD                    // коммуникатор\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    vector<double> pivotRow(N, 0.0);              // буфер для pivot строки, которую будем рассылать\n",
        "    double pivotB = 0.0;                          // pivot элемент вектора b\n",
        "\n",
        "    // прямой ход метода Гаусса\n",
        "    for (int k = 0; k < N; k++) {                 // k это текущий столбец и строка pivot\n",
        "\n",
        "        int owner = -1;                           // owner это процесс, у которого хранится строка k\n",
        "\n",
        "        for (int r = 0; r < size; r++) {          // ищу owner перебором процессов\n",
        "            int r_rows = base + (r < rem ? 1 : 0);// сколько строк у процесса r\n",
        "            int r_start;                          // стартовая строка процесса r\n",
        "            if (r < rem) r_start = r * (base + 1);// старт для процессов с extra строкой\n",
        "            else r_start = rem * (base + 1) + (r - rem) * base; // старт для остальных\n",
        "\n",
        "            if (k >= r_start && k < r_start + r_rows) { // если k попадает в диапазон\n",
        "                owner = r;                         // значит owner найден\n",
        "                break;                             // выхожу из цикла\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (rank == owner) {                       // только владелец строки k готовит pivotRow\n",
        "            int local_k = k - start_row;           // локальный индекс этой строки в localA\n",
        "            for (int j = 0; j < N; j++) {          // копирую pivot строку\n",
        "                pivotRow[j] = localA[local_k * N + j];\n",
        "            }\n",
        "            pivotB = localb[local_k];              // копирую pivot элемент b\n",
        "        }\n",
        "\n",
        "        MPI_Bcast(pivotRow.data(), N, MPI_DOUBLE, owner, MPI_COMM_WORLD); // рассылаю pivotRow всем\n",
        "        MPI_Bcast(&pivotB, 1, MPI_DOUBLE, owner, MPI_COMM_WORLD);        // рассылаю pivotB всем\n",
        "\n",
        "        double pivot = pivotRow[k];                // pivot элемент (диагональ) для деления\n",
        "\n",
        "        for (int li = 0; li < local_rows; li++) {  // иду по локальным строкам процесса\n",
        "            int global_i = start_row + li;         // считаю глобальный индекс строки\n",
        "\n",
        "            if (global_i <= k) continue;           // беру только строки ниже pivot строки\n",
        "\n",
        "            double factor = localA[li * N + k] / pivot; // коэффициент для вычитания\n",
        "\n",
        "            localA[li * N + k] = 0.0;              // зануляю элемент под диагональю\n",
        "\n",
        "            for (int j = k + 1; j < N; j++) {      // обновляю остальные элементы строки\n",
        "                localA[li * N + j] -= factor * pivotRow[j]; // вычитаю factor * pivotRow\n",
        "            }\n",
        "\n",
        "            localb[li] -= factor * pivotB;         // обновляю правую часть\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // теперь собираю матрицу и вектор обратно на rank 0, чтобы сделать обратный ход\n",
        "    vector<int> recvcountsA;                       // сколько элементов A приходит от каждого процесса\n",
        "    vector<int> displsA;                           // смещения в итоговом массиве A\n",
        "    vector<int> recvcountsB;                       // сколько элементов b приходит от каждого процесса\n",
        "    vector<int> displsB;                           // смещения в итоговом векторе b\n",
        "\n",
        "    if (rank == 0) {                               // эти массивы нужны только root\n",
        "        recvcountsA.resize(size);\n",
        "        displsA.resize(size);\n",
        "        recvcountsB.resize(size);\n",
        "        displsB.resize(size);\n",
        "\n",
        "        int offA = 0;                              // текущее смещение по A\n",
        "        int offB = 0;                              // текущее смещение по b\n",
        "\n",
        "        for (int r = 0; r < size; r++) {           // для каждого процесса считаю размеры\n",
        "            int r_rows = base + (r < rem ? 1 : 0); // строки процесса r\n",
        "            recvcountsA[r] = r_rows * N;           // сколько doubles матрицы от процесса r\n",
        "            displsA[r] = offA;                     // куда класть\n",
        "            offA += recvcountsA[r];                // сдвигаю оффсет\n",
        "\n",
        "            recvcountsB[r] = r_rows;               // сколько doubles b от процесса r\n",
        "            displsB[r] = offB;                     // куда класть\n",
        "            offB += recvcountsB[r];                // сдвигаю оффсет\n",
        "        }\n",
        "\n",
        "        A.assign(N * N, 0.0);                      // выделяю заново место под A (уже верхнетреугольная)\n",
        "        b.assign(N, 0.0);                          // выделяю заново место под b (обновленный)\n",
        "    }\n",
        "\n",
        "    MPI_Gatherv(                                   // собираю localA на root\n",
        "        localA.data(),                             // что отправляю\n",
        "        local_rows * N,                            // сколько отправляю\n",
        "        MPI_DOUBLE,                                // тип\n",
        "        (rank == 0 ? A.data() : nullptr),          // куда собирать на root\n",
        "        (rank == 0 ? recvcountsA.data() : nullptr),// массив количеств\n",
        "        (rank == 0 ? displsA.data() : nullptr),    // массив смещений\n",
        "        MPI_DOUBLE,                                // тип\n",
        "        0,                                         // root\n",
        "        MPI_COMM_WORLD                             // коммуникатор\n",
        "    );\n",
        "\n",
        "    MPI_Gatherv(                                   // собираю localb на root\n",
        "        localb.data(),                             // что отправляю\n",
        "        local_rows,                                // сколько отправляю\n",
        "        MPI_DOUBLE,                                // тип\n",
        "        (rank == 0 ? b.data() : nullptr),          // куда собирать\n",
        "        (rank == 0 ? recvcountsB.data() : nullptr),// количества\n",
        "        (rank == 0 ? displsB.data() : nullptr),    // смещения\n",
        "        MPI_DOUBLE,                                // тип\n",
        "        0,                                         // root\n",
        "        MPI_COMM_WORLD                             // коммуникатор\n",
        "    );\n",
        "\n",
        "    vector<double> x;                              // вектор решения x\n",
        "\n",
        "    if (rank == 0) {                               // обратный ход делаю только на root\n",
        "        x.assign(N, 0.0);                          // выделяю память под решение\n",
        "\n",
        "        for (int i = N - 1; i >= 0; i--) {         // иду снизу вверх\n",
        "            double diag = A[i * N + i];            // диагональный элемент\n",
        "\n",
        "            double sum = 0.0;                      // сумма A[i][j]*x[j]\n",
        "            for (int j = i + 1; j < N; j++) {      // пробегаю по уже известным x\n",
        "                sum += A[i * N + j] * x[j];        // накапливаю\n",
        "            }\n",
        "\n",
        "            x[i] = (b[i] - sum) / diag;            // вычисляю x[i]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                   // синхронизация перед концом времени\n",
        "    end_time = MPI_Wtime();                        // конец времени\n",
        "\n",
        "    if (rank == 0) {                               // выводит только root\n",
        "        cout << \"N = \" << N << \", processes = \" << size << endl; // печатаю параметры\n",
        "\n",
        "        cout << \"Solution x:\" << endl;             // заголовок\n",
        "        for (int i = 0; i < N; i++) {              // печатаю все 8 элементов\n",
        "            cout << \"x[\" << i << \"] = \" << x[i] << endl; // печать элемента\n",
        "        }\n",
        "\n",
        "        cout << \"Execution time: \"                 // печать времени\n",
        "             << (end_time - start_time)\n",
        "             << \" seconds.\" << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                                // завершаю MPI\n",
        "    return 0;                                      // выхожу из программы\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IaR8PF_PTeP",
        "outputId": "fff79e3d-028b-4587-ceff-e42f14e638d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing program2.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ program2.cpp -O2 -o program2\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./program2\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./program2\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./program2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpr03lOuWTIt",
        "outputId": "103062bf-6844-4968-acb3-313aacc4b95b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 8, processes = 2\n",
            "Solution x:\n",
            "x[0] = 0.00609089\n",
            "x[1] = -0.012175\n",
            "x[2] = 0.0316275\n",
            "x[3] = 0.0236677\n",
            "x[4] = 0.00741247\n",
            "x[5] = 0.0755191\n",
            "x[6] = 0.0356818\n",
            "x[7] = 0.023488\n",
            "Execution time: 0.000177596 seconds.\n",
            "N = 8, processes = 4\n",
            "Solution x:\n",
            "x[0] = 0.00609089\n",
            "x[1] = -0.012175\n",
            "x[2] = 0.0316275\n",
            "x[3] = 0.0236677\n",
            "x[4] = 0.00741247\n",
            "x[5] = 0.0755191\n",
            "x[6] = 0.0356818\n",
            "x[7] = 0.023488\n",
            "Execution time: 0.000588227 seconds.\n",
            "N = 8, processes = 8\n",
            "Solution x:\n",
            "x[0] = 0.00609089\n",
            "x[1] = -0.012175\n",
            "x[2] = 0.0316275\n",
            "x[3] = 0.0236677\n",
            "x[4] = 0.00741247\n",
            "x[5] = 0.0755191\n",
            "x[6] = 0.0356818\n",
            "x[7] = 0.023488\n",
            "Execution time: 0.000890414 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В ходе задания была реализована распределенная MPI программа для решения системы линейных уравнений Ax=b методом Гаусса. Процесс с rank = 0 формировал матрицу коэффициентов A размера 8×8 и вектор правых частей b, после чего строки матрицы и соответствующие элементы b распределялись между процессами. Для распределения основной части строк использовалась функция MPI_Scatter, а оставшиеся строки (если N не делится на число процессов) корректно передавались дополнительными сообщениями, поэтому программа сохраняет работоспособность при любом значении -np.\n",
        "\n",
        "Прямой ход метода Гаусса выполнялся параллельно: на каждом шаге k процесс, владеющий текущей ведущей строкой, подготавливал pivot строку и рассылал ее всем остальным с помощью MPI_Bcast. После этого каждый процесс занулял элементы ниже диагонали только в своих локальных строках. Далее результаты прямого хода (верхнетреугольная матрица и обновленный вектор b) собирались на процессе rank = 0, где выполнялся обратный ход (обратная подстановка) и формировался итоговый вектор решения x.\n",
        "\n",
        "Корректность решения подтверждается тем, что при разном количестве процессов получен одинаковый вектор x. Это означает, что распределение строк, передача pivot строки через MPI_Bcast и последующая сборка результатов реализованы корректно и дают согласованный результат независимо от количества процессов.\n",
        "\n",
        "По времени выполнения для N=8 получено:\n",
        "\n",
        "* -np 2:\n",
        "t≈0.000178 сек\n",
        "\n",
        "* -np 4:\n",
        "t≈0.000588 сек\n",
        "\n",
        "* -np 8:\n",
        "t≈0.000890 сек\n",
        "\n",
        "Увеличение числа процессов в данном эксперименте не приводит к ускорению, так как размер системы очень мал, а основное время начинает уходить на коммуникации и синхронизацию между процессами (рассылка pivot строки и сбор данных), а не на вычисления. Такой результат ожидаем для маленьких N. Реальный выигрыш от MPI обычно проявляется при существенно больших размерах матриц, когда вычислительная нагрузка доминирует над накладными расходами обмена данными."
      ],
      "metadata": {
        "id": "JDGLb6eeWvMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3"
      ],
      "metadata": {
        "id": "G0jFw9RZXMCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program3.cpp\n",
        "#include <mpi.h>                     // подключаю MPI, чтобы работало mpirun и обмен между процессами\n",
        "#include <iostream>                  // для вывода cout\n",
        "#include <vector>                    // для vector\n",
        "#include <random>                    // для генерации случайного графа\n",
        "#include <limits>                    // для \"бесконечности\" INF\n",
        "#include <algorithm>                 // для min\n",
        "\n",
        "using namespace std;                 // чтобы не писать std:: каждый раз\n",
        "\n",
        "int main(int argc, char** argv) {                         // стандартный main для MPI\n",
        "\n",
        "    MPI_Init(&argc, &argv);                               // инициализирую MPI\n",
        "\n",
        "    int rank;                                             // номер процесса\n",
        "    int size;                                             // количество процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);                 // узнаю rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);                 // узнаю size\n",
        "\n",
        "    int N = 8;                                            // размер графа по умолчанию\n",
        "    if (argc >= 2) N = atoi(argv[1]);                     // если в командной строке дали N, то беру его\n",
        "    if (N <= 0) {                                         // защита от неправильного N\n",
        "        if (rank == 0) cout << \"N must be positive\\n\";     // печатаю ошибку только на root\n",
        "        MPI_Finalize();                                   // завершаю MPI\n",
        "        return 1;                                         // выхожу с кодом ошибки\n",
        "    }\n",
        "\n",
        "    const int INF = 1000000000;                           // большое число вместо бесконечности\n",
        "\n",
        "    int base = N / size;                                  // сколько строк гарантированно каждому процессу\n",
        "    int rem  = N % size;                                  // остаток строк\n",
        "\n",
        "    int local_rows = base + (rank < rem ? 1 : 0);          // сколько строк именно у этого процесса\n",
        "\n",
        "    int start_row;                                        // глобальный индекс первой строки у процесса\n",
        "    if (rank < rem) {                                     // если процесс среди первых rem\n",
        "        start_row = rank * (base + 1);                    // у них блоки на 1 строку больше\n",
        "    } else {                                              // иначе\n",
        "        start_row = rem * (base + 1) + (rank - rem) * base;// старт после \"длинных\" блоков\n",
        "    }\n",
        "\n",
        "    vector<int> G;                                        // полная матрица графа будет только у rank 0\n",
        "    if (rank == 0) {                                      // только root создаёт граф\n",
        "        G.resize(N * N);                                  // выделяю N*N элементов\n",
        "\n",
        "        mt19937 rng(42);                                  // фиксированный seed, чтобы результаты повторялись\n",
        "        uniform_int_distribution<int> wdist(1, 20);        // веса рёбер от 1 до 20\n",
        "        uniform_int_distribution<int> edist(0, 99);        // шанс наличия ребра\n",
        "\n",
        "        for (int i = 0; i < N; i++) {                      // иду по строкам\n",
        "            for (int j = 0; j < N; j++) {                  // иду по столбцам\n",
        "                if (i == j) {                              // расстояние до себя\n",
        "                    G[i * N + j] = 0;                      // 0 на диагонали\n",
        "                } else {                                   // не диагональ\n",
        "                    int r = edist(rng);                    // генерирую шанс ребра\n",
        "                    if (r < 60) {                          // 60% что ребро есть\n",
        "                        G[i * N + j] = wdist(rng);         // задаю вес ребра\n",
        "                    } else {                               // иначе ребра нет\n",
        "                        G[i * N + j] = INF;                // ставлю INF\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    vector<int> localG(local_rows * N, INF);               // локальная часть матрицы, только свои строки\n",
        "    vector<int> fullG;                                     // общий буфер полной матрицы, чтобы делать Allgather\n",
        "    fullG.resize(N * N, INF);                              // выделяю место под полную матрицу у каждого процесса\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                           // синхронизация перед замером времени\n",
        "    double start_time = MPI_Wtime();                       // старт времени\n",
        "\n",
        "    if (base > 0) {                                        // Scatter делаю только если base > 0\n",
        "        vector<int> scatterBuf(base * N);                  // временный буфер под base строк\n",
        "\n",
        "        MPI_Scatter(                                       // раздаю base строк каждому процессу\n",
        "            (rank == 0 ? G.data() : nullptr),              // root отправляет, остальные nullptr\n",
        "            base * N,                                      // сколько int отправить каждому\n",
        "            MPI_INT,                                       // тип int\n",
        "            scatterBuf.data(),                             // куда принимаю\n",
        "            base * N,                                      // сколько принимаю\n",
        "            MPI_INT,                                       // тип\n",
        "            0,                                             // root\n",
        "            MPI_COMM_WORLD                                 // коммуникатор\n",
        "        );\n",
        "\n",
        "        for (int i = 0; i < base; i++) {                   // копирую base строк в localG\n",
        "            for (int j = 0; j < N; j++) {                  // копирую каждый элемент строки\n",
        "                localG[i * N + j] = scatterBuf[i * N + j]; // перенос\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (rank < rem) {                                      // если процесс получает ещё одну строку\n",
        "        int local_extra = base;                            // extra строка будет на позиции base\n",
        "\n",
        "        if (rank == 0) {                                   // root свою extra строку копирует сам\n",
        "            int global_row = base * size + rank;           // глобальный индекс extra строки\n",
        "            for (int j = 0; j < N; j++) {                  // копирую строку\n",
        "                localG[local_extra * N + j] = G[global_row * N + j];\n",
        "            }\n",
        "        } else {                                           // остальные получают extra строку через Recv\n",
        "            MPI_Recv(                                      // принимаю 1 строку длины N\n",
        "                &localG[local_extra * N],                  // куда кладу\n",
        "                N,                                         // N элементов\n",
        "                MPI_INT,                                   // тип int\n",
        "                0,                                         // от root\n",
        "                300,                                       // tag\n",
        "                MPI_COMM_WORLD,                            // коммуникатор\n",
        "                MPI_STATUS_IGNORE                           // статус не нужен\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (rank == 0) {                                       // root рассылает extra строки процессам 1..rem-1\n",
        "        for (int r = 1; r < rem; r++) {                    // только тем, кому нужна extra строка\n",
        "            int global_row = base * size + r;              // индекс строки\n",
        "            MPI_Send(                                      // отправляю строку\n",
        "                &G[global_row * N],                        // адрес строки\n",
        "                N,                                         // длина N\n",
        "                MPI_INT,                                   // тип\n",
        "                r,                                         // кому\n",
        "                300,                                       // tag\n",
        "                MPI_COMM_WORLD                              // коммуникатор\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    vector<int> recvcounts(size);                          // сколько элементов от каждого для Allgather\n",
        "    vector<int> displs(size);                              // смещения для Allgather\n",
        "\n",
        "    int offset = 0;                                        // считаю смещение в fullG\n",
        "    for (int r = 0; r < size; r++) {                       // пробегаю все процессы\n",
        "        int r_rows = base + (r < rem ? 1 : 0);             // сколько строк у процесса r\n",
        "        recvcounts[r] = r_rows * N;                        // сколько элементов он отдаёт\n",
        "        displs[r] = offset;                                // куда складывать в fullG\n",
        "        offset += recvcounts[r];                           // увеличиваю offset\n",
        "    }\n",
        "\n",
        "    MPI_Allgatherv(                                        // собираю всю матрицу у всех процессов\n",
        "        localG.data(),                                     // что я отправляю (мои строки)\n",
        "        local_rows * N,                                    // сколько элементов я отправляю\n",
        "        MPI_INT,                                           // тип\n",
        "        fullG.data(),                                      // куда собираю\n",
        "        recvcounts.data(),                                 // сколько принимать от каждого\n",
        "        displs.data(),                                     // смещения\n",
        "        MPI_INT,                                           // тип\n",
        "        MPI_COMM_WORLD                                     // коммуникатор\n",
        "    );\n",
        "\n",
        "    for (int k = 0; k < N; k++) {                           // главный цикл Флойда-Уоршелла по промежуточной вершине k\n",
        "\n",
        "        for (int li = 0; li < local_rows; li++) {           // иду по локальным строкам, которые принадлежат процессу\n",
        "            int i = start_row + li;                         // глобальный индекс строки i\n",
        "\n",
        "            int dik = fullG[i * N + k];                     // расстояние i -> k из полной матрицы\n",
        "\n",
        "            if (dik >= INF) continue;                       // если i->k бесконечность, нет смысла обновлять\n",
        "\n",
        "            for (int j = 0; j < N; j++) {                   // иду по столбцам j\n",
        "                int dkj = fullG[k * N + j];                 // расстояние k -> j\n",
        "                if (dkj >= INF) continue;                   // если k->j бесконечность, пропускаю\n",
        "                int candidate = dik + dkj;                  // кандидат на более короткий путь i -> k -> j\n",
        "                int &dij = localG[li * N + j];              // ссылка на текущий локальный dij\n",
        "                if (candidate < dij) dij = candidate;       // если кандидат лучше, обновляю\n",
        "            }\n",
        "        }\n",
        "\n",
        "        MPI_Allgatherv(                                     // после обновления локальных строк собираю обновлённую матрицу у всех\n",
        "            localG.data(),                                  // отправляю свои локальные строки\n",
        "            local_rows * N,                                 // количество\n",
        "            MPI_INT,                                        // тип\n",
        "            fullG.data(),                                   // принимаю полную матрицу\n",
        "            recvcounts.data(),                              // сколько от каждого\n",
        "            displs.data(),                                  // смещения\n",
        "            MPI_INT,                                        // тип\n",
        "            MPI_COMM_WORLD                                  // коммуникатор\n",
        "        );\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                            // синхронизация перед концом времени\n",
        "    double end_time = MPI_Wtime();                           // конец времени\n",
        "\n",
        "    vector<int> finalG;                                      // финальная матрица только на root\n",
        "    if (rank == 0) finalG.resize(N * N);                     // выделяю память на root\n",
        "\n",
        "    MPI_Gatherv(                                             // собираю финальные строки на root\n",
        "        localG.data(),                                       // что отправляю\n",
        "        local_rows * N,                                      // сколько отправляю\n",
        "        MPI_INT,                                             // тип\n",
        "        (rank == 0 ? finalG.data() : nullptr),               // куда собирать\n",
        "        recvcounts.data(),                                   // сколько от каждого\n",
        "        displs.data(),                                       // смещения\n",
        "        MPI_INT,                                             // тип\n",
        "        0,                                                   // root\n",
        "        MPI_COMM_WORLD                                       // коммуникатор\n",
        "    );\n",
        "\n",
        "    if (rank == 0) {                                         // печатаю результат только на root\n",
        "        cout << \"N = \" << N << \", processes = \" << size << endl;\n",
        "\n",
        "        cout << \"All-pairs shortest paths matrix:\" << endl;  // заголовок\n",
        "\n",
        "        for (int i = 0; i < N; i++) {                        // печатаю матрицу\n",
        "            for (int j = 0; j < N; j++) {                    // печатаю элемент\n",
        "                int val = finalG[i * N + j];                 // беру значение\n",
        "                if (val >= INF / 2) cout << \"INF \";          // если далеко, печатаю INF\n",
        "                else cout << val << \" \";                     // иначе печатаю число\n",
        "            }\n",
        "            cout << endl;                                    // новая строка\n",
        "        }\n",
        "\n",
        "        cout << \"Execution time: \"                           // печатаю время\n",
        "             << (end_time - start_time)\n",
        "             << \" seconds.\" << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                                          // завершаю MPI\n",
        "    return 0;                                                // выхожу\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v23A102DXs6c",
        "outputId": "6537c374-f7d9-49a7-d79c-e7d6fa3cad9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing program3.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ program3.cpp -O2 -o program3\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./program3 8\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./program3 8\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./program3 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujllbdkEYKA7",
        "outputId": "7981d0b2-50ed-431f-d474-4870f4539d83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 8, processes = 2\n",
            "All-pairs shortest paths matrix:\n",
            "0 9 6 11 7 12 9 2 \n",
            "5 0 9 4 8 7 2 7 \n",
            "6 3 0 7 1 9 5 8 \n",
            "1 1 7 0 4 3 3 3 \n",
            "5 2 11 6 0 8 4 7 \n",
            "6 3 12 7 1 0 5 8 \n",
            "3 3 7 2 6 5 0 5 \n",
            "10 7 4 11 5 13 9 0 \n",
            "Execution time: 0.00210776 seconds.\n",
            "N = 8, processes = 4\n",
            "All-pairs shortest paths matrix:\n",
            "0 9 6 11 7 12 9 2 \n",
            "5 0 9 4 8 7 2 7 \n",
            "6 3 0 7 1 9 5 8 \n",
            "1 1 7 0 4 3 3 3 \n",
            "5 2 11 6 0 8 4 7 \n",
            "6 3 12 7 1 0 5 8 \n",
            "3 3 7 2 6 5 0 5 \n",
            "10 7 4 11 5 13 9 0 \n",
            "Execution time: 0.000520693 seconds.\n",
            "N = 8, processes = 8\n",
            "All-pairs shortest paths matrix:\n",
            "0 9 6 11 7 12 9 2 \n",
            "5 0 9 4 8 7 2 7 \n",
            "6 3 0 7 1 9 5 8 \n",
            "1 1 7 0 4 3 3 3 \n",
            "5 2 11 6 0 8 4 7 \n",
            "6 3 12 7 1 0 5 8 \n",
            "3 3 7 2 6 5 0 5 \n",
            "10 7 4 11 5 13 9 0 \n",
            "Execution time: 0.00196326 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В ходе задания была реализована MPI программа для параллельного поиска кратчайших путей между всеми парами вершин графа с использованием алгоритма Флойда Уоршелла. Процесс с rank = 0 сформировал матрицу смежности графа G размера 8×8, где веса ребер задавались случайно, а отсутствие ребра обозначалось большим числом INF. Далее строки матрицы были распределены между процессами: основная часть раздавалась через MPI_Scatter, а оставшиеся строки при необходимости передавались дополнительно, поэтому программа корректно работает при любом количестве процессов.\n",
        "\n",
        "В ходе выполнения алгоритма каждый процесс обновлял только свою часть матрицы расстояний, то есть свои строки. После каждой итерации по промежуточной вершине k процессы обменивались обновленными данными с помощью MPI_Allgather, чтобы у каждого процесса была актуальная полная матрица расстояний и можно было корректно выполнять следующий шаг алгоритма. После завершения всех итераций итоговая матрица была собрана на процессе rank = 0 и выведена на экран.\n",
        "\n",
        "Корректность реализации подтверждается тем, что при запуске на разном числе процессов получена одинаковая итоговая матрица кратчайших расстояний. Это означает, что распределение данных, локальные обновления и коллективный обмен через MPI_Allgather реализованы правильно и дают согласованный результат независимо от числа процессов.\n",
        "\n",
        "По измеренному времени выполнения для N=8 получено:\n",
        "\n",
        "* -np 2:\n",
        "t≈0.00211 сек\n",
        "\n",
        "* -np 4:\n",
        "t≈0.00052 сек\n",
        "\n",
        "* -np 8:\n",
        "t≈0.00196 сек\n",
        "\n",
        "Из результатов видно, что при np=4 время оказалось минимальным, а при np=8 снова увеличилось. Это ожидаемо для маленького графа: вычислительная часть (обновление матрицы) очень небольшая, поэтому на общую длительность сильно влияют накладные расходы MPI, особенно частые обмены MPI_Allgather на каждой итерации k. При слишком большом числе процессов по сравнению с размером задачи рост расходов на коммуникации и синхронизацию может перекрывать выгоду от распараллеливания.\n",
        "\n",
        "Таким образом, предложенный параллельный подход корректно реализует алгоритм Флойда Уоршелла и сохраняет правильность результата при любом количестве процессов. Наибольший выигрыш по времени ожидается при больших значениях N, когда вычислений становится существенно больше, а относительное влияние коммуникационных затрат уменьшается."
      ],
      "metadata": {
        "id": "Z6OIQkoNZW4B"
      }
    }
  ]
}