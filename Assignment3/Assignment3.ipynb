{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Assignment 3**\n",
        "##Задание 1"
      ],
      "metadata": {
        "id": "NL2gkN-BWZwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile elem_scale.cu\n",
        "#include <iostream>            // Для вывода информации в консоль (cout)\n",
        "#include <vector>              // Контейнер vector для хранения массива на CPU\n",
        "#include <random>              // Генерация случайных чисел\n",
        "#include <cuda_runtime.h>      // CUDA runtime API и CUDA events\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Макрос: проверка ошибок CUDA-вызовов (если ошибка - печатаем и завершаем программу)\n",
        "#define CUDA_CHECK(x) do { \\\n",
        "  cudaError_t e = (x); \\\n",
        "  if (e != cudaSuccess) { \\\n",
        "    cout << \"CUDA error: \" << cudaGetErrorString(e) \\\n",
        "         << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "\n",
        "// Версия 1: работа только с глобальной памятью\n",
        "// Каждый поток напрямую работает со своим элементом в глобальной памяти:\n",
        "// читает его, умножает на коэффициент и записывает результат обратно\n",
        "\n",
        "__global__ void scale_global(float* d, int N, float k) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;  // Вычисляем глобальный индекс элемента массива\n",
        "    if (idx < N) {                                    // Проверяем, что индекс не выходит за границы массива\n",
        "        d[idx] = d[idx] * k;                          // Умножаем элемент на k и записываем результат обратно\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Версия 2: использование shared memory\n",
        "// Каждый блок сначала загружает свою часть данных из глобальной памяти в shared memory,\n",
        "// затем выполняет вычисления в shared memory и только потом записывает результат обратно\n",
        "\n",
        "__global__ void scale_shared(float* d, int N, float k) {\n",
        "    extern __shared__ float s[];\n",
        "    // Динамическая shared memory\n",
        "    // Размер задаётся при запуске kernel (block.x * sizeof(float))\n",
        "    // Эта память общая для потоков одного блока и быстрее, чем global memory\n",
        "\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Глобальный индекс элемента:\n",
        "    // номер блока * размер блока + номер потока в блоке\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    // Локальный индекс потока внутри блока\n",
        "    // Используется как индекс для массива shared memory\n",
        "\n",
        "    if (gid < N) {\n",
        "        s[tid] = d[gid];\n",
        "        // Каждый поток копирует один элемент из глобальной памяти в shared memory\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Синхронизируем потоки, чтобы все данные были загружены перед вычислениями\n",
        "\n",
        "    if (gid < N) {\n",
        "        s[tid] = s[tid] * k;\n",
        "        // Выполняем умножение в shared memory\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Ждём, пока все потоки завершат вычисления перед записью обратно\n",
        "\n",
        "    if (gid < N) {\n",
        "        d[gid] = s[tid];\n",
        "        // Записываем результат обратно в глобальную память\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Функция: замер времени kernel через CUDA events\n",
        "\n",
        "template <typename Kernel>\n",
        "float timeKernel(Kernel kernel, float* d, int N, float k, dim3 grid, dim3 block, size_t shmemBytes) {\n",
        "    cudaEvent_t e1, e2;                               // CUDA события: начало и конец измерения\n",
        "\n",
        "    CUDA_CHECK(cudaEventCreate(&e1));                 // Создаём событие начала\n",
        "    CUDA_CHECK(cudaEventCreate(&e2));                 // Создаём событие конца\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(e1));                  // Записываем событие начала\n",
        "    kernel<<<grid, block, shmemBytes>>>(d, N, k);     // Запускаем kernel на GPU\n",
        "    CUDA_CHECK(cudaGetLastError());                   // Проверяем, что запуск прошёл без ошибок\n",
        "    CUDA_CHECK(cudaEventRecord(e2));                  // Записываем событие конца\n",
        "\n",
        "    CUDA_CHECK(cudaEventSynchronize(e2));             // Ждём, пока kernel полностью завершится\n",
        "    float ms = 0.0f;                                  // Переменная для хранения времени в миллисекундах\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, e1, e2));    // Вычисляем время между событиями\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(e1));                 // Удаляем событие начала\n",
        "    CUDA_CHECK(cudaEventDestroy(e2));                 // Удаляем событие конца\n",
        "\n",
        "    return ms;                                        // Возвращаем измеренное время выполнения kernel\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    const int N = 1'000'000;                          // Размер массива по заданию\n",
        "    const float k = 3.5f;                             // Множитель для поэлементного умножения\n",
        "\n",
        "    // Генерируем массив случайных чисел на CPU\n",
        "    vector<float> h(N);\n",
        "    mt19937 gen(42);                                  // Генератор с фиксированным seed для воспроизводимости\n",
        "    uniform_real_distribution<float> dist(0.0f, 100.0f);\n",
        "    for (int i = 0; i < N; ++i) h[i] = dist(gen);\n",
        "\n",
        "    // Выделяем память на GPU\n",
        "    float* d = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d, N * sizeof(float)));\n",
        "\n",
        "    // Копируем данные с CPU на GPU\n",
        "    CUDA_CHECK(cudaMemcpy(d, h.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Настраиваем сетку и блоки\n",
        "    dim3 block(256);                                  // 256 потоков в одном блоке\n",
        "    dim3 grid((N + block.x - 1) / block.x);           // Столько блоков, чтобы покрыть весь массив\n",
        "\n",
        "    // Тест 1: версия с глобальной памятью\n",
        "    CUDA_CHECK(cudaMemcpy(d, h.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    float t_global = timeKernel(scale_global, d, N, k, grid, block, 0);\n",
        "\n",
        "    vector<float> h_global(N);\n",
        "    CUDA_CHECK(cudaMemcpy(h_global.data(), d, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Тест 2: версия с shared memory\n",
        "    CUDA_CHECK(cudaMemcpy(d, h.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    size_t shmemBytes = block.x * sizeof(float);      // Размер shared memory на блок\n",
        "    float t_shared = timeKernel(scale_shared, d, N, k, grid, block, shmemBytes);\n",
        "\n",
        "    vector<float> h_shared(N);\n",
        "    CUDA_CHECK(cudaMemcpy(h_shared.data(), d, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Вывод результатов\n",
        "    cout << \"N = \" << N << \"\\n\";\n",
        "    cout << \"Global memory kernel time: \" << t_global << \" ms\\n\";\n",
        "    cout << \"Shared memory kernel time: \" << t_shared << \" ms\\n\";\n",
        "\n",
        "    if (t_shared > 0.0f) {\n",
        "        cout << \"Speedup (Global/Shared): \" << (t_global / t_shared) << \"x\\n\";\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d));                          // Освобождаем память на GPU\n",
        "    return 0;                                         // Завершаем программу\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brQVZjaTVBfq",
        "outputId": "d0753593-ad04-461d-9b04-3ea6dbf2a1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting elem_scale.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 elem_scale.cu -o elem_scale\n",
        "!./elem_scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPm-CQentACa",
        "outputId": "ecf0c331-f777-4f38-edae-8b16003af5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1000000\n",
            "Global memory kernel time: 0.119808 ms\n",
            "Shared memory kernel time: 0.047232 ms\n",
            "Speedup (Global/Shared): 2.53659x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Вывод**\n",
        "В данном задании была реализована поэлементная обработка массива на GPU в двух вариантах: с использованием только глобальной памяти и с использованием разделяемой памяти (shared memory). Эксперименты проводились для массива размером 1 000 000 элементов.\n",
        "\n",
        "Время выполнения версии с глобальной памятью составило 0.119808 мс, а версии с разделяемой памятью 0.047232 мс. Это соответствует ускорению примерно в 2.54 раза в пользу реализации с shared memory.\n",
        "\n",
        "Полученный выигрыш объясняется тем, что доступ к разделяемой памяти на GPU значительно быстрее, чем к глобальной памяти. В реализации с shared memory данные сначала загружаются в быструю память блока, и дальнейшие операции выполняются уже над ними, что снижает задержки доступа и повышает общую производительность.\n",
        "\n",
        "Таким образом, использование разделяемой памяти позволило заметно ускорить выполнение даже простой поэлементной операции при большом размере массива, что подтверждает её важную роль в оптимизации CUDA-программ."
      ],
      "metadata": {
        "id": "_UNddxkSWuuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 2"
      ],
      "metadata": {
        "id": "xLy9vGo_eDqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vec_add_blocks.cu\n",
        "#include <iostream>            // Для вывода информации в консоль (cout)\n",
        "#include <vector>              // Контейнер vector для хранения массива на CPU\n",
        "#include <random>              // Генерация случайных чисел\n",
        "#include <cuda_runtime.h>      // CUDA runtime API и CUDA events\n",
        "#include <iomanip>             // Для красивого вывода таблиц\n",
        "\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Макрос: проверка ошибок CUDA-вызовов (если ошибка - печатаем и завершаем программу)\n",
        "#define CUDA_CHECK(x) do { \\\n",
        "  cudaError_t e = (x); \\\n",
        "  if (e != cudaSuccess) { \\\n",
        "    cout << \"CUDA error: \" << cudaGetErrorString(e) \\\n",
        "         << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "// CUDA kernel: поэлементное сложение двух массивов\n",
        "// Каждый поток отвечает ровно за один элемент результата\n",
        "__global__ void vecAdd(const float* A, const float* B, float* C, int N) {\n",
        "    // Вычисляем глобальный индекс элемента, который обрабатывает этот поток\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверяем, чтобы не выйти за границы массива (на последнем блоке могут быть лишние потоки)\n",
        "    if (i < N) {\n",
        "        // Основная операция: сложение соответствующих элементов массивов\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Функция для замера времени выполнения kernel (только kernel, без memcpy)\n",
        "float timeKernelVecAdd(const float* dA, const float* dB, float* dC, int N, int blockSize) {\n",
        "    dim3 block(blockSize);                          // задаём размер блока потоков\n",
        "    dim3 grid((N + block.x - 1) / block.x);         // считаем сколько блоков нужно под N\n",
        "\n",
        "    cudaEvent_t e1, e2;                             // события для замера времени на GPU\n",
        "    CUDA_CHECK(cudaEventCreate(&e1));\n",
        "    CUDA_CHECK(cudaEventCreate(&e2));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(e1));                // старт таймера на GPU\n",
        "    vecAdd<<<grid, block>>>(dA, dB, dC, N);         // запуск kernel\n",
        "    CUDA_CHECK(cudaGetLastError());                 // проверка, что kernel запустился нормально\n",
        "    CUDA_CHECK(cudaEventRecord(e2));                // стоп таймера\n",
        "    CUDA_CHECK(cudaEventSynchronize(e2));           // ждём завершения kernel\n",
        "\n",
        "    float ms = 0.0f;                                // сюда запишется время в миллисекундах\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, e1, e2));  // считаем разницу между событиями\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(e1));\n",
        "    CUDA_CHECK(cudaEventDestroy(e2));\n",
        "\n",
        "    return ms;                                      // возвращаем время kernel в ms\n",
        "}\n",
        "\n",
        "// Быстрая проверка корректности результата\n",
        "// Мы не проверяем весь массив, а смотрим только несколько случайных позиций\n",
        "bool quickCheck(const vector<float>& A, const vector<float>& B, const vector<float>& C) {\n",
        "\n",
        "    // Выбираем несколько индексов для проверки\n",
        "    int idxs[] = {0, 1, 2, 123, 999, 500000, 999999};\n",
        "\n",
        "    // Проходим по выбранным индексам\n",
        "    for (int idx : idxs) {\n",
        "\n",
        "        // Если вдруг индекс больше размера массива, просто пропускаем его\n",
        "        if (idx >= (int)A.size()) continue;\n",
        "\n",
        "        // Считаем, каким должен быть результат\n",
        "        float expected = A[idx] + B[idx];\n",
        "\n",
        "        // Берём фактическое значение из GPU-результата\n",
        "        float got = C[idx];\n",
        "\n",
        "        // Если разница слишком большая — считаем, что ошибка\n",
        "        // Допускаем маленькую погрешность из-за float\n",
        "        if (fabs(expected - got) > 1e-3f) return false;\n",
        "    }\n",
        "\n",
        "    // Если все проверенные элементы совпали - считаем, что всё корректно\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    const int N = 1'000'000;                         // Размер массивов (по заданию)\n",
        "    vector<int> blocks = {128, 256, 512};            // Три разных размера блока для эксперимента\n",
        "\n",
        "    // Создаём массивы на CPU\n",
        "    vector<float> hA(N), hB(N), hC(N);\n",
        "\n",
        "    // Генератор случайных чисел с фиксированным seed (чтобы результаты были воспроизводимыми)\n",
        "    mt19937 gen(42);\n",
        "    uniform_real_distribution<float> dist(0.0f, 100.0f);\n",
        "\n",
        "    // Заполняем массивы случайными числами\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        hA[i] = dist(gen);\n",
        "        hB[i] = dist(gen);\n",
        "    }\n",
        "\n",
        "    // Указатели на память на GPU\n",
        "    float *dA = nullptr, *dB = nullptr, *dC = nullptr;\n",
        "\n",
        "    // Выделяем память на GPU\n",
        "    CUDA_CHECK(cudaMalloc(&dA, N * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&dB, N * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&dC, N * sizeof(float)));\n",
        "\n",
        "    // Копируем A и B на GPU (это одинаково для всех тестов)\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cout << \"\\nVector addition on GPU\\n\";\n",
        "    cout << \"Array size N = \" << N << \"\\n\\n\";\n",
        "\n",
        "    cout << left << setw(12) << \"Block size\"\n",
        "     << setw(18) << \"Kernel time (ms)\"\n",
        "     << \"\\n\";\n",
        "\n",
        "    // Переменные для поиска лучшего результата\n",
        "    float bestTime = 1e9f;\n",
        "    int bestBlock = -1;\n",
        "\n",
        "    // Перебираем разные размеры блока\n",
        "    for (int bs : blocks) {\n",
        "\n",
        "        // Обнуляем массив результата на GPU перед каждым запуском\n",
        "        CUDA_CHECK(cudaMemset(dC, 0, N * sizeof(float)));\n",
        "\n",
        "        // Замеряем время работы kernel для текущего block size\n",
        "        float ms = timeKernelVecAdd(dA, dB, dC, N, bs);\n",
        "\n",
        "        // Копируем результат обратно на CPU для проверки\n",
        "        CUDA_CHECK(cudaMemcpy(hC.data(), dC, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "        // Проверяем, правильно ли выполнено сложение\n",
        "        bool ok = quickCheck(hA, hB, hC);\n",
        "\n",
        "        // Если это лучшее время - запоминаем его\n",
        "        if (ms < bestTime) {\n",
        "            bestTime = ms;\n",
        "            bestBlock = bs;\n",
        "        }\n",
        "\n",
        "        // Печатаем результат в таблицу\n",
        "        cout << left << setw(12) << bs\n",
        "             << setw(18) << fixed << setprecision(6) << ms\n",
        "             << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Печатаем лучший размер блока\n",
        "    cout << \"\\nBest block size: \" << bestBlock\n",
        "         << \" with time \" << fixed << setprecision(6) << bestTime << \" ms\\n\";\n",
        "\n",
        "    // Освобождаем память на GPU\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4YU_r3seKAV",
        "outputId": "7a869bf8-a2b2-47a7-bc69-05adda4b4606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vec_add_blocks.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 vec_add_blocks.cu -o vec_add_blocks\n",
        "!./vec_add_blocks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-v1A7VymXHK",
        "outputId": "92c017d6-4bd2-46e8-b973-7de23933e9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vector addition on GPU\n",
            "Array size N = 1000000\n",
            "\n",
            "Block size  Kernel time (ms)  \n",
            "128         0.143232          \n",
            "256         0.051776          \n",
            "512         0.053184          \n",
            "\n",
            "Best block size: 256 with time 0.051776 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Вывод**"
      ],
      "metadata": {
        "id": "WNYuui-4s2gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном задании была реализована CUDA-программа для поэлементного сложения двух массивов и исследовано влияние размера блока потоков на производительность. Эксперименты проводились для массива размером 1 000 000 элементов при размерах блока 128, 256 и 512 потоков.\n",
        "\n",
        "Результаты показали, что наилучшее время выполнения было достигнуто при размере блока 256 потоков. При меньшем размере блока GPU используется не полностью, так как создаётся недостаточно потоков для эффективного скрытия задержек доступа к памяти. При большем размере блока возрастает конкуренция за аппаратные ресурсы, такие как регистры и разделяемая память, что снижает эффективность выполнения.\n",
        "\n",
        "Таким образом, оптимальный размер блока потоков является компромиссом между степенью параллелизма и затратами на использование ресурсов GPU. Полученные результаты подтверждают, что правильный выбор параметров запуска CUDA-ядра существенно влияет на производительность вычислений."
      ],
      "metadata": {
        "id": "I9yDIAV0skvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 3"
      ],
      "metadata": {
        "id": "F-v1W7Zjjpw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coalescing.cu\n",
        "#include <iostream>            // Для вывода информации в консоль (cout)\n",
        "#include <vector>              // Контейнер vector для хранения массива на CPU\n",
        "#include <random>              // Генерация случайных чисел\n",
        "#include <cuda_runtime.h>      // CUDA runtime API и CUDA events\n",
        "#include <iomanip>             // Для красивого вывода таблиц\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Макрос: проверка ошибок CUDA-вызовов (если ошибка - печатаем и завершаем программу)\n",
        "#define CUDA_CHECK(x) do { \\\n",
        "  cudaError_t e = (x); \\\n",
        "  if (e != cudaSuccess) { \\\n",
        "    cout << \"CUDA error: \" << cudaGetErrorString(e) \\\n",
        "         << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "// Коалесцированный доступ: поток i берет элемент i (соседи читают соседей)\n",
        "__global__ void coalescedKernel(const float* in, float* out, int N) {\n",
        "\n",
        "// Вычисляем глобальный индекс потока\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "// Проверяем, чтобы не выйти за границы массива\n",
        "    if (i < N) {\n",
        "        // Поток читает in[i] и записывает в out[i]\n",
        "        // Соседние потоки читают соседние адреса -> коалесцированный доступ\n",
        "        out[i] = in[i] * 2.0f;\n",
        "}\n",
        "\n",
        "// Некоалесцированный доступ: поток i берет элемент j (скачки по памяти), потоки читают данные \"вразнобой\"\n",
        "\n",
        "__global__ void nonCoalescedKernel(const float* in, float* out, int N, int STRIDE) {\n",
        "\n",
        "// Глобальный индекс потока\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < N) {\n",
        "\n",
        "        // Вычисляем \"разбросанный\" индекс\n",
        "        // (i * STRIDE) % N гарантирует, что мы останемся внутри массива\n",
        "        int j = (i * STRIDE) % N;\n",
        "\n",
        "        // Каждый поток читает и пишет в \"случайное\" место в памяти\n",
        "        // Это ломает коалесцинг и делает доступ медленным\n",
        "        out[j] = in[j] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Функция измерения времени: коалесцированный kernel\n",
        "\n",
        "float timeKernelCoalesced(const float* dIn, float* dOut, int N,\n",
        "                          dim3 grid, dim3 block, int repeats) {\n",
        "\n",
        "    // Создаём два CUDA-события: начало и конец\n",
        "    cudaEvent_t e1, e2;\n",
        "    CUDA_CHECK(cudaEventCreate(&e1));\n",
        "    CUDA_CHECK(cudaEventCreate(&e2));\n",
        "\n",
        "    // Фиксируем момент старта\n",
        "    CUDA_CHECK(cudaEventRecord(e1));\n",
        "\n",
        "    // Запускаем kernel много раз (repeats), чтобы получить стабильный замер\n",
        "    for (int r = 0; r < repeats; ++r) {\n",
        "        coalescedKernel<<<grid, block>>>(dIn, dOut, N);\n",
        "    }\n",
        "\n",
        "    // Проверяем, что kernel запустился без ошибок\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "    // Фиксируем момент окончания\n",
        "    CUDA_CHECK(cudaEventRecord(e2));\n",
        "\n",
        "    // Ждём, пока всё выполнится\n",
        "    CUDA_CHECK(cudaEventSynchronize(e2));\n",
        "\n",
        "    // Переменная для времени (в миллисекундах)\n",
        "    float ms = 0.0f;\n",
        "\n",
        "    // Считаем разницу между e1 и e2\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, e1, e2));\n",
        "\n",
        "    // Удаляем события\n",
        "    CUDA_CHECK(cudaEventDestroy(e1));\n",
        "    CUDA_CHECK(cudaEventDestroy(e2));\n",
        "\n",
        "    // Возвращаем время выполнения\n",
        "    return ms;\n",
        "}\n",
        "\n",
        "\n",
        "// Функция измерения времени: некоалесцированный Kernel\n",
        "\n",
        "float timeKernelNonCoalesced(const float* dIn, float* dOut, int N,\n",
        "                             dim3 grid, dim3 block, int repeats, int stride) {\n",
        "\n",
        "    cudaEvent_t e1, e2;\n",
        "    CUDA_CHECK(cudaEventCreate(&e1));\n",
        "    CUDA_CHECK(cudaEventCreate(&e2));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(e1));\n",
        "\n",
        "    // Запускаем kernel много раз для стабильности\n",
        "    for (int r = 0; r < repeats; ++r) {\n",
        "        nonCoalescedKernel<<<grid, block>>>(dIn, dOut, N, stride);\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventRecord(e2));\n",
        "    CUDA_CHECK(cudaEventSynchronize(e2));\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, e1, e2));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(e1));\n",
        "    CUDA_CHECK(cudaEventDestroy(e2));\n",
        "\n",
        "    return ms;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    const int N = 1'000'000;          // размер массива по заданию\n",
        "    const int BLOCK = 256;            // стандартный удобный размер блока\n",
        "    const int REPEATS = 500;          // повторяем kernel много раз для точности\n",
        "    const int STRIDE = 997;           // шаг доступа (некратный 32), чтобы сломать коалесцинг\n",
        "\n",
        "    // Создаём массив на CPU\n",
        "    vector<float> h(N);\n",
        "\n",
        "    // Генератор случайных чисел\n",
        "    mt19937 gen(42);\n",
        "    uniform_real_distribution<float> dist(0.0f, 100.0f);\n",
        "\n",
        "    // Заполняем массив случайными числами\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h[i] = dist(gen);\n",
        "    }\n",
        "\n",
        "    // Указатели на память на GPU\n",
        "    float *dIn = nullptr, *dOut = nullptr;\n",
        "\n",
        "    // Выделяем память на GPU\n",
        "    CUDA_CHECK(cudaMalloc(&dIn, N * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&dOut, N * sizeof(float)));\n",
        "\n",
        "    // Копируем данные с CPU на GPU\n",
        "    CUDA_CHECK(cudaMemcpy(dIn, h.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Настройки запуска kernel\n",
        "    dim3 block(BLOCK);                       // сколько потоков в блоке\n",
        "    dim3 grid((N + BLOCK - 1) / BLOCK);     // сколько блоков нужно, чтобы покрыть N\n",
        "\n",
        "\n",
        "    // Первый запуск иногда медленнее из-за инициализации,\n",
        "    // поэтому мы его не учитываем в замере\n",
        "\n",
        "    coalescedKernel<<<grid, block>>>(dIn, dOut, N);\n",
        "    nonCoalescedKernel<<<grid, block>>>(dIn, dOut, N, STRIDE);\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Реальный замер времени\n",
        "    float tCoalesced = timeKernelCoalesced(dIn, dOut, N, grid, block, REPEATS);\n",
        "    float tNon       = timeKernelNonCoalesced(dIn, dOut, N, grid, block, REPEATS, STRIDE);\n",
        "\n",
        "    // Результаты\n",
        "\n",
        "    cout << \"Array size N = \" << N << \"\\n\";\n",
        "    cout << \"Block size = \" << BLOCK << \"\\n\";\n",
        "    cout << \"Repeats = \" << REPEATS << \"\\n\";\n",
        "    cout << \"Stride (non-coalesced) = \" << STRIDE << \"\\n\\n\";\n",
        "\n",
        "    cout << fixed << setprecision(6);\n",
        "    cout << \"Coalesced time (ms):     \" << tCoalesced << \"\\n\";\n",
        "    cout << \"Non-coalesced time (ms): \" << tNon << \"\\n\";\n",
        "\n",
        "    if (tCoalesced > 0.0f) {\n",
        "        cout << \"Slowdown (Non/Coalesced): \" << (tNon / tCoalesced) << \"x\\n\";\n",
        "    }\n",
        "\n",
        "    // Освобождаем память на GPU\n",
        "    CUDA_CHECK(cudaFree(dIn));\n",
        "    CUDA_CHECK(cudaFree(dOut));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiYVSlUns_f2",
        "outputId": "0f0e0759-bb57-4fca-a725-049a69db84f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coalescing.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 coalescing.cu -o coalescing\n",
        "!./coalescing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkJ3CJFFvAqh",
        "outputId": "8e16aa1d-110d-4e72-c06d-26f7475d90a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array size N = 1000000\n",
            "Block size = 256\n",
            "Repeats = 500\n",
            "Stride (non-coalesced) = 997\n",
            "\n",
            "Coalesced time (ms):     19.030016\n",
            "Non-coalesced time (ms): 320.446289\n",
            "Slowdown (Non/Coalesced): 16.838993x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Вывод**\n",
        "\n",
        "В работе была реализована обработка массива на GPU с двумя вариантами доступа к глобальной памяти: коалесцированным и некоалесцированным. Эксперимент проведён для массива размером 1 000 000 элементов при размере блока 256 потоков, время измерялось по 500 повторениям для повышения стабильности результатов.\n",
        "\n",
        "Результаты показали, что коалесцированный доступ выполняется значительно быстрее: 19.03 ms против 320.45 ms для некоалесцированного доступа. Некоалесцированный вариант оказался медленнее примерно в 16.84 раза. Это объясняется тем, что при коалесцированном доступе соседние потоки обращаются к соседним адресам памяти, и GPU может объединять запросы в меньшее число транзакций. При некоалесцированном доступе потоки обращаются к памяти с большим шагом, что приводит к большему числу отдельных обращений и снижает эффективность использования пропускной способности памяти.\n",
        "\n",
        "Таким образом, эксперимент подтверждает, что правильная организация доступа к глобальной памяти (коалесцирование) является критически важной для производительности CUDA-программ."
      ],
      "metadata": {
        "id": "jWFGHOqaYF5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 4"
      ],
      "metadata": {
        "id": "ldrEw1_EjvKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cu\n",
        "#include <iostream>              // вывод в консоль\n",
        "#include <vector>                // массивы на CPU\n",
        "#include <random>                // генерация случайных чисел\n",
        "#include <cuda_runtime.h>        // CUDA runtime API (cudaMalloc, cudaMemcpy, events)\n",
        "#include <iomanip>               // красивое форматирование таблицы (setw, setprecision)\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Макрос для проверки ошибок CUDA (если ошибка, печатаем и выходим)\n",
        "#define CUDA_CHECK(x) do { \\\n",
        "  cudaError_t e = (x); \\\n",
        "  if (e != cudaSuccess) { \\\n",
        "    cout << \"CUDA error: \" << cudaGetErrorString(e) \\\n",
        "         << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "\n",
        "// Берём программу из задания 2 (поэлементное сложение массивов)\n",
        "// и подбираем оптимальный размер блока (block size), а grid size считаем автоматически.\n",
        "\n",
        "// CUDA kernel: поэлементное сложение двух массивов: C[i] = A[i] + B[i]\n",
        "__global__ void vecAdd(const float* A, const float* B, float* C, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;   // глобальный индекс элемента для данного потока\n",
        "    if (i < N) {                                     // защита от выхода за границы массива\n",
        "        C[i] = A[i] + B[i];                          // основная операция: сложение\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Функция для замера времени выполнения kernel через CUDA events\n",
        "// Важно: делаем несколько повторов и возвращаем среднее время одного запуска\n",
        "float timeKernelVecAdd(const float* dA, const float* dB, float* dC, int N,\n",
        "                       int blockSize, int repeats) {\n",
        "\n",
        "    dim3 block(blockSize);                           // block: сколько потоков в одном блоке\n",
        "    dim3 grid((N + block.x - 1) / block.x);          // grid: сколько блоков нужно, чтобы покрыть N элементов\n",
        "\n",
        "    // Прогрев, чтобы первый запуск не дал \"лишнее\" время\n",
        "    vecAdd<<<grid, block>>>(dA, dB, dC, N);\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // CUDA events для измерения времени на GPU\n",
        "    cudaEvent_t e1, e2;\n",
        "    CUDA_CHECK(cudaEventCreate(&e1));\n",
        "    CUDA_CHECK(cudaEventCreate(&e2));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(e1));                 // отметили старт на GPU таймлайне\n",
        "\n",
        "    for (int r = 0; r < repeats; ++r) {              // повторяем kernel несколько раз для устойчивости\n",
        "        vecAdd<<<grid, block>>>(dA, dB, dC, N);      // запуск kernel\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());                  // проверка: kernel запустился без ошибок\n",
        "    CUDA_CHECK(cudaEventRecord(e2));                 // отметили конец\n",
        "    CUDA_CHECK(cudaEventSynchronize(e2));            // ждём завершения всех запусков kernel\n",
        "\n",
        "    float totalMs = 0.0f;                            // суммарное время за все repeats\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&totalMs, e1, e2));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(e1));                // чистим события\n",
        "    CUDA_CHECK(cudaEventDestroy(e2));\n",
        "\n",
        "    return totalMs / repeats;                        // возвращаем среднее время одного запуска\n",
        "}\n",
        "\n",
        "\n",
        "// Утилита: печать одной строки таблицы\n",
        "void printRow(int blockSize, int gridX, double ms) {\n",
        "    cout << left << setw(12) << blockSize            // размер блока\n",
        "         << setw(16) << gridX                        // grid.x (количество блоков)\n",
        "         << fixed << setprecision(6) << ms << \"\\n\";  // время (мс)\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;                         // число элементов\n",
        "    const int repeats = 300;                         // сколько раз повторяем kernel для замера\n",
        "\n",
        "    // Генерация входных данных на CPU\n",
        "    vector<float> hA(N), hB(N);                      // два входных массива\n",
        "    mt19937 gen(42);                                 // фиксированный seed для повторяемости результатов\n",
        "    uniform_real_distribution<float> dist(0.0f, 1.0f);// случайные числа [0;1]\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {                    // заполняем массивы случайными значениями\n",
        "        hA[i] = dist(gen);\n",
        "        hB[i] = dist(gen);\n",
        "    }\n",
        "\n",
        "    // Выделяем память на GPU\n",
        "    float *dA = nullptr, *dB = nullptr, *dC = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&dA, N * sizeof(float)));  // GPU память под A\n",
        "    CUDA_CHECK(cudaMalloc(&dB, N * sizeof(float)));  // GPU память под B\n",
        "    CUDA_CHECK(cudaMalloc(&dC, N * sizeof(float)));  // GPU память под C (результат)\n",
        "\n",
        "    // Копируем данные на GPU\n",
        "    CUDA_CHECK(cudaMemcpy(dA, hA.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(dB, hB.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Кандидаты block size (типичные значения, кратные warp=32)\n",
        "    vector<int> candidates = {64, 128, 256, 512, 1024};\n",
        "\n",
        "    cout << \"Vector addition tuning (Task 4)\\n\";\n",
        "    cout << \"Array size N = \" << N << \"\\n\\n\";\n",
        "\n",
        "    // Подбор оптимальной конфигурации: ищем лучший block size по времени\n",
        "    cout << left << setw(12) << \"Block size\"\n",
        "         << setw(16) << \"Grid.x\"\n",
        "         << \"Kernel time (ms)\\n\";\n",
        "\n",
        "    double bestTime = 1e100;                         // лучшее время\n",
        "    int bestBlock = -1;                              // лучший размер блока\n",
        "    int bestGridX = -1;                              // сколько блоков по x у лучшей конфигурации\n",
        "\n",
        "    for (int bs : candidates) {                      // пробуем каждый block size\n",
        "        dim3 block(bs);                              // создаём block\n",
        "        dim3 grid((N + block.x - 1) / block.x);      // считаем grid.x\n",
        "\n",
        "        double t = timeKernelVecAdd(dA, dB, dC, N, bs, repeats);  // замеряем время\n",
        "\n",
        "        printRow(bs, grid.x, t);                     // печатаем строку в таблице\n",
        "\n",
        "        if (t < bestTime) {                          // обновляем лучший вариант\n",
        "            bestTime = t;\n",
        "            bestBlock = bs;\n",
        "            bestGridX = grid.x;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Сравнение \"неоптимальной\" и \"оптимальной\" конфигурации\n",
        "    // В качестве неоптимальной обычно берут слишком маленький block size\n",
        "    int badBlock = 64;                               // пример неоптимального (маленький блок)\n",
        "    dim3 badGrid((N + badBlock - 1) / badBlock);     // grid для badBlock\n",
        "    double badTime = timeKernelVecAdd(dA, dB, dC, N, badBlock, repeats);\n",
        "\n",
        "    cout << \"\\nComparison (Task 4)\\n\";\n",
        "    cout << \"Non-optimal config: block = \" << badBlock\n",
        "         << \", grid.x = \" << badGrid.x\n",
        "         << \", time = \" << fixed << setprecision(6) << badTime << \" ms\\n\";\n",
        "\n",
        "    cout << \"Optimal config:     block = \" << bestBlock\n",
        "         << \", grid.x = \" << bestGridX\n",
        "         << \", time = \" << fixed << setprecision(6) << bestTime << \" ms\\n\";\n",
        "\n",
        "    // Освобождаем память на GPU\n",
        "    CUDA_CHECK(cudaFree(dA));\n",
        "    CUDA_CHECK(cudaFree(dB));\n",
        "    CUDA_CHECK(cudaFree(dC));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt2hJjZuYFcz",
        "outputId": "657aff28-77c2-4644-de6d-c60394e03a76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 task4.cu -o task4\n",
        "!./task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG0jZJX7fhiU",
        "outputId": "b7c43d31-1a8a-4a11-89e8-bcb86a3275f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector addition tuning (Task 4)\n",
            "Array size N = 1000000\n",
            "\n",
            "Block size  Grid.x          Kernel time (ms)\n",
            "64          15625           0.050439\n",
            "128         7813            0.049268\n",
            "256         3907            0.049282\n",
            "512         1954            0.049417\n",
            "1024        977             0.051004\n",
            "\n",
            "Comparison (Task 4)\n",
            "Non-optimal config: block = 64, grid.x = 15625, time = 0.050438 ms\n",
            "Optimal config:     block = 128, grid.x = 7813, time = 0.049268 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Вывод**\n",
        "В качестве базовой программы для подбора оптимальной конфигурации было выбрано Задание 2, поэлементное сложение двух массивов.  Эксперимент проводился для массива размером 1 000 000 элементов, при этом выполнялся перебор нескольких значений размера блока потоков (64, 128, 256, 512, 1024), а размер сетки рассчитывался автоматически так, чтобы покрыть весь массив.\n",
        "\n",
        "По результатам измерений наименьшее время выполнения показал блок размером 128 потоков (0.049268 мс), что было принято в качестве оптимальной конфигурации. В качестве неоптимальной конфигурации был выбран меньший размер блока 64 потока, при котором время выполнения составило 0.050438 мс.\n",
        "\n",
        "Сравнение показало, что оптимизированная конфигурация (block = 128, grid.x = 7813) обеспечивает немного более высокую производительность по сравнению с неоптимальной (block = 64, grid.x = 15625). Это объясняется более эффективной загрузкой GPU и лучшим балансом между количеством потоков в блоке и числом блоков в сетке.\n",
        "\n",
        "Таким образом, эксперимент подтвердил, что выбор размера блока потоков влияет на производительность CUDA-программы, и подбор оптимальной конфигурации позволяет улучшить время выполнения."
      ],
      "metadata": {
        "id": "MgYAZ7O4i9Yz"
      }
    }
  ]
}