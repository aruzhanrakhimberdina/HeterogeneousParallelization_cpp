{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Assignment 2**\n",
        "##Задача 4"
      ],
      "metadata": {
        "id": "ziNMj-Zu4-pC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритм состоит из следующих этапов:\n",
        "\n",
        "массив разбивается на подмассивы, каждый из которых сортируется отдельно на GPU;\n",
        "\n",
        "далее выполняются параллельные проходы слияния;\n",
        "\n",
        "измеряется время выполнения для массивов размером 10 000 и 100 000 элементов."
      ],
      "metadata": {
        "id": "elNl32Yk6tqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzBjo-omZc38",
        "outputId": "dfaabe94-0a5c-429d-ae0f-980c003261cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 30 05:17:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_merge_sort.cu\n",
        "#include <iostream>                 // cout/cin для вывода в консоль\n",
        "#include <vector>                   // vector<int> для хранения массива на CPU\n",
        "#include <algorithm>                // sort() для эталонной CPU-сортировки\n",
        "#include <random>                   // mt19937 и распределение для генерации случайных чисел\n",
        "#include <chrono>                   // измерение времени на CPU\n",
        "#include <climits>                  // INT_MAX / INT_MIN для паддинга и граничных значений\n",
        "#include <cuda_runtime.h>           // CUDA runtime: cudaMalloc/cudaMemcpy/cudaEvent и т.д.\n",
        "\n",
        "using namespace std;                // чтобы не писать std:: везде\n",
        "\n",
        "static const int CHUNK = 1024;      // размер \"куска\" (chunk), который сортирует один CUDA-блок; степень 2 нужна для bitonic sort\n",
        "static const int THREADS = 256;     // количество потоков в блоке для merge kernel\n",
        "\n",
        "// Макрос: проверка ошибок CUDA-вызовов (если ошибка - печатаем и завершаем программу)\n",
        "#define CUDA_CHECK(x) do{ cudaError_t e=(x); if(e!=cudaSuccess){ \\\n",
        "  printf(\"CUDA error %s:%d: %s\\n\",__FILE__,__LINE__,cudaGetErrorString(e)); exit(1);} }while(0)\n",
        "\n",
        "// Сортировка чанков (bitonic внутри блока)\n",
        "\n",
        "// kernel: каждый CUDA-блок сортирует свой подмассив длины CHUNK, используя shared memory (быстро, потому что shared быстрее global)\n",
        "__global__ void bitonicSortChunks(int* d, int N) {                    // d - массив в глобальной памяти GPU, N - реальный размер массива\n",
        "    __shared__ int s[CHUNK];                                          // shared memory: общий буфер для потоков текущего блока (вмещает CHUNK чисел)\n",
        "    int base = blockIdx.x * CHUNK;                                    // base - начало чанка, который обрабатывает этот блок\n",
        "\n",
        "    for (int i = threadIdx.x; i < CHUNK; i += blockDim.x) {           // каждый поток грузит несколько элементов чанка (шаг = blockDim.x)\n",
        "        int idx = base + i;                                           // idx - глобальный индекс элемента в массиве d\n",
        "        s[i] = (idx < N) ? d[idx] : INT_MAX;                          // если вышли за границы - кладём INT_MAX (паддинг), чтобы сортировка не ломалась\n",
        "    }\n",
        "    __syncthreads();                                                  // синхронизация: все данные должны оказаться в shared перед сортировкой\n",
        "\n",
        "    for (int k = 2; k <= CHUNK; k <<= 1) {                            // k - размер bitonic-последовательности (растёт: 2,4,8,...CHUNK)\n",
        "        for (int j = k >> 1; j > 0; j >>= 1) {                        // j - “дистанция” для сравнения пар (уменьшается внутри стадии)\n",
        "            for (int i = threadIdx.x; i < CHUNK; i += blockDim.x) {   // каждый поток обрабатывает свой набор i (часть элементов)\n",
        "                int ixj = i ^ j;                                      // ixj - индекс “пары” для i\n",
        "                if (ixj > i) {                                        // сравниваем пару только один раз, чтобы не дублировать работу\n",
        "                    bool up = ((i & k) == 0);                         // up=true: сортируем по возрастанию, иначе по убыванию (зависит от стадии k)\n",
        "                    int a = s[i], b = s[ixj];                         // читаем два значения из shared, которые нужно сравнить\n",
        "                    if ((up && a > b) || (!up && a < b)) {            // если порядок неверный для выбранного направления\n",
        "                        s[i] = b;                                     // меняем местами (swap вручную, чтобы не тянуть std::swap на device)\n",
        "                        s[ixj] = a;                                   // вторая часть обмена\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            __syncthreads();                                          // важная синхронизация после каждого шага - иначе потоки будут мешать друг другу\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int i = threadIdx.x; i < CHUNK; i += blockDim.x) {           // после сортировки записываем результат обратно в global memory\n",
        "        int idx = base + i;                                           // глобальный индекс для записи\n",
        "        if (idx < N) d[idx] = s[i];                                   // пишем только реальные элементы (без INT_MAX паддинга)\n",
        "    }\n",
        "}\n",
        "\n",
        "// Параллельное слияние (merge-path)\n",
        "\n",
        "// device-функция merge-path: находит, сколько элементов нужно взять из A для позиции k в объединённом массиве\n",
        "// Мы хотим out[k]. Пусть i элементов берём из A, тогда j=k-i берём из B. Ищем i бинарным поиском.\n",
        "__device__ __forceinline__ int mergePath(const int* A,int m,const int* B,int n,int k){\n",
        "    int lo = max(0, k - n);                                           // минимально возможное i (если взять слишком много из B, выйдем за границы)\n",
        "    int hi = min(k, m);                                               // максимально возможное i (нельзя взять больше m элементов из A)\n",
        "    while (lo < hi) {                                                 // бинарный поиск по i\n",
        "        int i = (lo + hi) >> 1;                                       // пробуем i примерно посередине\n",
        "        int j = k - i;                                                // тогда из B берём j\n",
        "\n",
        "        int Ai   = (i < m) ? A[i] : INT_MAX;                          // A[i] или +∞ если i==m (за правой границей)\n",
        "        int Aim1 = (i > 0) ? A[i-1] : INT_MIN;                        // A[i-1] или -∞ если i==0 (за левой границей)\n",
        "        int Bj   = (j < n) ? B[j] : INT_MAX;                          // B[j] или +∞ если j==n\n",
        "        int Bjm1 = (j > 0) ? B[j-1] : INT_MIN;                        // B[j-1] или -∞ если j==0\n",
        "\n",
        "        if (Ai < Bjm1) lo = i + 1;                                    // Ai слишком маленький → взяли мало из A → увеличиваем i\n",
        "        else if (Aim1 > Bj) hi = i;                                   // Aim1 слишком большой → взяли много из A → уменьшаем i\n",
        "        else return i;                                                // нашли корректную точку разбиения (i подходит)\n",
        "    }\n",
        "    return lo;                                                        // возвращаем найденное i\n",
        "}\n",
        "\n",
        "// kernel merge: сливает пары сегментов длины width; каждый поток пишет один элемент результата (позицию k)\n",
        "__global__ void mergePass(const int* in, int* out, int N, int width) {\n",
        "    int pairId = blockIdx.x;                                          // номер пары сегментов (по оси x): 0,1,2...\n",
        "    int base = pairId * (2 * width);                                  // начало пары: [base ... base+2*width)\n",
        "\n",
        "    int m = max(0, min(width, N - base));                             // реальная длина левого сегмента (учитываем, что в конце может быть меньше width)\n",
        "    int n = max(0, min(width, N - (base + width)));                   // реальная длина правого сегмента\n",
        "    int outLen = m + n;                                               // сколько всего элементов будет после слияния (левая+правая часть)\n",
        "\n",
        "    int k = blockIdx.y * blockDim.x + threadIdx.x;                    // позиция k в объединённом сегменте, которую считает данный поток\n",
        "    if (k >= outLen) return;                                          // если поток “вылез” за длину - он ничего не делает\n",
        "\n",
        "    const int* A = in + base;                                         // указатель на начало левого сегмента во входном массиве\n",
        "    const int* B = in + base + width;                                 // указатель на начало правого сегмента\n",
        "\n",
        "    int i = mergePath(A, m, B, n, k);                                 // сколько элементов нужно “взять” из A до позиции k\n",
        "    int j = k - i;                                                    // сколько элементов взято из B\n",
        "\n",
        "    int a = (i < m) ? A[i] : INT_MAX;                                 // кандидат из A для позиции k\n",
        "    int b = (j < n) ? B[j] : INT_MAX;                                 // кандидат из B для позиции k\n",
        "\n",
        "    out[base + k] = (a <= b) ? a : b;                                 // выбираем меньший кандидат и записываем в выход\n",
        "}\n",
        "\n",
        "// Хост-функция: GPU MERGE SORT\n",
        "\n",
        "// Функция на CPU: выделяет память на GPU, запускает kernels и возвращает результат обратно на CPU\n",
        "static void gpuMergeSort(vector<int>& h) {\n",
        "    int N = (int)h.size();                                            // размер массива на CPU\n",
        "\n",
        "    int *dA=nullptr, *dB=nullptr;                                     // два буфера на GPU: один для чтения, второй для записи (ping-pong)\n",
        "    CUDA_CHECK(cudaMalloc(&dA, N * sizeof(int)));                     // выделяем память под dA на GPU\n",
        "    CUDA_CHECK(cudaMalloc(&dB, N * sizeof(int)));                     // выделяем память под dB на GPU\n",
        "    CUDA_CHECK(cudaMemcpy(dA, h.data(), N * sizeof(int), cudaMemcpyHostToDevice)); // копируем данные CPU → GPU в dA\n",
        "\n",
        "    int numChunks = (N + CHUNK - 1) / CHUNK;                          // сколько блоков нужно, чтобы покрыть N элементов чанками по CHUNK\n",
        "    bitonicSortChunks<<<numChunks, 256>>>(dA, N);                     // сортируем каждый чанк отдельно (получаем много маленьких отсортированных сегментов)\n",
        "    CUDA_CHECK(cudaGetLastError());                                   // проверяем, не было ли ошибки запуска kernel\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());                              // ждём, пока kernel полностью завершится\n",
        "\n",
        "    int width = CHUNK;                                                // текущая длина отсортированного сегмента (сначала = CHUNK)\n",
        "    bool ping = true;                                                 // ping=true: in=dA out=dB, потом меняем местами\n",
        "\n",
        "    while (width < N) {                                               // продолжаем, пока сегменты не станут размером хотя бы N\n",
        "        int numPairs = (N + (2 * width) - 1) / (2 * width);           // сколько пар сегментов нужно слить (каждая пара даёт новый сегмент 2*width)\n",
        "\n",
        "        int maxOutLen = min(2 * width, N);                            // максимальная длина результата в одной паре (обычно 2*width, но N может быть меньше)\n",
        "        int blocksY = (maxOutLen + THREADS - 1) / THREADS;            // сколько блоков по оси y нужно, чтобы покрыть все k (один поток = один k)\n",
        "\n",
        "        dim3 block(THREADS);                                          // конфигурация: THREADS потоков в одном блоке\n",
        "        dim3 grid(numPairs, blocksY);                                 // grid.x - пары сегментов, grid.y - “слои” по k внутри пары\n",
        "\n",
        "        const int* in = ping ? dA : dB;                               // выбираем, откуда читаем (вход)\n",
        "        int* out = ping ? dB : dA;                                    // выбираем, куда пишем (выход)\n",
        "\n",
        "        mergePass<<<grid, block>>>(in, out, N, width);                // выполняем один проход слияния для всех пар сегментов\n",
        "        CUDA_CHECK(cudaGetLastError());                               // проверяем запуск merge kernel\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());                          // ждём завершения merge kernel\n",
        "\n",
        "        ping = !ping;                                                 // меняем буферы местами на следующий проход\n",
        "        width *= 2;                                                   // после merge сегменты удваиваются\n",
        "    }\n",
        "\n",
        "    int* dRes = ping ? dA : dB;                                       // выбираем, в каком буфере лежит конечный результат\n",
        "    CUDA_CHECK(cudaMemcpy(h.data(), dRes, N * sizeof(int), cudaMemcpyDeviceToHost)); // копируем результат GPU на CPU\n",
        "\n",
        "    CUDA_CHECK(cudaFree(dA));                                         // освобождаем память dA на GPU\n",
        "    CUDA_CHECK(cudaFree(dB));                                         // освобождаем память dB на GPU\n",
        "}\n",
        "\n",
        "// Main: генерация данных и сравнение CPU/GPU\n",
        "\n",
        "int main() {\n",
        "    vector<int> sizes = {10000, 100000};                              // размеры массивов, которые требуются в задании\n",
        "\n",
        "    mt19937 gen(random_device{}());                                   // генератор случайных чисел\n",
        "    uniform_int_distribution<int> dist(0, 100000);                    // распределение: числа от 0 до 100000\n",
        "\n",
        "    for (int N : sizes) {                                             // прогоняем эксперимент для каждого N\n",
        "        cout << \"Размер массива N = \" << N << \"\\n\";                   // печатаем текущий размер\n",
        "\n",
        "        vector<int> a(N);                                             // создаём исходный массив на CPU\n",
        "        for (int i = 0; i < N; ++i) a[i] = dist(gen);                 // заполняем массив случайными значениями\n",
        "\n",
        "        vector<int> cpu = a, gpu = a;                                 // делаем две копии: одна пойдёт в CPU sort, другая - в GPU sort\n",
        "\n",
        "        auto c1 = chrono::high_resolution_clock::now();               // начало замера CPU времени\n",
        "        sort(cpu.begin(), cpu.end());                                 // сортировка на CPU (эталон / сравнение)\n",
        "        auto c2 = chrono::high_resolution_clock::now();               // конец замера CPU времени\n",
        "        double cpu_ms = chrono::duration<double, milli>(c2 - c1).count(); // переводим длительность в миллисекунды\n",
        "\n",
        "        cudaEvent_t e1, e2;                                           // CUDA events - более корректный способ мерить время на GPU\n",
        "        CUDA_CHECK(cudaEventCreate(&e1));                              // создаём событие начала\n",
        "        CUDA_CHECK(cudaEventCreate(&e2));                              // создаём событие конца\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(e1));                               // ставим “метку времени” на GPU перед запуском сортировки\n",
        "        gpuMergeSort(gpu);                                            // выполняем сортировку на GPU (внутри: kernels + копирования)\n",
        "        CUDA_CHECK(cudaEventRecord(e2));                               // ставим “метку времени” на GPU после сортировки\n",
        "        CUDA_CHECK(cudaEventSynchronize(e2));                          // ждём, пока событие e2 реально завершится (значит GPU закончил работу)\n",
        "\n",
        "        float gpu_ms = 0.0f;                                          // сюда запишем время GPU в миллисекундах\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&gpu_ms, e1, e2));             // вычисляем разницу между e1 и e2 (это и есть GPU time)\n",
        "\n",
        "        CUDA_CHECK(cudaEventDestroy(e1));                              // удаляем событие начала (чтобы не текли ресурсы)\n",
        "        CUDA_CHECK(cudaEventDestroy(e2));                              // удаляем событие конца\n",
        "\n",
        "        cout << \"CPU time: \" << cpu_ms << \" ms\\n\";                    // выводим CPU время\n",
        "        cout << \"GPU time: \" << gpu_ms << \" ms\\n\";                    // выводим GPU время\n",
        "\n",
        "        if (gpu_ms > 0.0f)                                            // проверка на ноль (чтобы не делить на 0)\n",
        "            cout << \"Speedup (CPU/GPU): \" << (cpu_ms / gpu_ms) << \"x\\n\"; // ускорение: >1 значит GPU быстрее, <1 значит CPU быстрее\n",
        "    }\n",
        "\n",
        "    return 0;                                                         // корректное завершение программы\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwNANu0fzEt6",
        "outputId": "45d281a3-1fcf-4786-d8e8-dd55e76453a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gpu_merge_sort.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_75 gpu_merge_sort.cu -o gpu_merge_sort\n",
        "!./gpu_merge_sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvo_TVRBaCVr",
        "outputId": "7fc54c4f-3c57-48c9-f5a7-daef45b5b69b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер массива N = 10000\n",
            "CPU time: 0.552883 ms\n",
            "GPU time: 0.600704 ms\n",
            "Speedup (CPU/GPU): 0.920392x\n",
            "Размер массива N = 100000\n",
            "CPU time: 6.88141 ms\n",
            "GPU time: 0.751328 ms\n",
            "Speedup (CPU/GPU): 9.15899x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Вывод**\n",
        "\n",
        "В ходе решения задачи 4 была измерена производительность сортировки на CPU и GPU для массивов размером 10 000 и 100 000 элементов.\n",
        "\n",
        "Для массива из 10 000 элементов время выполнения на CPU составило 0.55 мс, а на GPU 0.60 мс. Ускорение оказалось меньше единицы (0.92x), что означает, что при таком размере данных использование GPU не даёт выигрыша по времени. Это объясняется накладными расходами на передачу данных между CPU и GPU, а также на запуск вычислительных ядер, которые становятся значимыми при малых объёмах данных.\n",
        "\n",
        "Для массива из 100 000 элементов время выполнения на CPU составило 6.88 мс, тогда как на GPU 0.75 мс. В этом случае было получено ускорение около 9.16x, что демонстрирует высокую эффективность параллельной обработки на GPU при достаточно большом объёме данных.\n",
        "\n",
        "Таким образом, результаты показывают, что использование GPU оправдано и эффективно для обработки больших массивов, тогда как для малых массивов последовательная реализация на CPU может быть более выгодной из-за меньших накладных расходов."
      ],
      "metadata": {
        "id": "UoYg5Ud04WDV"
      }
    }
  ]
}