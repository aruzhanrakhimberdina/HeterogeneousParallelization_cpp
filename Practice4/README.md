# Контрольные вопросы

## 1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?

В CUDA есть несколько основных типов памяти. Глобальная память самая большая, но и самая медленная, поэтому ее используют для хранения основного массива данных. Разделяемая память находится внутри блока потоков, она намного быстрее, поэтому ее используют, когда несколько потоков работают с одними и теми же данными. Локальная память принадлежит отдельному потоку и используется автоматически, если регистров не хватает. Регистры самые быстрые, но их очень мало, поэтому в них хранят только временные переменные. Константная память подходит для данных, которые не меняются во время работы программы.

## 2. Как использование разделяемой памяти влияет на производительность?

Разделяемая память обычно сильно ускоряет работу программы, потому что доступ к ней намного быстрее, чем к глобальной памяти. Если данные сначала загрузить в shared memory, а потом многократно использовать внутри блока, то уменьшается количество медленных обращений к глобальной памяти. Это особенно полезно в редукции, матричных вычислениях и сортировке небольших подмассивов.

## 3. Доступ и как его обеспечить?

Чтобы обеспечить быстрый доступ к памяти, нужно использовать коалесцированный доступ к глобальной памяти, то есть организовывать чтение так, чтобы соседние потоки читали соседние элементы массива. Также важно правильно выбирать размер блока потоков, чтобы эффективно использовать кэш и shared memory. Если данные часто используются повторно, их лучше загружать в разделяемую память.

## 4. Какие сложности возникают при работе с большим объемом данных на GPU?

При больших объемах данных возникают проблемы с пропускной способностью памяти, так как копирование данных между CPU и GPU занимает время. Также может не хватать видеопамяти, особенно для массивов в миллионы элементов. Еще одна сложность это синхронизация потоков и блоков, так как неправильная организация работы может привести к простоям и снижению производительности.

## 5. Почему важно минимизировать доступ к глобальной памяти?

Глобальная память самая медленная в CUDA, поэтому частый доступ к ней сильно замедляет программу. Если можно заменить многократные обращения к глобальной памяти на работу в разделяемой памяти или регистрах, то производительность значительно возрастает. Минимизация доступа к глобальной памяти особенно важна в циклах и редукциях.

## 6. Как использовать профилирование для анализа производительности CUDA-программ?

Для профилирования можно использовать инструменты вроде NVIDIA Nsight или nvprof. Они показывают, сколько времени занимает каждый kernel, сколько обращений идет к памяти и есть ли проблемы с коалесцированием. С помощью профилирования можно увидеть узкие места в программе и понять, где стоит оптимизировать использование памяти, размер блоков или структуру алгоритма.

