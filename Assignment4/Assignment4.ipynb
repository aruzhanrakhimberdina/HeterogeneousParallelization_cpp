{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Assignment 4**"
      ],
      "metadata": {
        "id": "ndx7PW4FJxu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 1"
      ],
      "metadata": {
        "id": "DvKOvYW6J7Q6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8UxI48Ij6i",
        "outputId": "357fc26b-9df1-4092-8e67-e5db4c7b4c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sum_global.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile sum_global.cu\n",
        "\n",
        "#include <cuda_runtime.h>        // Подключаем CUDA Runtime API для работы с GPU\n",
        "#include <iostream>              // Подключаем библиотеку для ввода и вывода\n",
        "#include <vector>                // Подключаем контейнер std::vector\n",
        "#include <chrono>                // Подключаем таймер для измерения времени на CPU\n",
        "#include <cmath>                 // Подключаем математические функции (abs)\n",
        "\n",
        "using namespace std;             // Используем пространство имён std, чтобы не писать std::\n",
        "\n",
        "// Макрос для проверки ошибок CUDA-вызовов\n",
        "#define CHECK_CUDA(call) do {                               \\\n",
        "    cudaError_t err = call;                                 \\\n",
        "    if (err != cudaSuccess) {                                \\\n",
        "        cerr << \"CUDA error: \"                              \\\n",
        "             << cudaGetErrorString(err)                      \\\n",
        "             << \" at line \" << __LINE__ << endl;           \\\n",
        "        exit(EXIT_FAILURE);                                 \\\n",
        "    }                                                       \\\n",
        "} while(0)\n",
        "\n",
        "// CUDA-ядро для суммирования элементов массива с использованием глобальной памяти\n",
        "__global__\n",
        "void sum_global_kernel(const float* d_in, float* d_out, int n)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x; // Вычисляем глобальный индекс потока\n",
        "\n",
        "    if (idx < n)                                     // Проверяем, не вышли ли за границы массива\n",
        "        atomicAdd(d_out, d_in[idx]);                 // Атомарно добавляем элемент в общую сумму\n",
        "}\n",
        "\n",
        "// Последовательная функция для вычисления суммы на CPU\n",
        "float cpu_sum(const vector<float>& data)\n",
        "{\n",
        "    float sum = 0.0f;                                // Переменная для хранения суммы\n",
        "\n",
        "    for (float x : data)                             // Проходим по всем элементам массива\n",
        "        sum += x;                                   // Последовательно суммируем элементы\n",
        "\n",
        "    return sum;                                     // Возвращаем итоговую сумму\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int N = 100000;                            // Размер массива\n",
        "\n",
        "    vector<float> h_data(N);                         // Создаём массив на CPU\n",
        "\n",
        "    for (int i = 0; i < N; i++)                      // Заполняем массив случайными числами\n",
        "        h_data[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    // CPU вычисления\n",
        "\n",
        "    auto cpu_start = chrono::high_resolution_clock::now(); // Запускаем таймер CPU\n",
        "    float cpu_result = cpu_sum(h_data);                     // Вычисляем сумму на CPU\n",
        "    auto cpu_end = chrono::high_resolution_clock::now();   // Останавливаем таймер CPU\n",
        "\n",
        "    double cpu_time = chrono::duration<double, milli>(cpu_end - cpu_start).count(); // Время CPU\n",
        "\n",
        "    // GPU вычисления\n",
        "\n",
        "    float* d_in = nullptr;                           // Указатель на входной массив на GPU\n",
        "    float* d_out = nullptr;                          // Указатель на результат на GPU\n",
        "\n",
        "    CHECK_CUDA(cudaMalloc(&d_in, N * sizeof(float))); // Выделяем память под массив на GPU\n",
        "    CHECK_CUDA(cudaMalloc(&d_out, sizeof(float)));    // Выделяем память под результат\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(d_in, h_data.data(),        // Копируем данные с CPU на GPU\n",
        "                           N * sizeof(float),\n",
        "                           cudaMemcpyHostToDevice));\n",
        "\n",
        "    CHECK_CUDA(cudaMemset(d_out, 0, sizeof(float)));  // Обнуляем результат на GPU\n",
        "\n",
        "    int threads = 256;                               // Количество потоков в одном блоке\n",
        "    int blocks = (N + threads - 1) / threads;        // Вычисляем количество блоков\n",
        "\n",
        "    auto gpu_start = chrono::high_resolution_clock::now(); // Запускаем таймер GPU\n",
        "    sum_global_kernel<<<blocks, threads>>>(d_in, d_out, N); // Запускаем CUDA-ядро\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());              // Ждём завершения выполнения ядра\n",
        "    auto gpu_end = chrono::high_resolution_clock::now();   // Останавливаем таймер GPU\n",
        "\n",
        "    double gpu_time = chrono::duration<double, milli>(gpu_end - gpu_start).count(); // Время GPU\n",
        "\n",
        "    float gpu_result = 0.0f;                          // Переменная для хранения результата GPU\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(&gpu_result, d_out,         // Копируем результат с GPU на CPU\n",
        "                           sizeof(float),\n",
        "                           cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Вывод результатов\n",
        "\n",
        "    cout << \"CPU sum: \" << cpu_result << endl;        // Вывод суммы, вычисленной на CPU\n",
        "    cout << \"GPU sum: \" << gpu_result << endl;        // Вывод суммы, вычисленной на GPU\n",
        "    cout << \"Absolute error: \"                         // Вывод абсолютной ошибки\n",
        "         << abs(cpu_result - gpu_result) << endl;\n",
        "    cout << \"CPU time (ms): \" << cpu_time << endl;    // Вывод времени CPU\n",
        "    cout << \"GPU time (ms): \" << gpu_time << endl;    // Вывод времени GPU\n",
        "\n",
        "    cudaFree(d_in);                                   // Освобождаем память GPU для входных данных\n",
        "    cudaFree(d_out);                                  // Освобождаем память GPU для результата\n",
        "\n",
        "    return 0;                                         // Завершаем программу\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -O2 sum_global.cu -o sum_global\n",
        "!./sum_global"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grk6MN-EImBk",
        "outputId": "3d6595bd-fa2c-43f3-d8df-0264c4a29ec7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU sum: 49986.7\n",
            "GPU sum: 49986.9\n",
            "Absolute error: 0.191406\n",
            "CPU time (ms): 0.302751\n",
            "GPU time (ms): 0.465655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была реализована CUDA-программа для вычисления суммы элементов массива размером 100 000 с использованием глобальной памяти. Корректность реализации была подтверждена сравнением результатов, полученных на CPU и GPU.\n",
        "\n",
        "Сумма, вычисленная на CPU, составила 49986.7, в то время как результат GPU составил 49986.9. Абсолютная погрешность равна 0.191406. Данная разница обусловлена использованием чисел с плавающей точкой и различным порядком выполнения операций сложения при последовательных вычислениях на CPU и параллельных вычислениях на GPU. Полученная погрешность является допустимой и не влияет на корректность результата.\n",
        "\n",
        "Время выполнения последовательной реализации на CPU составило 0.30 мс, тогда как время выполнения CUDA-реализации на GPU составило 0.47 мс. Более длительное время выполнения на GPU объясняется накладными расходами на запуск CUDA-ядра и использованием атомарных операций в глобальной памяти, которые приводят к сериализации доступа потоков.\n",
        "\n",
        "Таким образом, реализованная версия корректно вычисляет сумму элементов массива, однако не является оптимальной с точки зрения производительности. Данная реализация служит базовым вариантом для последующего сравнения с оптимизированными реализациями, использующими более эффективные типы памяти CUDA."
      ],
      "metadata": {
        "id": "F1PlKkGjKhdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "nPxtVzqQKon1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scan_shared.cu\n",
        "#include <cuda_runtime.h>          // Подключаем CUDA Runtime API для работы с GPU\n",
        "#include <iostream>                // Подключаем библиотеку для ввода и вывода\n",
        "#include <vector>                  // Подключаем контейнер std::vector\n",
        "#include <chrono>                  // Подключаем таймер для измерения времени\n",
        "#include <cmath>                   // Подключаем математические функции (abs)\n",
        "\n",
        "using namespace std;               // Используем пространство имён std\n",
        "\n",
        "// Макрос для проверки ошибок CUDA\n",
        "#define CHECK_CUDA(call) do {                               \\\n",
        "    cudaError_t err = call;                                 \\\n",
        "    if (err != cudaSuccess) {                               \\\n",
        "        cerr << \"CUDA error: \"                              \\\n",
        "             << cudaGetErrorString(err)                     \\\n",
        "             << \" at line \" << __LINE__ << endl;            \\\n",
        "        exit(EXIT_FAILURE);                                 \\\n",
        "    }                                                       \\\n",
        "} while(0)\n",
        "\n",
        "\n",
        "// GPU KERNEL 1\n",
        "// Префиксная сумма внутри каждого блока с использованием shared memory\n",
        "__global__\n",
        "void block_scan_kernel(const float* d_in,\n",
        "                       float* d_out,\n",
        "                       float* d_block_sums,\n",
        "                       int n)\n",
        "{\n",
        "    extern __shared__ float sdata[];        // Разделяемая память блока\n",
        "\n",
        "    int tid = threadIdx.x;                  // Индекс потока внутри блока\n",
        "    int idx = blockIdx.x * blockDim.x + tid;// Глобальный индекс элемента\n",
        "\n",
        "    // Загружаем элементы из глобальной памяти в shared memory\n",
        "    if (idx < n)\n",
        "        sdata[tid] = d_in[idx];             // Копируем элемент массива\n",
        "    else\n",
        "        sdata[tid] = 0.0f;                  // Если вышли за границы, записываем 0\n",
        "\n",
        "    __syncthreads();                        // Синхронизация всех потоков блока\n",
        "\n",
        "    // Алгоритм Hillis–Steele для вычисления префиксной суммы\n",
        "    for (int offset = 1; offset < blockDim.x; offset <<= 1) {\n",
        "        float temp = 0.0f;                  // Временная переменная\n",
        "        if (tid >= offset)\n",
        "            temp = sdata[tid - offset];     // Читаем значение на расстоянии offset\n",
        "        __syncthreads();                    // Синхронизация перед обновлением\n",
        "        sdata[tid] += temp;                 // Добавляем значение к текущему элементу\n",
        "        __syncthreads();                    // Синхронизация после обновления\n",
        "    }\n",
        "\n",
        "    // Записываем результат префиксной суммы в глобальную память\n",
        "    if (idx < n)\n",
        "        d_out[idx] = sdata[tid];\n",
        "\n",
        "    // Последний поток блока сохраняет сумму всего блока\n",
        "    if (tid == blockDim.x - 1)\n",
        "        d_block_sums[blockIdx.x] = sdata[tid];\n",
        "}\n",
        "\n",
        "// GPU KERNEL 2\n",
        "// Добавление оффсетов блоков ко всем элементам\n",
        "__global__\n",
        "void add_offsets_kernel(float* d_data,\n",
        "                        const float* d_offsets,\n",
        "                        int n)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x; // Глобальный индекс элемента\n",
        "    if (idx < n)\n",
        "        d_data[idx] += d_offsets[blockIdx.x];        // Добавляем оффсет блока\n",
        "}\n",
        "\n",
        "// CPU PREFIX SUM\n",
        "// Последовательная реализация префиксной суммы на CPU\n",
        "void cpu_scan(const vector<float>& in, vector<float>& out)\n",
        "{\n",
        "    out.resize(in.size());               // Приводим размер выходного массива\n",
        "    float acc = 0.0f;                    // Накопительная сумма\n",
        "    for (size_t i = 0; i < in.size(); i++) {\n",
        "        acc += in[i];                    // Добавляем текущий элемент\n",
        "        out[i] = acc;                    // Записываем результат\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int N = 1'000'000;             // Размер массива\n",
        "    const int threads = 256;             // Количество потоков в блоке\n",
        "    int blocks = (N + threads - 1) / threads; // Количество блоков\n",
        "\n",
        "    vector<float> h_in(N);               // Входной массив на CPU\n",
        "    for (int i = 0; i < N; i++)           // Заполняем массив случайными числами\n",
        "        h_in[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    // CPU\n",
        "\n",
        "    vector<float> h_cpu;                 // Результат CPU\n",
        "    auto cpu_start = chrono::high_resolution_clock::now(); // Запуск таймера CPU\n",
        "    cpu_scan(h_in, h_cpu);               // Вычисление префиксной суммы на CPU\n",
        "    auto cpu_end = chrono::high_resolution_clock::now();   // Остановка таймера CPU\n",
        "    double cpu_time = chrono::duration<double, milli>(cpu_end - cpu_start).count();\n",
        "\n",
        "    // GPU\n",
        "\n",
        "    float *d_in, *d_out, *d_block_sums;  // Указатели на данные на GPU\n",
        "\n",
        "    CHECK_CUDA(cudaMalloc(&d_in, N * sizeof(float)));        // Выделение памяти под вход\n",
        "    CHECK_CUDA(cudaMalloc(&d_out, N * sizeof(float)));       // Память под результат\n",
        "    CHECK_CUDA(cudaMalloc(&d_block_sums, blocks * sizeof(float))); // Память под суммы блоков\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(d_in, h_in.data(),                 // Копируем данные на GPU\n",
        "                           N * sizeof(float),\n",
        "                           cudaMemcpyHostToDevice));\n",
        "\n",
        "    auto gpu_start = chrono::high_resolution_clock::now();   // Запуск таймера GPU\n",
        "\n",
        "    block_scan_kernel<<<blocks, threads, threads * sizeof(float)>>>(\n",
        "        d_in, d_out, d_block_sums, N);                        // Запуск первого ядра\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());                      // Ожидание завершения\n",
        "\n",
        "    // Копируем суммы блоков на CPU\n",
        "    vector<float> h_block_sums(blocks);\n",
        "    vector<float> h_offsets(blocks);\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(h_block_sums.data(),\n",
        "                           d_block_sums,\n",
        "                           blocks * sizeof(float),\n",
        "                           cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Вычисляем оффсеты блоков на CPU\n",
        "    float running = 0.0f;\n",
        "    for (int i = 0; i < blocks; i++) {\n",
        "        h_offsets[i] = running;        // Оффсет текущего блока\n",
        "        running += h_block_sums[i];    // Обновляем накопленную сумму\n",
        "    }\n",
        "\n",
        "    float* d_offsets;                  // Указатель на оффсеты на GPU\n",
        "    CHECK_CUDA(cudaMalloc(&d_offsets, blocks * sizeof(float)));\n",
        "    CHECK_CUDA(cudaMemcpy(d_offsets,\n",
        "                           h_offsets.data(),\n",
        "                           blocks * sizeof(float),\n",
        "                           cudaMemcpyHostToDevice));\n",
        "\n",
        "    add_offsets_kernel<<<blocks, threads>>>(d_out, d_offsets, N); // Добавление оффсетов\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());                           // Ожидание\n",
        "\n",
        "    auto gpu_end = chrono::high_resolution_clock::now();          // Остановка таймера GPU\n",
        "    double gpu_time = chrono::duration<double, milli>(gpu_end - gpu_start).count();\n",
        "\n",
        "    // Проверка\n",
        "\n",
        "    vector<float> h_gpu(N);                 // Результат GPU на CPU\n",
        "    CHECK_CUDA(cudaMemcpy(h_gpu.data(),\n",
        "                           d_out,\n",
        "                           N * sizeof(float),\n",
        "                           cudaMemcpyDeviceToHost));\n",
        "\n",
        "    float max_error = 0.0f;                 // Максимальная ошибка\n",
        "    for (int i = 0; i < N; i++)\n",
        "        max_error = max(max_error, abs(h_cpu[i] - h_gpu[i]));\n",
        "\n",
        "    // Вывод\n",
        "\n",
        "    cout << \"Array size: \" << N << endl;\n",
        "    cout << \"Threads per block: \" << threads << endl << endl;\n",
        "\n",
        "    cout << \"CPU time (ms): \" << cpu_time << endl;\n",
        "    cout << \"GPU time (ms): \" << gpu_time << endl;\n",
        "    cout << \"Speedup (CPU / GPU): \" << cpu_time / gpu_time << \"x\" << endl << endl;\n",
        "\n",
        "    cout << \"First 10 elements:\" << endl;\n",
        "    cout << \"CPU: \";\n",
        "    for (int i = 0; i < 10; i++) cout << h_cpu[i] << \" \";\n",
        "    cout << endl;\n",
        "\n",
        "    cout << \"GPU: \";\n",
        "    for (int i = 0; i < 10; i++) cout << h_gpu[i] << \" \";\n",
        "    cout << endl << endl;\n",
        "\n",
        "    cout << \"Last element (total sum):\" << endl;\n",
        "    cout << \"CPU: \" << h_cpu[N - 1] << endl;\n",
        "    cout << \"GPU: \" << h_gpu[N - 1] << endl << endl;\n",
        "\n",
        "    cout << \"Max absolute error: \" << max_error << endl;\n",
        "\n",
        "    cudaFree(d_in);                         // Освобождаем память GPU\n",
        "    cudaFree(d_out);\n",
        "    cudaFree(d_block_sums);\n",
        "    cudaFree(d_offsets);\n",
        "\n",
        "    return 0;                               // Завершаем программу\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qAeLUyTKnc8",
        "outputId": "76d33ae7-f346-42bd-a965-0adb94385a24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scan_shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -O2 scan_shared.cu -o scan_shared\n",
        "!./scan_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6-UvaOaMQaB",
        "outputId": "d45c46dd-f2bb-4c09-bfb7-dab51c2fa68a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array size: 1000000\n",
            "Threads per block: 256\n",
            "\n",
            "CPU time (ms): 3.63966\n",
            "GPU time (ms): 0.461058\n",
            "Speedup (CPU / GPU): 7.89414x\n",
            "\n",
            "First 10 elements:\n",
            "CPU: 0.840188 1.23457 2.01767 2.81611 3.72776 3.92531 4.26053 5.02876 5.30654 5.86051 \n",
            "GPU: 0.840188 1.23457 2.01767 2.81611 3.72776 3.92531 4.26053 5.02876 5.30654 5.86051 \n",
            "\n",
            "Last element (total sum):\n",
            "CPU: 500004\n",
            "GPU: 500007\n",
            "\n",
            "Max absolute error: 5.53125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была реализована CUDA-программа для вычисления префиксной суммы массива размером 1 000 000 элементов с использованием разделяемой памяти. Корректность реализации была проверена путём сравнения результатов, полученных на CPU и GPU.\n",
        "\n",
        "Время выполнения последовательной реализации на CPU составило 3.64 мс, тогда как время выполнения реализации на GPU составило 0.46 мс. Таким образом, использование GPU позволило получить ускорение примерно в 7.9 раза по сравнению с CPU. Это показывает эффективность параллельных вычислений и применения разделяемой памяти при обработке больших массивов данных.\n",
        "\n",
        "Сравнение первых элементов префиксной суммы показало полное совпадение результатов CPU и GPU, что подтверждает корректность вычислений. Значение последнего элемента массива, соответствующее полной сумме, отличается на небольшую величину. Максимальная абсолютная погрешность составила 5.53, что объясняется использованием чисел с плавающей точкой и различием порядка операций сложения при параллельных вычислениях на GPU. Данная погрешность является допустимой для типа данных float.\n",
        "\n",
        "Таким образом, реализованная версия префиксной суммы с использованием разделяемой памяти корректно выполняет поставленную задачу и существенно превосходит последовательную реализацию на CPU по производительности. Результаты эксперимента подтверждают, что оптимизация доступа к памяти и использование архитектуры GPU позволяют значительно ускорить вычисление операций сканирования."
      ],
      "metadata": {
        "id": "zLvGN8bjN-uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3"
      ],
      "metadata": {
        "id": "BQ8cBmA0OluO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_sum.cu\n",
        "\n",
        "#include <cuda_runtime.h>          // Подключаем CUDA Runtime API для работы с GPU\n",
        "#include <iostream>                // Подключаем библиотеку для ввода и вывода\n",
        "#include <vector>                  // Подключаем контейнер std::vector\n",
        "#include <chrono>                  // Подключаем таймер для измерения времени\n",
        "#include <cmath>                   // Подключаем математические функции (abs)\n",
        "#include <thread>                  // std::thread\n",
        "\n",
        "using namespace std;               // Используем пространство имён std\n",
        "\n",
        "// Макрос для проверки ошибок CUDA\n",
        "#define CHECK_CUDA(call) do {                               \\\n",
        "    cudaError_t err = call;                                 \\\n",
        "    if (err != cudaSuccess) {                               \\\n",
        "        cerr << \"CUDA error: \"                              \\\n",
        "             << cudaGetErrorString(err)                     \\\n",
        "             << \" at line \" << __LINE__ << endl;            \\\n",
        "        exit(EXIT_FAILURE);                                 \\\n",
        "    }                                                       \\\n",
        "} while(0)\n",
        "\n",
        "//  GPU KERNEL\n",
        "// CUDA-ядро для суммирования элементов массива с использованием глобальной памяти\n",
        "__global__\n",
        "void sum_kernel(const float* d_in, float* d_out, int n)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x; // Вычисляем глобальный индекс потока\n",
        "\n",
        "    if (idx < n)                                     // Проверяем выход за границы массива\n",
        "        atomicAdd(d_out, d_in[idx]);                 // Атомарно добавляем элемент к общей сумме\n",
        "}\n",
        "\n",
        "//  CPU SUM\n",
        "// Последовательная функция для суммирования части массива на CPU\n",
        "float cpu_sum_part(const vector<float>& data, int start, int end)\n",
        "{\n",
        "    float sum = 0.0f;                                // Переменная для накопления суммы\n",
        "\n",
        "    for (int i = start; i < end; i++)                // Проходим по заданному диапазону\n",
        "        sum += data[i];                              // Добавляем текущий элемент\n",
        "\n",
        "    return sum;                                      // Возвращаем частичную сумму\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int N = 1'000'000;                         // Размер массива\n",
        "    const int threads = 256;                         // Количество потоков в одном блоке\n",
        "\n",
        "    vector<float> h_data(N);                         // Создаём массив на CPU\n",
        "\n",
        "    for (int i = 0; i < N; i++)                      // Заполняем массив случайными числами\n",
        "        h_data[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    //  CPU ONLY\n",
        "\n",
        "    auto cpu_start = chrono::high_resolution_clock::now(); // Запускаем таймер CPU\n",
        "    float cpu_result = cpu_sum_part(h_data, 0, N);         // Считаем сумму всего массива на CPU\n",
        "    auto cpu_end = chrono::high_resolution_clock::now();   // Останавливаем таймер CPU\n",
        "\n",
        "    double cpu_time =\n",
        "        chrono::duration<double, milli>(cpu_end - cpu_start).count(); // Время CPU\n",
        "\n",
        "    //  GPU ONLY\n",
        "\n",
        "    float *d_in, *d_out;                            // Указатели на данные на GPU\n",
        "\n",
        "    CHECK_CUDA(cudaMalloc(&d_in, N * sizeof(float))); // Выделяем память под массив на GPU\n",
        "    CHECK_CUDA(cudaMalloc(&d_out, sizeof(float)));    // Выделяем память под результат\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(d_in, h_data.data(),       // Копируем массив с CPU на GPU\n",
        "                           N * sizeof(float),\n",
        "                           cudaMemcpyHostToDevice));\n",
        "\n",
        "    CHECK_CUDA(cudaMemset(d_out, 0, sizeof(float))); // Обнуляем результат на GPU\n",
        "\n",
        "    int blocks = (N + threads - 1) / threads;        // Вычисляем количество блоков\n",
        "\n",
        "    auto gpu_start = chrono::high_resolution_clock::now(); // Запускаем таймер GPU\n",
        "    sum_kernel<<<blocks, threads>>>(d_in, d_out, N); // Запускаем CUDA-ядро\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());              // Ждём завершения GPU\n",
        "    auto gpu_end = chrono::high_resolution_clock::now();   // Останавливаем таймер GPU\n",
        "\n",
        "    double gpu_time =\n",
        "        chrono::duration<double, milli>(gpu_end - gpu_start).count(); // Время GPU\n",
        "\n",
        "    float gpu_result = 0.0f;                          // Переменная для GPU-результата\n",
        "    CHECK_CUDA(cudaMemcpy(&gpu_result, d_out,         // Копируем результат с GPU на CPU\n",
        "                           sizeof(float),\n",
        "                           cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // HYBRID CPU + GPU\n",
        "\n",
        "    int mid = N / 2;                                 // Делим массив пополам\n",
        "\n",
        "    CHECK_CUDA(cudaMemset(d_out, 0, sizeof(float))); // Обнуляем GPU-результат\n",
        "\n",
        "    auto hybrid_start = chrono::high_resolution_clock::now(); // Запуск таймера гибридной версии\n",
        "\n",
        "    float cpu_partial = 0.0f;                        // Частичная сумма CPU\n",
        "\n",
        "    // Запускаем вычисление первой половины массива на CPU в отдельном потоке\n",
        "    thread cpu_thread([&]() {\n",
        "        cpu_partial = cpu_sum_part(h_data, 0, mid);\n",
        "    });\n",
        "\n",
        "    // GPU вычисляет сумму второй половины массива параллельно с CPU\n",
        "    sum_kernel<<<(N - mid + threads - 1) / threads, threads>>>(\n",
        "        d_in + mid, d_out, N - mid);\n",
        "\n",
        "    cpu_thread.join();                               // Ждём завершения CPU-потока\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());             // Ждём завершения GPU\n",
        "\n",
        "    auto hybrid_end = chrono::high_resolution_clock::now(); // Остановка таймера\n",
        "\n",
        "    double hybrid_time =\n",
        "        chrono::duration<double, milli>(hybrid_end - hybrid_start).count(); // Время гибридной версии\n",
        "\n",
        "    float gpu_partial = 0.0f;                        // Частичная сумма GPU\n",
        "    CHECK_CUDA(cudaMemcpy(&gpu_partial, d_out,       // Копируем частичный результат GPU\n",
        "                           sizeof(float),\n",
        "                           cudaMemcpyDeviceToHost));\n",
        "\n",
        "    float hybrid_result = cpu_partial + gpu_partial; // Итоговая сумма (CPU + GPU)\n",
        "\n",
        "    // Вывод\n",
        "\n",
        "    cout << \"Array size: \" << N << endl << endl;\n",
        "\n",
        "    cout << \"CPU only:\" << endl;\n",
        "    cout << \"Result: \" << cpu_result << endl;\n",
        "    cout << \"Time (ms): \" << cpu_time << endl << endl;\n",
        "\n",
        "    cout << \"GPU only:\" << endl;\n",
        "    cout << \"Result: \" << gpu_result << endl;\n",
        "    cout << \"Time (ms): \" << gpu_time << endl << endl;\n",
        "\n",
        "    cout << \"Hybrid CPU + GPU:\" << endl;\n",
        "    cout << \"Result: \" << hybrid_result << endl;\n",
        "    cout << \"Time (ms): \" << hybrid_time << endl << endl;\n",
        "\n",
        "    cout << \"Speedup (CPU / Hybrid): \"\n",
        "         << cpu_time / hybrid_time << \"x\" << endl;\n",
        "\n",
        "    cudaFree(d_in);                                  // Освобождаем память GPU\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    return 0;                                        // Завершаем программу\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGutz7BKOsHn",
        "outputId": "ed393d88-8618-4e3d-ea9b-c2dd425ac5f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hybrid_sum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -O2 hybrid_sum.cu -o hybrid_sum\n",
        "!./hybrid_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDzbVbGRPEov",
        "outputId": "dac2716b-2fbf-4339-c45d-7b2613b515d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array size: 1000000\n",
            "\n",
            "CPU only:\n",
            "Result: 500004\n",
            "Time (ms): 2.56455\n",
            "\n",
            "GPU only:\n",
            "Result: 500003\n",
            "Time (ms): 3.55758\n",
            "\n",
            "Hybrid CPU + GPU:\n",
            "Result: 500008\n",
            "Time (ms): 1.80905\n",
            "\n",
            "Speedup (CPU / Hybrid): 1.41762x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была реализована гибридная программа, в которой обработка массива выполнялась параллельно на CPU и GPU. Массив размером 1 000 000 элементов был разделён на две части: первая часть обрабатывалась на CPU, а вторая на GPU. После завершения вычислений частичные результаты объединялись на CPU.\n",
        "\n",
        "Последовательная реализация на CPU показала время выполнения 2.56 мс. Реализация, использующая только GPU, выполнила вычисления за 3.56 мс, что оказалось медленнее CPU. Это связано с накладными расходами на запуск CUDA-ядра и использованием атомарных операций в глобальной памяти, которые приводят к сериализации доступа потоков.\n",
        "\n",
        "Гибридная реализация продемонстрировала наилучший результат по времени выполнения 1.81 мс. Это обеспечило ускорение примерно в 1.42 раза по сравнению с последовательной реализацией на CPU. Полученные результаты показывают, что одновременное использование CPU и GPU позволяет более эффективно задействовать вычислительные ресурсы системы.\n",
        "\n",
        "Результаты вычислений для CPU-, GPU- и гибридной реализаций совпадают с допустимой погрешностью, обусловленной вычислениями с плавающей точкой и различием порядка операций сложения. Это подтверждает корректность реализованного гибридного подхода.\n",
        "\n",
        "Таким образом, эксперимент подтвердил, что гибридная модель вычислений может быть более эффективной, чем использование только CPU или только GPU, и демонстрирует преимущества гетерогенных вычислений при обработке больших массивов данных."
      ],
      "metadata": {
        "id": "l2CJngHfP_pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4"
      ],
      "metadata": {
        "id": "IeCtpJjUQmzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum.cpp\n",
        "\n",
        "#include <mpi.h>          // Основная библиотека MPI\n",
        "#include <iostream>      // Ввод и вывод\n",
        "#include <vector>        // Контейнер std::vector\n",
        "#include <cstdlib>       // rand, RAND_MAX\n",
        "\n",
        "using namespace std;     // Используем пространство имён std\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    MPI_Init(&argc, &argv);              // Инициализация MPI\n",
        "\n",
        "    int rank, size;                      // rank - номер процесса, size - их количество\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    const int N = 1'000'000;             // Общий размер массива\n",
        "    int local_n = N / size;              // Размер части для каждого процесса\n",
        "\n",
        "    vector<float> local_data(local_n);   // Локальный массив процесса\n",
        "\n",
        "    vector<float> data;                  // Полный массив (только у процесса 0)\n",
        "    if (rank == 0) {\n",
        "        data.resize(N);                  // Выделяем память под массив\n",
        "        for (int i = 0; i < N; i++)       // Заполняем массив случайными числами\n",
        "            data[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);          // Синхронизация процессов\n",
        "    double start_time = MPI_Wtime();      // Начало замера времени\n",
        "\n",
        "    MPI_Scatter(data.data(),              // Отправляем данные от процесса 0\n",
        "                local_n, MPI_FLOAT,       // Размер и тип части\n",
        "                local_data.data(),        // Локальный буфер\n",
        "                local_n, MPI_FLOAT,       // Размер и тип принимаемых данных\n",
        "                0, MPI_COMM_WORLD);       // Корневой процесс — 0\n",
        "\n",
        "    float local_sum = 0.0f;               // Локальная сумма\n",
        "    for (int i = 0; i < local_n; i++)\n",
        "        local_sum += local_data[i];       // Суммируем локальную часть массива\n",
        "\n",
        "    float global_sum = 0.0f;              // Итоговая сумма\n",
        "\n",
        "    MPI_Reduce(&local_sum,                // Локальные суммы\n",
        "               &global_sum,               // Итоговый результат\n",
        "               1, MPI_FLOAT,              // Один элемент типа float\n",
        "               MPI_SUM,                   // Операция суммирования\n",
        "               0, MPI_COMM_WORLD);        // Корневой процесс — 0\n",
        "\n",
        "    double end_time = MPI_Wtime();        // Конец замера времени\n",
        "\n",
        "    if (rank == 0) {\n",
        "        cout << \"Processes: \" << size << endl;\n",
        "        cout << \"Result (sum): \" << global_sum << endl;\n",
        "        cout << \"Execution time (s): \"\n",
        "             << end_time - start_time << endl << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                       // Завершение MPI\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljNJrf-jQya2",
        "outputId": "86c9d1b7-a677-496d-9836-79fa9074d93e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_sum.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_sum.cpp -o mpi_sum"
      ],
      "metadata": {
        "id": "MflIwbE-SKco"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_sum\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_sum\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./mpi_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmV5-nrvSOTc",
        "outputId": "419f5571-d4b9-4619-f63c-cad5bd65bdce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processes: 2\n",
            "Result (sum): 500008\n",
            "Execution time (s): 0.00317955\n",
            "\n",
            "Processes: 4\n",
            "Result (sum): 500007\n",
            "Execution time (s): 0.00443688\n",
            "\n",
            "Processes: 8\n",
            "Result (sum): 500007\n",
            "Execution time (s): 0.00353687\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**\n",
        "\n",
        "В рамках задания была реализована распределённая программа с использованием технологии MPI для обработки массива данных размером 1 000 000 элементов. Массив был разделён между процессами, каждый процесс выполнял локальные вычисления, после чего частичные результаты объединялись с помощью операции MPI_Reduce.\n",
        "\n",
        "Экспериментальные замеры времени выполнения были проведены для 2, 4 и 8 процессов. При использовании 2 процессов время выполнения составило 0.00318 с. При увеличении числа процессов до 4 время выполнения возросло до 0.00444 с, а при использовании 8 процессов составило 0.00354 с. Полученные результаты показывают, что увеличение числа процессов не всегда приводит к уменьшению времени выполнения.\n",
        "\n",
        "Такое поведение объясняется накладными расходами на коммуникацию между процессами и синхронизацию данных. Для относительно небольшого объёма вычислений затраты на обмен данными могут превосходить выигрыш от дополнительного параллелизма, что приводит к снижению эффективности масштабирования.\n",
        "\n",
        "Результаты вычислений для всех запусков совпадают с допустимой погрешностью, обусловленной использованием чисел с плавающей точкой. Это подтверждает корректность реализации распределённого алгоритма.\n",
        "\n",
        "Таким образом, эксперимент показал, что MPI-подход эффективен для распределения вычислений, однако его производительность существенно зависит от соотношения объёма вычислений и коммуникационных затрат. Для достижения лучшей масштабируемости требуется увеличение размера задачи или более сложные вычисления внутри каждого процесса."
      ],
      "metadata": {
        "id": "DzbixH3YVseQ"
      }
    }
  ]
}